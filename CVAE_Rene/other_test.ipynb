{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "bs = 100\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='../data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='../data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/datasets/mnist.py:52: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim, c_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim + c_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim + c_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "    \n",
    "    def encoder(self, x, c):\n",
    "        concat_input = torch.cat([x, c], 1)\n",
    "        h = F.relu(self.fc1(concat_input))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h)\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add(mu) # return z sample\n",
    "    \n",
    "    def decoder(self, z, c):\n",
    "        concat_input = torch.cat([z, c], 1)\n",
    "        h = F.relu(self.fc4(concat_input))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h))\n",
    "    \n",
    "    def forward(self, x, c):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784), c)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z, c), mu, log_var\n",
    "\n",
    "# build model\n",
    "cond_dim = train_loader.dataset.train_labels.unique().size(0)\n",
    "cvae = CVAE(x_dim=784, h_dim1=512, h_dim2=256, z_dim=2, c_dim=cond_dim)\n",
    "if torch.cuda.is_available():\n",
    "    cvae.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer = optim.Adam(cvae.parameters())\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# one-hot encoding\n",
    "def one_hot(labels, class_size): \n",
    "    targets = torch.zeros(labels.size(0), class_size)\n",
    "    for i, label in enumerate(labels):\n",
    "        targets[i, label] = 1\n",
    "    return Variable(targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    cvae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, cond) in enumerate(train_loader):\n",
    "        data, cond = data.cuda(), one_hot(cond, cond_dim).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = cvae(data, cond)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test():\n",
    "    cvae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, cond in test_loader:\n",
    "            data, cond = data.cuda(), one_hot(cond, cond_dim).cuda()\n",
    "            recon, mu, log_var = cvae(data, cond)\n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1905: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 542.922930\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 175.651152\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 167.365508\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 151.432646\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 144.553662\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 142.057148\n",
      "====> Epoch: 1 Average loss: 162.4478\n",
      "====> Test set loss: 141.1527\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 143.317627\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 148.638652\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 135.379805\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 138.878730\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 150.546680\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 138.759570\n",
      "====> Epoch: 2 Average loss: 138.4400\n",
      "====> Test set loss: 136.1983\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 143.725098\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 132.865947\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 129.392725\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 134.886309\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 136.612412\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 133.598652\n",
      "====> Epoch: 3 Average loss: 135.1572\n",
      "====> Test set loss: 134.2754\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 134.462461\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 134.116914\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 128.649932\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 130.013369\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 129.050342\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 132.930488\n",
      "====> Epoch: 4 Average loss: 133.5283\n",
      "====> Test set loss: 133.3251\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 126.891543\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 132.586377\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 135.106172\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 129.827354\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 135.293008\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 124.739756\n",
      "====> Epoch: 5 Average loss: 132.4477\n",
      "====> Test set loss: 132.4812\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 126.997578\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 134.407744\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 131.492549\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 145.928516\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 134.484102\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 134.497363\n",
      "====> Epoch: 6 Average loss: 131.6958\n",
      "====> Test set loss: 132.2612\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 122.538262\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 131.021172\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 130.111641\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 132.243281\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 127.902021\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 136.613105\n",
      "====> Epoch: 7 Average loss: 131.1238\n",
      "====> Test set loss: 131.7179\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 125.223125\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 127.965771\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 129.260088\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 125.204463\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 130.655576\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 129.493398\n",
      "====> Epoch: 8 Average loss: 130.6115\n",
      "====> Test set loss: 131.0987\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 131.201484\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 127.435918\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 132.456230\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 126.061152\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 128.905322\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 139.166572\n",
      "====> Epoch: 9 Average loss: 130.2192\n",
      "====> Test set loss: 131.2503\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 131.940684\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 133.036113\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 125.120029\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 136.352490\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 130.072080\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 136.464033\n",
      "====> Epoch: 10 Average loss: 129.8323\n",
      "====> Test set loss: 130.6072\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 133.637188\n",
      "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 129.402900\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 131.523701\n",
      "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 130.071055\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 125.749658\n",
      "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 128.737695\n",
      "====> Epoch: 11 Average loss: 129.4891\n",
      "====> Test set loss: 130.4899\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 127.217832\n",
      "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 133.009121\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 134.257578\n",
      "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 135.831914\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 133.604326\n",
      "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 124.711494\n",
      "====> Epoch: 12 Average loss: 129.1146\n",
      "====> Test set loss: 130.3752\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 116.793086\n",
      "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 129.262822\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 136.280293\n",
      "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 134.654590\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 133.304131\n",
      "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 124.315137\n",
      "====> Epoch: 13 Average loss: 128.8422\n",
      "====> Test set loss: 129.8635\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 135.422354\n",
      "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 136.662578\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 128.991445\n",
      "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 133.007305\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 124.024570\n",
      "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 123.128418\n",
      "====> Epoch: 14 Average loss: 128.5337\n",
      "====> Test set loss: 129.8290\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 124.012480\n",
      "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 131.958740\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 133.832969\n",
      "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 123.883926\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 136.877549\n",
      "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 131.471445\n",
      "====> Epoch: 15 Average loss: 128.2671\n",
      "====> Test set loss: 129.3106\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 130.084121\n",
      "Train Epoch: 16 [10000/60000 (17%)]\tLoss: 127.479717\n",
      "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 137.814092\n",
      "Train Epoch: 16 [30000/60000 (50%)]\tLoss: 126.368418\n",
      "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 128.620654\n",
      "Train Epoch: 16 [50000/60000 (83%)]\tLoss: 126.730898\n",
      "====> Epoch: 16 Average loss: 127.9871\n",
      "====> Test set loss: 129.4484\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 117.696836\n",
      "Train Epoch: 17 [10000/60000 (17%)]\tLoss: 134.252930\n",
      "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 131.170420\n",
      "Train Epoch: 17 [30000/60000 (50%)]\tLoss: 130.768730\n",
      "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 129.491094\n",
      "Train Epoch: 17 [50000/60000 (83%)]\tLoss: 122.594600\n",
      "====> Epoch: 17 Average loss: 127.8154\n",
      "====> Test set loss: 129.2261\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 126.935098\n",
      "Train Epoch: 18 [10000/60000 (17%)]\tLoss: 132.948242\n",
      "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 127.700391\n",
      "Train Epoch: 18 [30000/60000 (50%)]\tLoss: 131.597412\n",
      "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 127.388018\n",
      "Train Epoch: 18 [50000/60000 (83%)]\tLoss: 116.528223\n",
      "====> Epoch: 18 Average loss: 127.5400\n",
      "====> Test set loss: 129.4491\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 127.142295\n",
      "Train Epoch: 19 [10000/60000 (17%)]\tLoss: 123.954434\n",
      "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 122.969795\n",
      "Train Epoch: 19 [30000/60000 (50%)]\tLoss: 126.937930\n",
      "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 130.692305\n",
      "Train Epoch: 19 [50000/60000 (83%)]\tLoss: 123.378086\n",
      "====> Epoch: 19 Average loss: 127.3112\n",
      "====> Test set loss: 129.1604\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 129.230820\n",
      "Train Epoch: 20 [10000/60000 (17%)]\tLoss: 128.255039\n",
      "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 130.458379\n",
      "Train Epoch: 20 [30000/60000 (50%)]\tLoss: 121.941836\n",
      "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 126.330752\n",
      "Train Epoch: 20 [50000/60000 (83%)]\tLoss: 134.975576\n",
      "====> Epoch: 20 Average loss: 127.1242\n",
      "====> Test set loss: 129.0057\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 125.589980\n",
      "Train Epoch: 21 [10000/60000 (17%)]\tLoss: 126.143594\n",
      "Train Epoch: 21 [20000/60000 (33%)]\tLoss: 134.616562\n",
      "Train Epoch: 21 [30000/60000 (50%)]\tLoss: 127.420244\n",
      "Train Epoch: 21 [40000/60000 (67%)]\tLoss: 125.113135\n",
      "Train Epoch: 21 [50000/60000 (83%)]\tLoss: 124.404961\n",
      "====> Epoch: 21 Average loss: 126.9461\n",
      "====> Test set loss: 128.8658\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 122.156016\n",
      "Train Epoch: 22 [10000/60000 (17%)]\tLoss: 128.033760\n",
      "Train Epoch: 22 [20000/60000 (33%)]\tLoss: 128.201709\n",
      "Train Epoch: 22 [30000/60000 (50%)]\tLoss: 130.951270\n",
      "Train Epoch: 22 [40000/60000 (67%)]\tLoss: 127.807578\n",
      "Train Epoch: 22 [50000/60000 (83%)]\tLoss: 131.300527\n",
      "====> Epoch: 22 Average loss: 126.7501\n",
      "====> Test set loss: 128.8510\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 133.915771\n",
      "Train Epoch: 23 [10000/60000 (17%)]\tLoss: 132.703594\n",
      "Train Epoch: 23 [20000/60000 (33%)]\tLoss: 130.404561\n",
      "Train Epoch: 23 [30000/60000 (50%)]\tLoss: 125.718945\n",
      "Train Epoch: 23 [40000/60000 (67%)]\tLoss: 129.699121\n",
      "Train Epoch: 23 [50000/60000 (83%)]\tLoss: 125.838457\n",
      "====> Epoch: 23 Average loss: 126.5613\n",
      "====> Test set loss: 128.7201\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 125.538174\n",
      "Train Epoch: 24 [10000/60000 (17%)]\tLoss: 130.806729\n",
      "Train Epoch: 24 [20000/60000 (33%)]\tLoss: 125.663896\n",
      "Train Epoch: 24 [30000/60000 (50%)]\tLoss: 124.517041\n",
      "Train Epoch: 24 [40000/60000 (67%)]\tLoss: 122.878047\n",
      "Train Epoch: 24 [50000/60000 (83%)]\tLoss: 131.456436\n",
      "====> Epoch: 24 Average loss: 126.4686\n",
      "====> Test set loss: 128.9233\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 133.063047\n",
      "Train Epoch: 25 [10000/60000 (17%)]\tLoss: 129.436953\n",
      "Train Epoch: 25 [20000/60000 (33%)]\tLoss: 129.880439\n",
      "Train Epoch: 25 [30000/60000 (50%)]\tLoss: 128.578457\n",
      "Train Epoch: 25 [40000/60000 (67%)]\tLoss: 126.453877\n",
      "Train Epoch: 25 [50000/60000 (83%)]\tLoss: 130.467412\n",
      "====> Epoch: 25 Average loss: 126.2918\n",
      "====> Test set loss: 128.8162\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 123.111777\n",
      "Train Epoch: 26 [10000/60000 (17%)]\tLoss: 121.196553\n",
      "Train Epoch: 26 [20000/60000 (33%)]\tLoss: 124.677891\n",
      "Train Epoch: 26 [30000/60000 (50%)]\tLoss: 136.226650\n",
      "Train Epoch: 26 [40000/60000 (67%)]\tLoss: 126.234863\n",
      "Train Epoch: 26 [50000/60000 (83%)]\tLoss: 125.907725\n",
      "====> Epoch: 26 Average loss: 126.1359\n",
      "====> Test set loss: 128.4986\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 123.354102\n",
      "Train Epoch: 27 [10000/60000 (17%)]\tLoss: 123.109854\n",
      "Train Epoch: 27 [20000/60000 (33%)]\tLoss: 122.660283\n",
      "Train Epoch: 27 [30000/60000 (50%)]\tLoss: 121.504287\n",
      "Train Epoch: 27 [40000/60000 (67%)]\tLoss: 123.307666\n",
      "Train Epoch: 27 [50000/60000 (83%)]\tLoss: 125.356406\n",
      "====> Epoch: 27 Average loss: 125.9088\n",
      "====> Test set loss: 128.5171\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 128.642559\n",
      "Train Epoch: 28 [10000/60000 (17%)]\tLoss: 126.216660\n",
      "Train Epoch: 28 [20000/60000 (33%)]\tLoss: 129.876895\n",
      "Train Epoch: 28 [30000/60000 (50%)]\tLoss: 128.646914\n",
      "Train Epoch: 28 [40000/60000 (67%)]\tLoss: 122.732490\n",
      "Train Epoch: 28 [50000/60000 (83%)]\tLoss: 123.763721\n",
      "====> Epoch: 28 Average loss: 125.7896\n",
      "====> Test set loss: 128.3136\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 127.001387\n",
      "Train Epoch: 29 [10000/60000 (17%)]\tLoss: 126.061611\n",
      "Train Epoch: 29 [20000/60000 (33%)]\tLoss: 122.439102\n",
      "Train Epoch: 29 [30000/60000 (50%)]\tLoss: 127.728223\n",
      "Train Epoch: 29 [40000/60000 (67%)]\tLoss: 125.491357\n",
      "Train Epoch: 29 [50000/60000 (83%)]\tLoss: 122.404219\n",
      "====> Epoch: 29 Average loss: 125.6653\n",
      "====> Test set loss: 128.5505\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 127.354287\n",
      "Train Epoch: 30 [10000/60000 (17%)]\tLoss: 127.644189\n",
      "Train Epoch: 30 [20000/60000 (33%)]\tLoss: 127.298037\n",
      "Train Epoch: 30 [30000/60000 (50%)]\tLoss: 125.605518\n",
      "Train Epoch: 30 [40000/60000 (67%)]\tLoss: 125.036865\n",
      "Train Epoch: 30 [50000/60000 (83%)]\tLoss: 131.885869\n",
      "====> Epoch: 30 Average loss: 125.5467\n",
      "====> Test set loss: 128.2296\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 125.264766\n",
      "Train Epoch: 31 [10000/60000 (17%)]\tLoss: 129.184844\n",
      "Train Epoch: 31 [20000/60000 (33%)]\tLoss: 128.072627\n",
      "Train Epoch: 31 [30000/60000 (50%)]\tLoss: 126.338174\n",
      "Train Epoch: 31 [40000/60000 (67%)]\tLoss: 124.268262\n",
      "Train Epoch: 31 [50000/60000 (83%)]\tLoss: 129.074316\n",
      "====> Epoch: 31 Average loss: 125.4234\n",
      "====> Test set loss: 128.3858\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 128.694775\n",
      "Train Epoch: 32 [10000/60000 (17%)]\tLoss: 123.520039\n",
      "Train Epoch: 32 [20000/60000 (33%)]\tLoss: 126.022793\n",
      "Train Epoch: 32 [30000/60000 (50%)]\tLoss: 123.596699\n",
      "Train Epoch: 32 [40000/60000 (67%)]\tLoss: 118.947705\n",
      "Train Epoch: 32 [50000/60000 (83%)]\tLoss: 127.419951\n",
      "====> Epoch: 32 Average loss: 125.2659\n",
      "====> Test set loss: 128.3764\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 128.069258\n",
      "Train Epoch: 33 [10000/60000 (17%)]\tLoss: 123.618213\n",
      "Train Epoch: 33 [20000/60000 (33%)]\tLoss: 134.670859\n",
      "Train Epoch: 33 [30000/60000 (50%)]\tLoss: 127.238086\n",
      "Train Epoch: 33 [40000/60000 (67%)]\tLoss: 123.250986\n",
      "Train Epoch: 33 [50000/60000 (83%)]\tLoss: 125.325303\n",
      "====> Epoch: 33 Average loss: 125.0631\n",
      "====> Test set loss: 128.4131\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 128.386172\n",
      "Train Epoch: 34 [10000/60000 (17%)]\tLoss: 120.471650\n",
      "Train Epoch: 34 [20000/60000 (33%)]\tLoss: 120.923965\n",
      "Train Epoch: 34 [30000/60000 (50%)]\tLoss: 125.213623\n",
      "Train Epoch: 34 [40000/60000 (67%)]\tLoss: 122.308271\n",
      "Train Epoch: 34 [50000/60000 (83%)]\tLoss: 127.972354\n",
      "====> Epoch: 34 Average loss: 125.0016\n",
      "====> Test set loss: 128.3509\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 131.325928\n",
      "Train Epoch: 35 [10000/60000 (17%)]\tLoss: 127.472305\n",
      "Train Epoch: 35 [20000/60000 (33%)]\tLoss: 118.767764\n",
      "Train Epoch: 35 [30000/60000 (50%)]\tLoss: 128.385029\n",
      "Train Epoch: 35 [40000/60000 (67%)]\tLoss: 121.425938\n",
      "Train Epoch: 35 [50000/60000 (83%)]\tLoss: 119.929307\n",
      "====> Epoch: 35 Average loss: 124.8660\n",
      "====> Test set loss: 128.2001\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 123.396172\n",
      "Train Epoch: 36 [10000/60000 (17%)]\tLoss: 133.797383\n",
      "Train Epoch: 36 [20000/60000 (33%)]\tLoss: 129.277324\n",
      "Train Epoch: 36 [30000/60000 (50%)]\tLoss: 125.038281\n",
      "Train Epoch: 36 [40000/60000 (67%)]\tLoss: 120.595215\n",
      "Train Epoch: 36 [50000/60000 (83%)]\tLoss: 114.748027\n",
      "====> Epoch: 36 Average loss: 124.7139\n",
      "====> Test set loss: 128.3186\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 127.126660\n",
      "Train Epoch: 37 [10000/60000 (17%)]\tLoss: 127.063955\n",
      "Train Epoch: 37 [20000/60000 (33%)]\tLoss: 123.584580\n",
      "Train Epoch: 37 [30000/60000 (50%)]\tLoss: 116.782559\n",
      "Train Epoch: 37 [40000/60000 (67%)]\tLoss: 128.868867\n",
      "Train Epoch: 37 [50000/60000 (83%)]\tLoss: 123.167500\n",
      "====> Epoch: 37 Average loss: 124.5788\n",
      "====> Test set loss: 128.0800\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 126.667949\n",
      "Train Epoch: 38 [10000/60000 (17%)]\tLoss: 123.308896\n",
      "Train Epoch: 38 [20000/60000 (33%)]\tLoss: 127.306025\n",
      "Train Epoch: 38 [30000/60000 (50%)]\tLoss: 127.187871\n",
      "Train Epoch: 38 [40000/60000 (67%)]\tLoss: 129.189238\n",
      "Train Epoch: 38 [50000/60000 (83%)]\tLoss: 123.913145\n",
      "====> Epoch: 38 Average loss: 124.4862\n",
      "====> Test set loss: 128.2947\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 125.417881\n",
      "Train Epoch: 39 [10000/60000 (17%)]\tLoss: 120.256533\n",
      "Train Epoch: 39 [20000/60000 (33%)]\tLoss: 129.392031\n",
      "Train Epoch: 39 [30000/60000 (50%)]\tLoss: 122.073135\n",
      "Train Epoch: 39 [40000/60000 (67%)]\tLoss: 129.188340\n",
      "Train Epoch: 39 [50000/60000 (83%)]\tLoss: 121.813008\n",
      "====> Epoch: 39 Average loss: 124.3919\n",
      "====> Test set loss: 127.8743\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 126.797041\n",
      "Train Epoch: 40 [10000/60000 (17%)]\tLoss: 124.435947\n",
      "Train Epoch: 40 [20000/60000 (33%)]\tLoss: 129.887373\n",
      "Train Epoch: 40 [30000/60000 (50%)]\tLoss: 126.414316\n",
      "Train Epoch: 40 [40000/60000 (67%)]\tLoss: 130.190156\n",
      "Train Epoch: 40 [50000/60000 (83%)]\tLoss: 131.702061\n",
      "====> Epoch: 40 Average loss: 124.3217\n",
      "====> Test set loss: 128.0463\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 136.873193\n",
      "Train Epoch: 41 [10000/60000 (17%)]\tLoss: 127.952500\n",
      "Train Epoch: 41 [20000/60000 (33%)]\tLoss: 124.751777\n",
      "Train Epoch: 41 [30000/60000 (50%)]\tLoss: 126.058477\n",
      "Train Epoch: 41 [40000/60000 (67%)]\tLoss: 124.321377\n",
      "Train Epoch: 41 [50000/60000 (83%)]\tLoss: 124.706475\n",
      "====> Epoch: 41 Average loss: 124.1838\n",
      "====> Test set loss: 128.1307\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 120.565840\n",
      "Train Epoch: 42 [10000/60000 (17%)]\tLoss: 129.315654\n",
      "Train Epoch: 42 [20000/60000 (33%)]\tLoss: 122.348086\n",
      "Train Epoch: 42 [30000/60000 (50%)]\tLoss: 125.103203\n",
      "Train Epoch: 42 [40000/60000 (67%)]\tLoss: 128.402041\n",
      "Train Epoch: 42 [50000/60000 (83%)]\tLoss: 116.686660\n",
      "====> Epoch: 42 Average loss: 124.0336\n",
      "====> Test set loss: 127.9660\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 123.082119\n",
      "Train Epoch: 43 [10000/60000 (17%)]\tLoss: 118.930703\n",
      "Train Epoch: 43 [20000/60000 (33%)]\tLoss: 131.931855\n",
      "Train Epoch: 43 [30000/60000 (50%)]\tLoss: 124.649648\n",
      "Train Epoch: 43 [40000/60000 (67%)]\tLoss: 120.733437\n",
      "Train Epoch: 43 [50000/60000 (83%)]\tLoss: 120.097949\n",
      "====> Epoch: 43 Average loss: 123.9648\n",
      "====> Test set loss: 127.9095\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 121.540752\n",
      "Train Epoch: 44 [10000/60000 (17%)]\tLoss: 129.078027\n",
      "Train Epoch: 44 [20000/60000 (33%)]\tLoss: 115.885479\n",
      "Train Epoch: 44 [30000/60000 (50%)]\tLoss: 125.041123\n",
      "Train Epoch: 44 [40000/60000 (67%)]\tLoss: 127.555127\n",
      "Train Epoch: 44 [50000/60000 (83%)]\tLoss: 117.344902\n",
      "====> Epoch: 44 Average loss: 123.9247\n",
      "====> Test set loss: 128.1561\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 121.259629\n",
      "Train Epoch: 45 [10000/60000 (17%)]\tLoss: 121.824307\n",
      "Train Epoch: 45 [20000/60000 (33%)]\tLoss: 124.923965\n",
      "Train Epoch: 45 [30000/60000 (50%)]\tLoss: 128.717520\n",
      "Train Epoch: 45 [40000/60000 (67%)]\tLoss: 129.636543\n",
      "Train Epoch: 45 [50000/60000 (83%)]\tLoss: 118.722588\n",
      "====> Epoch: 45 Average loss: 123.8376\n",
      "====> Test set loss: 128.0307\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 121.552148\n",
      "Train Epoch: 46 [10000/60000 (17%)]\tLoss: 115.935576\n",
      "Train Epoch: 46 [20000/60000 (33%)]\tLoss: 122.873281\n",
      "Train Epoch: 46 [30000/60000 (50%)]\tLoss: 115.581045\n",
      "Train Epoch: 46 [40000/60000 (67%)]\tLoss: 124.494854\n",
      "Train Epoch: 46 [50000/60000 (83%)]\tLoss: 126.593369\n",
      "====> Epoch: 46 Average loss: 123.7546\n",
      "====> Test set loss: 128.0998\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 129.864082\n",
      "Train Epoch: 47 [10000/60000 (17%)]\tLoss: 123.712520\n",
      "Train Epoch: 47 [20000/60000 (33%)]\tLoss: 125.013955\n",
      "Train Epoch: 47 [30000/60000 (50%)]\tLoss: 124.542266\n",
      "Train Epoch: 47 [40000/60000 (67%)]\tLoss: 130.095840\n",
      "Train Epoch: 47 [50000/60000 (83%)]\tLoss: 125.528105\n",
      "====> Epoch: 47 Average loss: 123.5982\n",
      "====> Test set loss: 127.9351\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 119.482617\n",
      "Train Epoch: 48 [10000/60000 (17%)]\tLoss: 115.824922\n",
      "Train Epoch: 48 [20000/60000 (33%)]\tLoss: 120.807227\n",
      "Train Epoch: 48 [30000/60000 (50%)]\tLoss: 125.799229\n",
      "Train Epoch: 48 [40000/60000 (67%)]\tLoss: 118.469111\n",
      "Train Epoch: 48 [50000/60000 (83%)]\tLoss: 128.801025\n",
      "====> Epoch: 48 Average loss: 123.5076\n",
      "====> Test set loss: 127.8343\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 120.979619\n",
      "Train Epoch: 49 [10000/60000 (17%)]\tLoss: 122.753291\n",
      "Train Epoch: 49 [20000/60000 (33%)]\tLoss: 123.814453\n",
      "Train Epoch: 49 [30000/60000 (50%)]\tLoss: 124.481123\n",
      "Train Epoch: 49 [40000/60000 (67%)]\tLoss: 125.249238\n",
      "Train Epoch: 49 [50000/60000 (83%)]\tLoss: 117.407480\n",
      "====> Epoch: 49 Average loss: 123.4223\n",
      "====> Test set loss: 128.0586\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 131.416465\n",
      "Train Epoch: 50 [10000/60000 (17%)]\tLoss: 117.020098\n",
      "Train Epoch: 50 [20000/60000 (33%)]\tLoss: 122.405059\n",
      "Train Epoch: 50 [30000/60000 (50%)]\tLoss: 123.056963\n",
      "Train Epoch: 50 [40000/60000 (67%)]\tLoss: 127.425371\n",
      "Train Epoch: 50 [50000/60000 (83%)]\tLoss: 119.290703\n",
      "====> Epoch: 50 Average loss: 123.3287\n",
      "====> Test set loss: 127.8780\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train\n",
    "for epoch in range(1, 51):\n",
    "    train(epoch)\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb86053abe0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# with torch.no_grad():\n",
    "#     z = torch.randn(10, 2).cuda()\n",
    "#     c = torch.eye(10).cuda()\n",
    "\n",
    "#     sample = cvae.decoder(z, c)\n",
    "#     save_image(sample.view(10, 1, 28, 28), '../samples/sample_' + '.png')\n",
    "img = sample[9].cpu().reshape(1, 28, 28)\n",
    "\n",
    "plt.imshow(img.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
