{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f48a68f3050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.distributions import Independent, Normal\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure as ssim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "PRINT_REQ= False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "BATCH_SIZE=256\n",
    "EPOCHS=150\n",
    "\n",
    "cond_shape=10\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size = 128\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_data = datasets.MNIST(root=\"../data\", train=True, \n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(root=\"../data\", train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True, num_workers=2)\n",
    "\n",
    "flat_img=torch.flatten(train_data[0][0])\n",
    "flat_shape=list(flat_img.shape)\n",
    "flat_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_debug(data):\n",
    "    if PRINT_REQ:\n",
    "        print(data)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_x, dim_y, dim_z):\n",
    "\n",
    "        super().__init__()\n",
    "         # Encoder layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=11, out_channels=32, kernel_size=5, stride=1,padding='same')\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=2,padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1,padding='same')\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, stride=2,padding=0)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=80, kernel_size=7, stride=1,padding='valid')\n",
    "        self.lin1 = nn.Linear(in_features=80, out_features=20)\n",
    "        self.lin2 = nn.Linear(in_features=80, out_features=20)\n",
    "\n",
    "        # reparameterization\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = inputs[0].to(device)\n",
    "        y = inputs[1].to(device)\n",
    "        \n",
    "        # y = F.one_hot(y, 10).to(device)\n",
    "        y = y.view(-1, 10, 1, 1).to(device)\n",
    "        \n",
    "        ones = torch.ones(x.size()[0], \n",
    "                            10,\n",
    "                            x.size()[2], \n",
    "                            x.size()[3], \n",
    "                            dtype=x.dtype).to(device)\n",
    "        y = ones * y\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        \n",
    "        print_debug(f\"input shape: {x.shape}\")\n",
    "        x = F.relu(self.conv1(x))\n",
    "        print_debug(x.shape)\n",
    "        # 32, 28, 28\n",
    "        x = F.pad(x, (0,3,0,3))\n",
    "        print_debug(x.shape)\n",
    "        # 32, 31, 31\n",
    "        x = F.relu(self.conv2(x))\n",
    "        print_debug(x.shape)\n",
    "        # 32, 14, 14\n",
    "        x = F.relu(self.conv3(x))\n",
    "        print_debug(x.shape)\n",
    "        # 64, 14, 14\n",
    "        x = F.pad(x, (0,3,0,3))\n",
    "        print_debug(x.shape)\n",
    "        # 64, 17, 17\n",
    "        x = F.relu(self.conv4(x))\n",
    "        print_debug(x.shape)\n",
    "        # 64, 7, 7\n",
    "        x = F.relu(self.conv5(x))\n",
    "        print_debug(x.shape)\n",
    "        # 80, 1, 1\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "        print_debug(f\"After flatten shape: {x.shape}\")\n",
    "        # 80\n",
    "        # print_debug(f\"Concatenating {x.shape} with {y.shape}\")\n",
    "        # concat = torch.cat([x, y], dim=-1)\n",
    "        # print_debug(f\"After concatenation shape: {concat.shape}\")\n",
    "        # 90\n",
    "        # loc=torch.zeros(mu_logvar.shape)\n",
    "        # scale=torch.ones(mu_logvar.shape)\n",
    "        # diagn = Independent(Normal(loc, scale), 1)\n",
    "        mu = self.lin1(x)\n",
    "        print_debug(f\"mu shape: {mu.shape}\")\n",
    "        # 20\n",
    "        logvar = self.lin2(x)\n",
    "        print_debug(f\"logvar shape: {logvar.shape}\")\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        print_debug(f\"Returning shape {z.shape}\")\n",
    "        return  mu, logvar, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim_y, dim_z):\n",
    "        super().__init__()\n",
    "        self.dim_z = dim_z\n",
    "        self.dim_y = dim_y\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=30, out_channels=64, kernel_size=7, stride=1, padding=0) # valid means no pad\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=5, stride=1, padding=2)\n",
    "        self.deconv3 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=5, stride=2, padding=2, output_padding=1) # pad operation added in forward\n",
    "        self.deconv4 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.deconv5 = nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=5, stride=2, padding=2, output_padding=1) # pad operation added in forward\n",
    "        self.deconv6 = nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=5, stride=1,padding='same')\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs[0].to(device)#.unsqueeze(dim=0)\n",
    "        y = inputs[1].to(device)\n",
    "        print_debug(f\"latent space shape: {x.shape}, labels shape: {y.shape}\")\n",
    "        x = torch.cat([x, y], dim=1)\n",
    "        x = torch.reshape(x, (-1, self.dim_z+self.dim_y, 1, 1))\n",
    "        print_debug(f\"After concatenation shape: {x.shape}\")\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        print_debug(f\"ConvTrans1 output shape: {x.shape}\")\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        print_debug(f\"ConvTrans2 output shape: {x.shape}\")\n",
    "        x = F.pad(x, (0,0,0,0))\n",
    "        x = F.relu(self.deconv3(x))\n",
    "        print_debug(f\"ConvTrans3 output shape: {x.shape}\")\n",
    "        x = F.relu(self.deconv4(x))\n",
    "        print_debug(f\"ConvTrans4 output shape: {x.shape}\")\n",
    "        # x = F.pad(x, (0,3,0,3))\n",
    "        x = F.relu(self.deconv5(x))\n",
    "        print_debug(f\"ConvTrans5 output shape: {x.shape}\")\n",
    "        x = F.relu(self.deconv6(x))\n",
    "        print_debug(f\"ConvTrans6 output shape: {x.shape}\")\n",
    "        x = torch.sigmoid(self.conv(x))\n",
    "        print_debug(f\"Conv output shape: {x.shape}\")\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "        print_debug(f\"After flatten shape: {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, dim_x, dim_y, dim_z):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=flat_shape[0], out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=dim_y),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        #Encoder \n",
    "        self.encoder = Encoder(dim_x=dim_x, dim_y=dim_y, dim_z=dim_z)\n",
    "\n",
    "        #Decoder\n",
    "        self.decoder = Decoder(dim_y=dim_y, dim_z=dim_z)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x, y = inputs      \n",
    "        x = x.to(device)\n",
    "        y = F.one_hot(y, 10).to(device)  \n",
    "        print_debug(f\"Inputs shape: {x.shape} and labels: {y.shape}\")\n",
    "        c_out = self.classifier(x)\n",
    "        mu, logvar, z = self.encoder((x,y))\n",
    "        out = self.decoder((z, y))\n",
    "        print_debug(f\"decoder output shape is: {out.shape}\")\n",
    "        return mu, logvar, out, c_out\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CVAE(dim_x=(28, 28, 1), dim_y=10, dim_z=20).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_fn(recon, x, mu, logvar, c_out, y_onehot):\n",
    "    y_onehot1 = y_onehot.type(torch.FloatTensor).to(device)\n",
    "    # print(c_out.shape, y_onehot.shape, c_out.dtype, y_onehot.dtype)\n",
    "    classif_loss = torch.nn.BCELoss()(c_out, y_onehot1)\n",
    "    BCE = F.binary_cross_entropy(recon, x, reduction='sum')        \n",
    "    KLD = -0.5*torch.sum(1+logvar-mu.pow(2)-logvar.exp())\n",
    "    return classif_loss+BCE+KLD, classif_loss, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one iteration to validate output shapes with PRINT_DEBUG = True\n",
    "for i, (x, y) in enumerate(train_dataloader):    \n",
    "    model((x,y))\n",
    "    # x = x.to(device)\n",
    "    # print(f\"ysghape is {y.shape}\")\n",
    "    # y = F.one_hot(y, 10).to(device)\n",
    "    # y = y.view(-1, 10, 1, 1).to(device)\n",
    "    \n",
    "    # ones = torch.ones(x.size()[0], \n",
    "    #                     10,\n",
    "    #                     x.size()[2], \n",
    "    #                     x.size()[3], \n",
    "    #                     dtype=x.dtype).to(device)\n",
    "    # y = ones * y\n",
    "    # print(ones.shape, y.shape)\n",
    "    # x = torch.cat((x, y), dim=1)\n",
    "    # print(x.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Truth labels for predictions.\n",
    "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
    "\n",
    "    Returns:\n",
    "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
    "    \"\"\"\n",
    "    \n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    classif_accuracy = 0\n",
    "    BCE_log = list()\n",
    "    KLD_log = list()\n",
    "    classif_log = list()\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X = X.to(device) #[64, 1, 28, 28]\n",
    "        y = y.to(device)\n",
    "        \n",
    "        \n",
    "        # 1. Forward pass\n",
    "        mu, logvar, recon_batch, c_out = model((X, y))\n",
    "        # print(f\"---------------{torch.argmax(c_out, dim=1).shape}\")\n",
    "        # print(f\"---------------{y.shape}\")\n",
    "        flat_data = X.view(-1, flat_shape[0]).to(device)                            \n",
    "        y_onehot = F.one_hot(y, cond_shape).to(device)\n",
    "        inp = torch.cat((flat_data, y_onehot), 1)\n",
    "        \n",
    "        # 2. Calculate loss\n",
    "        loss, C_loss, BCE, KLD = loss_fn(recon_batch, flat_data, mu, logvar, c_out, y_onehot)\n",
    "        train_loss += loss.item()\n",
    "        BCE_log.append(BCE)\n",
    "        KLD_log.append(KLD)\n",
    "        classif_log.append(C_loss)\n",
    "        classif_accuracy += accuracy_fn(y, torch.argmax(c_out, dim=1))\n",
    "        \n",
    "        \n",
    "\n",
    "        # 3. Zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Backprop\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Step\n",
    "        optimizer.step()\n",
    "    \n",
    "        if batch % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tBCE:{:.4f}\\tKLD:{:.4f}\\tC_loss:{:.4f}'.format(\n",
    "                epoch,\n",
    "                batch * len(X),\n",
    "                len(train_dataloader.dataset),\n",
    "                100. * batch / len(train_dataloader),\n",
    "                loss.item() / len(X), BCE.item() / len(X), KLD.item() / len(X), C_loss.item() / len(X)))\n",
    "    # classif_accuracy /= len(train_dataloader)\n",
    "    print('====> Epoch: {} Average loss: {:.4f}\\tClassifier Accuracy: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_dataloader.dataset), classif_accuracy/len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    #Sets the module in evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, (X, y) in enumerate(test_dataloader):\n",
    "            X = X.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            mu, logvar, recon_batch, c_out = model((X, y))\n",
    "            \n",
    "            flat_data = X.view(-1, flat_shape[0]).to(device)\n",
    "            y_onehot = F.one_hot(y, cond_shape).to(device)\n",
    "            inp = torch.cat((flat_data, y_onehot), 1)\n",
    "\n",
    "            # 2. Loss\n",
    "            tot_loss, C_loss, BCE, KLD = loss_fn(recon_batch, flat_data, mu, logvar, c_out, y_onehot)\n",
    "            test_loss += tot_loss.item()\n",
    "\n",
    "            # 3. Save images\n",
    "            if epoch%5==0 and i == 0:\n",
    "                n = min(X.size(0), 8)\n",
    "                recon_image = recon_batch[:, 0:recon_batch.shape[1]]\n",
    "                print(recon_image.shape)\n",
    "                recon_image = recon_image.view(BATCH_SIZE, 1, 28,28)\n",
    "                print('---',recon_image.shape)\n",
    "                comparison = torch.cat([X[:n],\n",
    "                                      recon_image.view(BATCH_SIZE, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.261414\tBCE:550.1899\tKLD:0.0702\tC_loss:0.0013\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 406.931580\tBCE:406.8782\tKLD:0.0523\tC_loss:0.0010\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 351.483185\tBCE:351.4215\tKLD:0.0609\tC_loss:0.0007\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 286.758301\tBCE:286.6847\tKLD:0.0731\tC_loss:0.0005\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 232.082733\tBCE:231.9910\tKLD:0.0914\tC_loss:0.0004\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 210.742844\tBCE:210.6472\tKLD:0.0953\tC_loss:0.0003\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 200.601379\tBCE:200.5148\tKLD:0.0863\tC_loss:0.0003\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 195.218430\tBCE:195.1197\tKLD:0.0984\tC_loss:0.0003\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 192.613327\tBCE:192.3486\tKLD:0.2646\tC_loss:0.0002\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 191.658264\tBCE:191.5774\tKLD:0.0806\tC_loss:0.0003\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 183.900787\tBCE:183.8277\tKLD:0.0729\tC_loss:0.0002\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 183.550018\tBCE:183.4224\tKLD:0.1273\tC_loss:0.0003\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 179.721802\tBCE:179.5769\tKLD:0.1447\tC_loss:0.0002\n",
      "Train Epoch: 1 [33280/60000 (56%)]\tLoss: 182.174759\tBCE:181.9752\tKLD:0.1994\tC_loss:0.0002\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 177.418427\tBCE:175.4112\tKLD:2.0069\tC_loss:0.0003\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 174.754028\tBCE:170.4753\tKLD:4.2785\tC_loss:0.0002\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 166.818542\tBCE:162.5656\tKLD:4.2528\tC_loss:0.0002\n",
      "Train Epoch: 1 [43520/60000 (73%)]\tLoss: 164.177734\tBCE:159.1809\tKLD:4.9966\tC_loss:0.0002\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 155.378555\tBCE:150.3553\tKLD:5.0230\tC_loss:0.0002\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 159.810349\tBCE:153.9851\tKLD:5.8251\tC_loss:0.0002\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 151.139893\tBCE:146.2073\tKLD:4.9324\tC_loss:0.0002\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 153.424042\tBCE:147.6902\tKLD:5.7336\tC_loss:0.0002\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 145.301025\tBCE:138.6152\tKLD:6.6856\tC_loss:0.0002\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 143.421967\tBCE:137.1270\tKLD:6.2948\tC_loss:0.0002\n",
      "====> Epoch: 1 Average loss: 207.7665\tClassifier Accuracy: 86.6403\n",
      "====> Test set loss: 147.6961\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 146.930145\tBCE:140.7284\tKLD:6.2016\tC_loss:0.0002\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 146.585007\tBCE:139.1979\tKLD:7.3869\tC_loss:0.0002\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 146.972458\tBCE:139.1195\tKLD:7.8528\tC_loss:0.0001\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 137.250748\tBCE:130.0424\tKLD:7.2082\tC_loss:0.0001\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 140.471283\tBCE:132.7881\tKLD:7.6830\tC_loss:0.0002\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 139.942383\tBCE:131.2213\tKLD:8.7209\tC_loss:0.0002\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 137.680939\tBCE:128.9566\tKLD:8.7241\tC_loss:0.0002\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 137.805511\tBCE:127.9427\tKLD:9.8626\tC_loss:0.0002\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 134.993408\tBCE:124.7097\tKLD:10.2835\tC_loss:0.0002\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 137.296555\tBCE:126.8182\tKLD:10.4782\tC_loss:0.0002\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 134.600723\tBCE:123.9335\tKLD:10.6670\tC_loss:0.0002\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 129.481689\tBCE:119.6657\tKLD:9.8158\tC_loss:0.0002\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 128.510010\tBCE:117.4294\tKLD:11.0805\tC_loss:0.0001\n",
      "Train Epoch: 2 [33280/60000 (56%)]\tLoss: 129.386612\tBCE:118.2978\tKLD:11.0886\tC_loss:0.0002\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 132.745071\tBCE:121.8230\tKLD:10.9220\tC_loss:0.0001\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 124.943520\tBCE:114.0740\tKLD:10.8694\tC_loss:0.0001\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 127.434914\tBCE:115.8335\tKLD:11.6013\tC_loss:0.0002\n",
      "Train Epoch: 2 [43520/60000 (73%)]\tLoss: 118.673828\tBCE:107.2829\tKLD:11.3907\tC_loss:0.0001\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 122.849953\tBCE:110.7177\tKLD:12.1321\tC_loss:0.0002\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 122.447876\tBCE:111.1815\tKLD:11.2663\tC_loss:0.0001\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 118.302719\tBCE:106.1818\tKLD:12.1208\tC_loss:0.0001\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 117.322510\tBCE:105.6914\tKLD:11.6310\tC_loss:0.0001\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 119.343811\tBCE:107.6529\tKLD:11.6908\tC_loss:0.0001\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 119.203003\tBCE:107.3971\tKLD:11.8058\tC_loss:0.0001\n",
      "====> Epoch: 2 Average loss: 131.2522\tClassifier Accuracy: 93.4412\n",
      "====> Test set loss: 120.0906\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 120.037437\tBCE:107.8948\tKLD:12.1426\tC_loss:0.0001\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 120.868073\tBCE:108.2829\tKLD:12.5851\tC_loss:0.0001\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 118.491928\tBCE:105.7475\tKLD:12.7443\tC_loss:0.0001\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 121.579964\tBCE:108.7448\tKLD:12.8350\tC_loss:0.0001\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 122.746147\tBCE:110.0190\tKLD:12.7270\tC_loss:0.0002\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 117.013496\tBCE:104.5735\tKLD:12.4398\tC_loss:0.0002\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 117.739937\tBCE:104.4176\tKLD:13.3222\tC_loss:0.0001\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 117.528450\tBCE:104.3607\tKLD:13.1677\tC_loss:0.0001\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 116.315750\tBCE:102.9137\tKLD:13.4019\tC_loss:0.0001\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 119.542015\tBCE:106.9380\tKLD:12.6038\tC_loss:0.0001\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 116.834305\tBCE:103.3536\tKLD:13.4806\tC_loss:0.0001\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 114.503067\tBCE:101.5451\tKLD:12.9579\tC_loss:0.0001\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 116.282722\tBCE:102.8089\tKLD:13.4737\tC_loss:0.0001\n",
      "Train Epoch: 3 [33280/60000 (56%)]\tLoss: 115.494980\tBCE:102.1488\tKLD:13.3461\tC_loss:0.0001\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 115.276428\tBCE:102.3765\tKLD:12.8998\tC_loss:0.0001\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 112.693115\tBCE:99.4389\tKLD:13.2541\tC_loss:0.0001\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 114.382721\tBCE:101.1694\tKLD:13.2132\tC_loss:0.0001\n",
      "Train Epoch: 3 [43520/60000 (73%)]\tLoss: 114.116920\tBCE:100.8328\tKLD:13.2840\tC_loss:0.0001\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 112.917976\tBCE:99.4510\tKLD:13.4668\tC_loss:0.0001\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 116.758034\tBCE:102.8388\tKLD:13.9191\tC_loss:0.0001\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 110.574226\tBCE:96.8701\tKLD:13.7040\tC_loss:0.0001\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 108.787971\tBCE:94.8322\tKLD:13.9557\tC_loss:0.0001\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 109.250122\tBCE:95.7917\tKLD:13.4584\tC_loss:0.0001\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 108.752899\tBCE:94.9761\tKLD:13.7766\tC_loss:0.0001\n",
      "====> Epoch: 3 Average loss: 115.0374\tClassifier Accuracy: 94.9870\n",
      "====> Test set loss: 110.3856\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 111.988190\tBCE:98.3853\tKLD:13.6028\tC_loss:0.0001\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 113.583511\tBCE:99.5154\tKLD:14.0680\tC_loss:0.0001\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 114.414154\tBCE:100.1830\tKLD:14.2310\tC_loss:0.0001\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 110.329620\tBCE:96.3668\tKLD:13.9627\tC_loss:0.0001\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 110.851189\tBCE:96.5246\tKLD:14.3265\tC_loss:0.0001\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 108.185028\tBCE:94.6190\tKLD:13.5660\tC_loss:0.0001\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 108.805275\tBCE:95.3542\tKLD:13.4510\tC_loss:0.0001\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 111.008614\tBCE:96.9299\tKLD:14.0786\tC_loss:0.0001\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 108.915024\tBCE:94.9277\tKLD:13.9873\tC_loss:0.0001\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 108.360191\tBCE:94.7068\tKLD:13.6533\tC_loss:0.0001\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 108.249741\tBCE:94.6516\tKLD:13.5981\tC_loss:0.0001\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 107.877251\tBCE:93.7696\tKLD:14.1075\tC_loss:0.0001\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 105.694313\tBCE:91.6219\tKLD:14.0724\tC_loss:0.0001\n",
      "Train Epoch: 4 [33280/60000 (56%)]\tLoss: 108.066025\tBCE:93.9776\tKLD:14.0883\tC_loss:0.0001\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 110.802666\tBCE:97.2382\tKLD:13.5644\tC_loss:0.0001\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 106.105492\tBCE:91.9132\tKLD:14.1922\tC_loss:0.0001\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 108.158829\tBCE:94.1445\tKLD:14.0142\tC_loss:0.0001\n",
      "Train Epoch: 4 [43520/60000 (73%)]\tLoss: 106.882652\tBCE:92.8391\tKLD:14.0435\tC_loss:0.0001\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 107.517250\tBCE:93.1090\tKLD:14.4081\tC_loss:0.0001\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 110.017181\tBCE:95.8073\tKLD:14.2098\tC_loss:0.0001\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 106.456444\tBCE:92.8806\tKLD:13.5757\tC_loss:0.0001\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 106.250771\tBCE:92.0686\tKLD:14.1821\tC_loss:0.0001\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 107.911354\tBCE:93.9075\tKLD:14.0037\tC_loss:0.0001\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 106.868706\tBCE:92.7216\tKLD:14.1471\tC_loss:0.0001\n",
      "====> Epoch: 4 Average loss: 108.8944\tClassifier Accuracy: 95.8634\n",
      "====> Test set loss: 106.2886\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 108.989365\tBCE:94.3024\tKLD:14.6868\tC_loss:0.0001\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 108.417938\tBCE:93.5853\tKLD:14.8326\tC_loss:0.0001\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 104.549522\tBCE:89.9176\tKLD:14.6319\tC_loss:0.0001\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 104.470909\tBCE:90.5243\tKLD:13.9466\tC_loss:0.0001\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 107.259743\tBCE:92.5854\tKLD:14.6742\tC_loss:0.0001\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 110.421936\tBCE:95.9412\tKLD:14.4807\tC_loss:0.0001\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 103.941994\tBCE:88.9897\tKLD:14.9523\tC_loss:0.0000\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 106.070915\tBCE:91.5004\tKLD:14.5704\tC_loss:0.0001\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 106.951080\tBCE:92.2749\tKLD:14.6761\tC_loss:0.0001\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 107.188751\tBCE:92.0401\tKLD:15.1485\tC_loss:0.0001\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 105.801346\tBCE:91.0295\tKLD:14.7717\tC_loss:0.0001\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 106.931000\tBCE:92.2582\tKLD:14.6727\tC_loss:0.0001\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 102.155914\tBCE:87.8576\tKLD:14.2982\tC_loss:0.0001\n",
      "Train Epoch: 5 [33280/60000 (56%)]\tLoss: 106.519981\tBCE:91.8098\tKLD:14.7101\tC_loss:0.0001\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 101.610878\tBCE:87.1598\tKLD:14.4510\tC_loss:0.0001\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 110.852310\tBCE:95.6540\tKLD:15.1983\tC_loss:0.0001\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 107.570320\tBCE:93.1159\tKLD:14.4543\tC_loss:0.0001\n",
      "Train Epoch: 5 [43520/60000 (73%)]\tLoss: 108.444588\tBCE:93.4028\tKLD:15.0417\tC_loss:0.0001\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 102.999359\tBCE:88.4040\tKLD:14.5953\tC_loss:0.0001\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 106.264000\tBCE:90.9658\tKLD:15.2981\tC_loss:0.0001\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 107.583801\tBCE:92.3063\tKLD:15.2774\tC_loss:0.0001\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 101.056129\tBCE:86.9777\tKLD:14.0783\tC_loss:0.0001\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 105.811523\tBCE:90.7266\tKLD:15.0849\tC_loss:0.0001\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 105.778999\tBCE:91.1302\tKLD:14.6487\tC_loss:0.0001\n",
      "====> Epoch: 5 Average loss: 105.7948\tClassifier Accuracy: 96.5278\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 103.7881\n",
      "Random number: 5\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 102.469269\tBCE:87.8509\tKLD:14.6183\tC_loss:0.0001\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 105.957611\tBCE:91.0257\tKLD:14.9318\tC_loss:0.0001\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 102.689178\tBCE:88.1646\tKLD:14.5245\tC_loss:0.0001\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 105.237473\tBCE:90.3886\tKLD:14.8488\tC_loss:0.0001\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 104.212395\tBCE:88.9804\tKLD:15.2319\tC_loss:0.0000\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 105.528244\tBCE:90.3315\tKLD:15.1967\tC_loss:0.0001\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 103.738068\tBCE:88.7943\tKLD:14.9437\tC_loss:0.0001\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 102.935555\tBCE:87.9242\tKLD:15.0113\tC_loss:0.0001\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 102.652306\tBCE:87.5468\tKLD:15.1054\tC_loss:0.0001\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 100.098495\tBCE:85.1857\tKLD:14.9127\tC_loss:0.0001\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 105.590263\tBCE:90.7619\tKLD:14.8283\tC_loss:0.0001\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 103.946373\tBCE:88.7864\tKLD:15.1599\tC_loss:0.0001\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 105.823112\tBCE:90.4286\tKLD:15.3945\tC_loss:0.0001\n",
      "Train Epoch: 6 [33280/60000 (56%)]\tLoss: 104.272659\tBCE:89.1595\tKLD:15.1131\tC_loss:0.0001\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 104.335060\tBCE:89.4080\tKLD:14.9269\tC_loss:0.0001\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 106.097023\tBCE:91.3028\tKLD:14.7941\tC_loss:0.0001\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 105.816078\tBCE:90.8113\tKLD:15.0047\tC_loss:0.0001\n",
      "Train Epoch: 6 [43520/60000 (73%)]\tLoss: 105.626404\tBCE:90.4116\tKLD:15.2147\tC_loss:0.0001\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 106.150391\tBCE:90.8334\tKLD:15.3169\tC_loss:0.0001\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 100.980721\tBCE:85.9387\tKLD:15.0420\tC_loss:0.0000\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 104.483864\tBCE:89.6567\tKLD:14.8271\tC_loss:0.0001\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 101.977715\tBCE:86.5064\tKLD:15.4712\tC_loss:0.0001\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 102.921402\tBCE:87.8656\tKLD:15.0557\tC_loss:0.0001\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 108.659180\tBCE:93.0057\tKLD:15.6534\tC_loss:0.0001\n",
      "====> Epoch: 6 Average loss: 103.6965\tClassifier Accuracy: 97.0837\n",
      "====> Test set loss: 102.2656\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 100.652260\tBCE:85.4418\tKLD:15.2104\tC_loss:0.0001\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 101.073334\tBCE:86.2174\tKLD:14.8559\tC_loss:0.0001\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 101.706772\tBCE:86.6825\tKLD:15.0242\tC_loss:0.0000\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 101.954575\tBCE:87.5983\tKLD:14.3563\tC_loss:0.0001\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 100.602875\tBCE:85.5338\tKLD:15.0690\tC_loss:0.0001\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 101.928673\tBCE:86.6457\tKLD:15.2830\tC_loss:0.0000\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 104.206017\tBCE:88.9425\tKLD:15.2635\tC_loss:0.0000\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 103.901817\tBCE:88.4729\tKLD:15.4289\tC_loss:0.0001\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 103.132439\tBCE:87.9042\tKLD:15.2282\tC_loss:0.0001\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 102.825859\tBCE:87.3521\tKLD:15.4737\tC_loss:0.0001\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 103.200356\tBCE:87.4926\tKLD:15.7078\tC_loss:0.0000\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 102.176941\tBCE:87.0086\tKLD:15.1683\tC_loss:0.0001\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 104.265198\tBCE:88.8786\tKLD:15.3865\tC_loss:0.0001\n",
      "Train Epoch: 7 [33280/60000 (56%)]\tLoss: 103.259781\tBCE:87.5604\tKLD:15.6993\tC_loss:0.0001\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 102.870201\tBCE:87.6282\tKLD:15.2419\tC_loss:0.0001\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 103.420616\tBCE:87.4998\tKLD:15.9207\tC_loss:0.0001\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 101.715782\tBCE:85.9866\tKLD:15.7291\tC_loss:0.0001\n",
      "Train Epoch: 7 [43520/60000 (73%)]\tLoss: 105.558029\tBCE:90.0250\tKLD:15.5330\tC_loss:0.0001\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 102.690895\tBCE:87.3066\tKLD:15.3843\tC_loss:0.0001\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 104.792191\tBCE:89.2067\tKLD:15.5854\tC_loss:0.0001\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 101.622177\tBCE:86.2831\tKLD:15.3390\tC_loss:0.0001\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 100.400055\tBCE:84.6736\tKLD:15.7265\tC_loss:0.0000\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 103.979729\tBCE:88.7075\tKLD:15.2722\tC_loss:0.0001\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 101.833138\tBCE:86.0690\tKLD:15.7641\tC_loss:0.0000\n",
      "====> Epoch: 7 Average loss: 102.0663\tClassifier Accuracy: 97.5060\n",
      "====> Test set loss: 100.4243\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 100.733696\tBCE:85.5546\tKLD:15.1790\tC_loss:0.0000\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 98.339752\tBCE:82.7486\tKLD:15.5911\tC_loss:0.0000\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 101.035057\tBCE:85.6279\tKLD:15.4071\tC_loss:0.0001\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 104.414268\tBCE:88.7588\tKLD:15.6554\tC_loss:0.0000\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 102.449699\tBCE:86.5692\tKLD:15.8804\tC_loss:0.0001\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 102.552353\tBCE:86.9603\tKLD:15.5919\tC_loss:0.0001\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 103.166435\tBCE:88.4219\tKLD:14.7445\tC_loss:0.0001\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 101.718636\tBCE:85.8314\tKLD:15.8872\tC_loss:0.0001\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 102.481094\tBCE:86.3924\tKLD:16.0886\tC_loss:0.0001\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 99.097870\tBCE:83.7859\tKLD:15.3119\tC_loss:0.0001\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 102.383392\tBCE:86.2356\tKLD:16.1478\tC_loss:0.0000\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 105.969688\tBCE:90.1323\tKLD:15.8373\tC_loss:0.0001\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 99.462906\tBCE:83.7886\tKLD:15.6742\tC_loss:0.0000\n",
      "Train Epoch: 8 [33280/60000 (56%)]\tLoss: 103.644653\tBCE:88.2251\tKLD:15.4195\tC_loss:0.0001\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 100.549324\tBCE:84.2507\tKLD:16.2985\tC_loss:0.0001\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 100.675308\tBCE:84.5607\tKLD:16.1146\tC_loss:0.0000\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 100.504433\tBCE:84.9876\tKLD:15.5167\tC_loss:0.0000\n",
      "Train Epoch: 8 [43520/60000 (73%)]\tLoss: 99.157066\tBCE:83.5341\tKLD:15.6229\tC_loss:0.0001\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 103.471169\tBCE:87.3655\tKLD:16.1057\tC_loss:0.0000\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 103.533432\tBCE:87.6363\tKLD:15.8971\tC_loss:0.0000\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 100.158363\tBCE:84.0363\tKLD:16.1220\tC_loss:0.0000\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 98.736122\tBCE:82.8533\tKLD:15.8828\tC_loss:0.0000\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 99.068550\tBCE:83.4542\tKLD:15.6142\tC_loss:0.0001\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 99.397850\tBCE:83.9968\tKLD:15.4010\tC_loss:0.0000\n",
      "====> Epoch: 8 Average loss: 100.8508\tClassifier Accuracy: 97.7865\n",
      "====> Test set loss: 99.6533\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 101.370598\tBCE:85.5428\tKLD:15.8277\tC_loss:0.0001\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 100.426422\tBCE:84.4907\tKLD:15.9357\tC_loss:0.0000\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 99.760773\tBCE:84.1750\tKLD:15.5857\tC_loss:0.0000\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 101.424004\tBCE:84.8316\tKLD:16.5923\tC_loss:0.0000\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 98.489151\tBCE:83.1443\tKLD:15.3448\tC_loss:0.0000\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 99.103287\tBCE:82.8578\tKLD:16.2454\tC_loss:0.0000\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 98.778511\tBCE:82.5864\tKLD:16.1920\tC_loss:0.0000\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 99.508125\tBCE:83.5385\tKLD:15.9696\tC_loss:0.0001\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 101.138000\tBCE:84.8064\tKLD:16.3315\tC_loss:0.0000\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 99.711166\tBCE:84.1258\tKLD:15.5853\tC_loss:0.0000\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 99.471405\tBCE:83.4661\tKLD:16.0053\tC_loss:0.0001\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 102.185226\tBCE:86.0019\tKLD:16.1833\tC_loss:0.0001\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 101.674805\tBCE:85.7411\tKLD:15.9336\tC_loss:0.0000\n",
      "Train Epoch: 9 [33280/60000 (56%)]\tLoss: 98.271545\tBCE:81.7850\tKLD:16.4865\tC_loss:0.0000\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 102.419754\tBCE:86.4050\tKLD:16.0147\tC_loss:0.0000\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 99.127258\tBCE:83.2367\tKLD:15.8905\tC_loss:0.0001\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 97.901573\tBCE:82.0756\tKLD:15.8259\tC_loss:0.0000\n",
      "Train Epoch: 9 [43520/60000 (73%)]\tLoss: 99.111214\tBCE:83.3094\tKLD:15.8018\tC_loss:0.0001\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 98.317978\tBCE:82.6486\tKLD:15.6694\tC_loss:0.0000\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 99.180527\tBCE:83.7356\tKLD:15.4449\tC_loss:0.0000\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 98.411987\tBCE:82.9224\tKLD:15.4895\tC_loss:0.0000\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 96.256950\tBCE:80.7522\tKLD:15.5047\tC_loss:0.0000\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 101.610992\tBCE:85.6026\tKLD:16.0084\tC_loss:0.0000\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 99.032959\tBCE:83.3120\tKLD:15.7209\tC_loss:0.0000\n",
      "====> Epoch: 9 Average loss: 99.8140\tClassifier Accuracy: 98.0753\n",
      "====> Test set loss: 98.6722\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 100.308830\tBCE:83.9282\tKLD:16.3805\tC_loss:0.0001\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 97.907845\tBCE:82.0036\tKLD:15.9042\tC_loss:0.0000\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 102.430222\tBCE:85.9123\tKLD:16.5179\tC_loss:0.0000\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 102.551659\tBCE:86.9433\tKLD:15.6083\tC_loss:0.0001\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 97.555901\tBCE:81.9540\tKLD:15.6018\tC_loss:0.0000\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 97.125732\tBCE:80.9097\tKLD:16.2160\tC_loss:0.0000\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 98.560547\tBCE:82.7946\tKLD:15.7659\tC_loss:0.0000\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 98.389168\tBCE:82.4484\tKLD:15.9407\tC_loss:0.0000\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 99.473801\tBCE:82.8174\tKLD:16.6563\tC_loss:0.0000\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 96.420105\tBCE:80.5683\tKLD:15.8518\tC_loss:0.0000\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 96.937286\tBCE:81.0692\tKLD:15.8681\tC_loss:0.0000\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 101.350830\tBCE:84.8388\tKLD:16.5120\tC_loss:0.0000\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 96.896164\tBCE:80.6283\tKLD:16.2678\tC_loss:0.0000\n",
      "Train Epoch: 10 [33280/60000 (56%)]\tLoss: 99.211243\tBCE:83.3056\tKLD:15.9056\tC_loss:0.0001\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 99.424095\tBCE:83.0855\tKLD:16.3385\tC_loss:0.0000\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 93.217842\tBCE:77.3320\tKLD:15.8858\tC_loss:0.0000\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 101.742020\tBCE:85.0806\tKLD:16.6613\tC_loss:0.0001\n",
      "Train Epoch: 10 [43520/60000 (73%)]\tLoss: 100.161865\tBCE:84.2498\tKLD:15.9120\tC_loss:0.0000\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 98.642807\tBCE:82.4189\tKLD:16.2239\tC_loss:0.0001\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 102.242935\tBCE:86.0801\tKLD:16.1628\tC_loss:0.0000\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 100.428238\tBCE:83.9319\tKLD:16.4963\tC_loss:0.0001\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 99.720215\tBCE:83.1532\tKLD:16.5669\tC_loss:0.0000\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 98.374374\tBCE:82.8194\tKLD:15.5549\tC_loss:0.0000\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 99.684219\tBCE:83.3577\tKLD:16.3265\tC_loss:0.0000\n",
      "====> Epoch: 10 Average loss: 98.7827\tClassifier Accuracy: 98.3156\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 98.3076\n",
      "Random number: 0\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 100.255302\tBCE:84.3551\tKLD:15.9002\tC_loss:0.0000\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 101.236534\tBCE:84.3005\tKLD:16.9360\tC_loss:0.0000\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 96.919891\tBCE:80.9469\tKLD:15.9730\tC_loss:0.0000\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 97.720306\tBCE:81.1342\tKLD:16.5861\tC_loss:0.0000\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 95.177185\tBCE:79.1953\tKLD:15.9819\tC_loss:0.0000\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 96.134834\tBCE:80.1074\tKLD:16.0274\tC_loss:0.0000\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 98.246552\tBCE:81.4348\tKLD:16.8117\tC_loss:0.0000\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 96.985931\tBCE:80.6951\tKLD:16.2908\tC_loss:0.0000\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 98.611404\tBCE:82.2185\tKLD:16.3928\tC_loss:0.0001\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 97.996086\tBCE:81.3925\tKLD:16.6036\tC_loss:0.0000\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 100.267738\tBCE:83.9167\tKLD:16.3510\tC_loss:0.0001\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 101.772797\tBCE:85.0764\tKLD:16.6963\tC_loss:0.0001\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 97.298065\tBCE:80.7684\tKLD:16.5296\tC_loss:0.0000\n",
      "Train Epoch: 11 [33280/60000 (56%)]\tLoss: 96.679680\tBCE:80.5236\tKLD:16.1560\tC_loss:0.0000\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 95.902252\tBCE:79.4831\tKLD:16.4191\tC_loss:0.0000\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 97.914383\tBCE:81.3235\tKLD:16.5909\tC_loss:0.0000\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 100.790863\tBCE:83.9022\tKLD:16.8886\tC_loss:0.0000\n",
      "Train Epoch: 11 [43520/60000 (73%)]\tLoss: 98.784821\tBCE:82.2690\tKLD:16.5158\tC_loss:0.0000\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 100.065628\tBCE:83.6365\tKLD:16.4291\tC_loss:0.0001\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 94.679886\tBCE:78.4302\tKLD:16.2496\tC_loss:0.0000\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 99.721252\tBCE:83.2915\tKLD:16.4297\tC_loss:0.0000\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 100.319542\tBCE:83.8475\tKLD:16.4720\tC_loss:0.0001\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 99.326965\tBCE:82.9181\tKLD:16.4089\tC_loss:0.0000\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 96.265030\tBCE:80.1003\tKLD:16.1647\tC_loss:0.0000\n",
      "====> Epoch: 11 Average loss: 97.8613\tClassifier Accuracy: 98.4909\n",
      "====> Test set loss: 96.9995\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 98.193565\tBCE:81.1696\tKLD:17.0239\tC_loss:0.0000\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 95.115990\tBCE:78.8202\tKLD:16.2958\tC_loss:0.0000\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 97.714142\tBCE:81.7672\tKLD:15.9469\tC_loss:0.0000\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 96.086761\tBCE:79.6267\tKLD:16.4601\tC_loss:0.0000\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 95.999344\tBCE:79.4206\tKLD:16.5787\tC_loss:0.0000\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 100.095535\tBCE:83.7932\tKLD:16.3023\tC_loss:0.0000\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 94.151634\tBCE:78.2501\tKLD:15.9015\tC_loss:0.0000\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 95.188766\tBCE:79.1218\tKLD:16.0670\tC_loss:0.0000\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 96.886536\tBCE:80.5351\tKLD:16.3514\tC_loss:0.0000\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 96.835762\tBCE:80.2495\tKLD:16.5862\tC_loss:0.0000\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 95.631561\tBCE:79.3385\tKLD:16.2930\tC_loss:0.0000\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 96.440445\tBCE:79.7168\tKLD:16.7236\tC_loss:0.0000\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 101.730400\tBCE:84.7779\tKLD:16.9524\tC_loss:0.0000\n",
      "Train Epoch: 12 [33280/60000 (56%)]\tLoss: 98.818726\tBCE:82.4479\tKLD:16.3708\tC_loss:0.0000\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 97.008736\tBCE:80.9124\tKLD:16.0963\tC_loss:0.0000\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 95.740440\tBCE:79.1533\tKLD:16.5871\tC_loss:0.0000\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 95.845306\tBCE:79.7352\tKLD:16.1100\tC_loss:0.0000\n",
      "Train Epoch: 12 [43520/60000 (73%)]\tLoss: 96.987053\tBCE:79.9955\tKLD:16.9915\tC_loss:0.0000\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 96.759605\tBCE:80.7990\tKLD:15.9606\tC_loss:0.0000\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 95.149811\tBCE:78.2164\tKLD:16.9333\tC_loss:0.0000\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 94.914993\tBCE:78.9145\tKLD:16.0005\tC_loss:0.0000\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 94.323746\tBCE:77.9710\tKLD:16.3527\tC_loss:0.0000\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 99.004868\tBCE:82.7945\tKLD:16.2103\tC_loss:0.0000\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 98.103928\tBCE:81.1883\tKLD:16.9156\tC_loss:0.0000\n",
      "====> Epoch: 12 Average loss: 97.1559\tClassifier Accuracy: 98.7046\n",
      "====> Test set loss: 96.3598\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 98.444916\tBCE:81.6468\tKLD:16.7981\tC_loss:0.0000\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 96.068687\tBCE:79.9208\tKLD:16.1478\tC_loss:0.0000\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 94.426453\tBCE:77.8522\tKLD:16.5742\tC_loss:0.0000\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 96.321014\tBCE:79.0812\tKLD:17.2397\tC_loss:0.0000\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 95.375847\tBCE:78.3278\tKLD:17.0480\tC_loss:0.0000\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 97.669250\tBCE:81.2735\tKLD:16.3957\tC_loss:0.0000\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 96.612061\tBCE:80.1896\tKLD:16.4224\tC_loss:0.0000\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 99.015831\tBCE:82.0110\tKLD:17.0048\tC_loss:0.0000\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 97.819977\tBCE:81.2531\tKLD:16.5669\tC_loss:0.0000\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 96.786774\tBCE:80.2543\tKLD:16.5324\tC_loss:0.0000\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 94.362289\tBCE:77.8939\tKLD:16.4683\tC_loss:0.0000\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 99.084335\tBCE:81.9816\tKLD:17.1027\tC_loss:0.0000\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 95.118820\tBCE:78.6985\tKLD:16.4203\tC_loss:0.0000\n",
      "Train Epoch: 13 [33280/60000 (56%)]\tLoss: 100.309891\tBCE:83.1003\tKLD:17.2096\tC_loss:0.0000\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 97.709106\tBCE:80.6568\tKLD:17.0522\tC_loss:0.0000\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 98.121506\tBCE:81.4391\tKLD:16.6824\tC_loss:0.0000\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 97.985527\tBCE:81.2000\tKLD:16.7855\tC_loss:0.0000\n",
      "Train Epoch: 13 [43520/60000 (73%)]\tLoss: 98.269272\tBCE:81.4021\tKLD:16.8671\tC_loss:0.0000\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 96.533173\tBCE:79.4633\tKLD:17.0699\tC_loss:0.0000\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 98.714569\tBCE:81.5557\tKLD:17.1589\tC_loss:0.0000\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 97.420517\tBCE:80.3801\tKLD:17.0404\tC_loss:0.0000\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 93.399017\tBCE:76.9589\tKLD:16.4401\tC_loss:0.0000\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 99.009521\tBCE:81.4726\tKLD:17.5369\tC_loss:0.0000\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 96.210587\tBCE:79.7477\tKLD:16.4628\tC_loss:0.0000\n",
      "====> Epoch: 13 Average loss: 96.5884\tClassifier Accuracy: 98.8932\n",
      "====> Test set loss: 97.2953\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 93.973740\tBCE:77.0825\tKLD:16.8913\tC_loss:0.0000\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 96.516327\tBCE:80.0766\tKLD:16.4397\tC_loss:0.0000\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 95.997284\tBCE:79.8950\tKLD:16.1022\tC_loss:0.0001\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 96.259514\tBCE:79.3034\tKLD:16.9561\tC_loss:0.0000\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 96.324432\tBCE:79.7478\tKLD:16.5766\tC_loss:0.0000\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 99.677055\tBCE:82.3639\tKLD:17.3131\tC_loss:0.0000\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 95.009941\tBCE:78.2374\tKLD:16.7725\tC_loss:0.0000\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 94.449142\tBCE:77.5677\tKLD:16.8814\tC_loss:0.0000\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 96.155792\tBCE:80.2297\tKLD:15.9260\tC_loss:0.0000\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 95.677048\tBCE:79.0626\tKLD:16.6144\tC_loss:0.0000\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 94.301544\tBCE:77.7426\tKLD:16.5589\tC_loss:0.0000\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 97.044441\tBCE:79.7375\tKLD:17.3069\tC_loss:0.0000\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 95.881943\tBCE:79.2977\tKLD:16.5842\tC_loss:0.0000\n",
      "Train Epoch: 14 [33280/60000 (56%)]\tLoss: 97.095810\tBCE:80.0903\tKLD:17.0055\tC_loss:0.0000\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 96.413818\tBCE:79.6218\tKLD:16.7920\tC_loss:0.0000\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 96.698967\tBCE:79.5836\tKLD:17.1154\tC_loss:0.0000\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 97.915169\tBCE:80.7877\tKLD:17.1274\tC_loss:0.0000\n",
      "Train Epoch: 14 [43520/60000 (73%)]\tLoss: 94.697632\tBCE:77.5775\tKLD:17.1201\tC_loss:0.0000\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 97.563431\tBCE:80.9484\tKLD:16.6150\tC_loss:0.0000\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 96.707047\tBCE:79.5871\tKLD:17.1199\tC_loss:0.0000\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 98.146027\tBCE:80.8254\tKLD:17.3206\tC_loss:0.0000\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 98.634842\tBCE:81.4724\tKLD:17.1624\tC_loss:0.0000\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 94.475952\tBCE:77.5298\tKLD:16.9461\tC_loss:0.0000\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 98.139343\tBCE:80.8347\tKLD:17.3046\tC_loss:0.0000\n",
      "====> Epoch: 14 Average loss: 95.9860\tClassifier Accuracy: 99.0268\n",
      "====> Test set loss: 95.6229\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 95.341499\tBCE:78.2598\tKLD:17.0816\tC_loss:0.0000\n",
      "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 97.335854\tBCE:80.5960\tKLD:16.7398\tC_loss:0.0000\n",
      "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 93.575653\tBCE:76.7324\tKLD:16.8433\tC_loss:0.0000\n",
      "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 96.577286\tBCE:79.6372\tKLD:16.9401\tC_loss:0.0000\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 97.183250\tBCE:79.9879\tKLD:17.1954\tC_loss:0.0000\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 97.491272\tBCE:80.6791\tKLD:16.8122\tC_loss:0.0000\n",
      "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 97.125824\tBCE:80.1976\tKLD:16.9281\tC_loss:0.0000\n",
      "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 93.343071\tBCE:77.1101\tKLD:16.2330\tC_loss:0.0000\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 99.444611\tBCE:82.7529\tKLD:16.6917\tC_loss:0.0000\n",
      "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 96.965332\tBCE:80.1021\tKLD:16.8632\tC_loss:0.0000\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 96.742294\tBCE:79.7277\tKLD:17.0146\tC_loss:0.0000\n",
      "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 96.772034\tBCE:79.6654\tKLD:17.1066\tC_loss:0.0000\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 94.179581\tBCE:77.6328\tKLD:16.5468\tC_loss:0.0000\n",
      "Train Epoch: 15 [33280/60000 (56%)]\tLoss: 96.668556\tBCE:79.3357\tKLD:17.3329\tC_loss:0.0000\n",
      "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 94.552589\tBCE:77.9235\tKLD:16.6291\tC_loss:0.0000\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 96.987061\tBCE:80.1132\tKLD:16.8738\tC_loss:0.0000\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 95.091919\tBCE:79.0366\tKLD:16.0553\tC_loss:0.0000\n",
      "Train Epoch: 15 [43520/60000 (73%)]\tLoss: 96.641357\tBCE:79.5205\tKLD:17.1209\tC_loss:0.0000\n",
      "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 96.351204\tBCE:79.2374\tKLD:17.1138\tC_loss:0.0000\n",
      "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 96.673309\tBCE:79.8735\tKLD:16.7998\tC_loss:0.0000\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 94.515289\tBCE:77.7522\tKLD:16.7630\tC_loss:0.0000\n",
      "Train Epoch: 15 [53760/60000 (90%)]\tLoss: 96.816589\tBCE:80.1718\tKLD:16.6448\tC_loss:0.0000\n",
      "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 94.888657\tBCE:77.7938\tKLD:17.0949\tC_loss:0.0000\n",
      "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 98.211945\tBCE:81.6593\tKLD:16.5526\tC_loss:0.0000\n",
      "====> Epoch: 15 Average loss: 95.6628\tClassifier Accuracy: 99.1169\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 95.8155\n",
      "Random number: 3\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 95.458084\tBCE:78.6802\tKLD:16.7778\tC_loss:0.0000\n",
      "Train Epoch: 16 [2560/60000 (4%)]\tLoss: 96.754936\tBCE:79.5686\tKLD:17.1863\tC_loss:0.0000\n",
      "Train Epoch: 16 [5120/60000 (9%)]\tLoss: 95.866104\tBCE:79.3171\tKLD:16.5490\tC_loss:0.0000\n",
      "Train Epoch: 16 [7680/60000 (13%)]\tLoss: 93.948761\tBCE:76.9814\tKLD:16.9673\tC_loss:0.0000\n",
      "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 94.392982\tBCE:77.6510\tKLD:16.7420\tC_loss:0.0000\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 97.949814\tBCE:80.9817\tKLD:16.9681\tC_loss:0.0000\n",
      "Train Epoch: 16 [15360/60000 (26%)]\tLoss: 96.621933\tBCE:79.7286\tKLD:16.8933\tC_loss:0.0000\n",
      "Train Epoch: 16 [17920/60000 (30%)]\tLoss: 95.132660\tBCE:78.4748\tKLD:16.6579\tC_loss:0.0000\n",
      "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 94.246414\tBCE:77.5732\tKLD:16.6732\tC_loss:0.0000\n",
      "Train Epoch: 16 [23040/60000 (38%)]\tLoss: 92.933044\tBCE:76.0571\tKLD:16.8759\tC_loss:0.0000\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 96.764420\tBCE:79.0791\tKLD:17.6853\tC_loss:0.0000\n",
      "Train Epoch: 16 [28160/60000 (47%)]\tLoss: 97.191360\tBCE:80.1375\tKLD:17.0538\tC_loss:0.0000\n",
      "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 96.360046\tBCE:78.6957\tKLD:17.6643\tC_loss:0.0000\n",
      "Train Epoch: 16 [33280/60000 (56%)]\tLoss: 94.234848\tBCE:76.9186\tKLD:17.3162\tC_loss:0.0000\n",
      "Train Epoch: 16 [35840/60000 (60%)]\tLoss: 95.771294\tBCE:78.9208\tKLD:16.8505\tC_loss:0.0000\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 98.945297\tBCE:81.8677\tKLD:17.0775\tC_loss:0.0000\n",
      "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 94.679466\tBCE:78.2106\tKLD:16.4689\tC_loss:0.0000\n",
      "Train Epoch: 16 [43520/60000 (73%)]\tLoss: 93.882858\tBCE:76.9094\tKLD:16.9735\tC_loss:0.0000\n",
      "Train Epoch: 16 [46080/60000 (77%)]\tLoss: 96.917953\tBCE:79.4929\tKLD:17.4250\tC_loss:0.0000\n",
      "Train Epoch: 16 [48640/60000 (81%)]\tLoss: 99.749908\tBCE:82.6152\tKLD:17.1347\tC_loss:0.0000\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 96.818817\tBCE:79.7690\tKLD:17.0498\tC_loss:0.0000\n",
      "Train Epoch: 16 [53760/60000 (90%)]\tLoss: 96.392151\tBCE:79.0085\tKLD:17.3836\tC_loss:0.0000\n",
      "Train Epoch: 16 [56320/60000 (94%)]\tLoss: 95.091293\tBCE:78.2722\tKLD:16.8191\tC_loss:0.0000\n",
      "Train Epoch: 16 [58880/60000 (98%)]\tLoss: 94.629822\tBCE:77.9720\tKLD:16.6578\tC_loss:0.0000\n",
      "====> Epoch: 16 Average loss: 95.2164\tClassifier Accuracy: 99.2455\n",
      "====> Test set loss: 95.7425\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 97.812538\tBCE:80.7365\tKLD:17.0760\tC_loss:0.0000\n",
      "Train Epoch: 17 [2560/60000 (4%)]\tLoss: 95.009621\tBCE:78.2960\tKLD:16.7136\tC_loss:0.0000\n",
      "Train Epoch: 17 [5120/60000 (9%)]\tLoss: 93.740707\tBCE:76.7072\tKLD:17.0335\tC_loss:0.0000\n",
      "Train Epoch: 17 [7680/60000 (13%)]\tLoss: 97.305489\tBCE:80.1163\tKLD:17.1892\tC_loss:0.0000\n",
      "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 95.745209\tBCE:78.5039\tKLD:17.2413\tC_loss:0.0000\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 95.582359\tBCE:78.5880\tKLD:16.9944\tC_loss:0.0000\n",
      "Train Epoch: 17 [15360/60000 (26%)]\tLoss: 97.337578\tBCE:80.6533\tKLD:16.6843\tC_loss:0.0000\n",
      "Train Epoch: 17 [17920/60000 (30%)]\tLoss: 94.716499\tBCE:77.9465\tKLD:16.7700\tC_loss:0.0000\n",
      "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 96.016953\tBCE:78.5892\tKLD:17.4277\tC_loss:0.0000\n",
      "Train Epoch: 17 [23040/60000 (38%)]\tLoss: 93.880928\tBCE:77.0715\tKLD:16.8094\tC_loss:0.0000\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 91.540291\tBCE:75.2854\tKLD:16.2548\tC_loss:0.0000\n",
      "Train Epoch: 17 [28160/60000 (47%)]\tLoss: 95.226875\tBCE:77.7187\tKLD:17.5081\tC_loss:0.0000\n",
      "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 95.092148\tBCE:78.3789\tKLD:16.7132\tC_loss:0.0000\n",
      "Train Epoch: 17 [33280/60000 (56%)]\tLoss: 96.336823\tBCE:79.0125\tKLD:17.3243\tC_loss:0.0000\n",
      "Train Epoch: 17 [35840/60000 (60%)]\tLoss: 94.359924\tBCE:77.7934\tKLD:16.5665\tC_loss:0.0000\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 92.587646\tBCE:76.0834\tKLD:16.5042\tC_loss:0.0000\n",
      "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 97.085640\tBCE:80.0206\tKLD:17.0651\tC_loss:0.0000\n",
      "Train Epoch: 17 [43520/60000 (73%)]\tLoss: 94.683578\tBCE:77.4882\tKLD:17.1954\tC_loss:0.0000\n",
      "Train Epoch: 17 [46080/60000 (77%)]\tLoss: 99.139206\tBCE:81.5606\tKLD:17.5786\tC_loss:0.0000\n",
      "Train Epoch: 17 [48640/60000 (81%)]\tLoss: 93.906799\tBCE:76.9368\tKLD:16.9700\tC_loss:0.0000\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 96.038918\tBCE:79.0554\tKLD:16.9835\tC_loss:0.0000\n",
      "Train Epoch: 17 [53760/60000 (90%)]\tLoss: 94.301605\tBCE:77.3710\tKLD:16.9306\tC_loss:0.0000\n",
      "Train Epoch: 17 [56320/60000 (94%)]\tLoss: 92.804626\tBCE:75.9478\tKLD:16.8568\tC_loss:0.0000\n",
      "Train Epoch: 17 [58880/60000 (98%)]\tLoss: 94.752914\tBCE:77.7218\tKLD:17.0311\tC_loss:0.0000\n",
      "====> Epoch: 17 Average loss: 94.8874\tClassifier Accuracy: 99.3289\n",
      "====> Test set loss: 95.2033\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 94.092964\tBCE:77.1791\tKLD:16.9139\tC_loss:0.0000\n",
      "Train Epoch: 18 [2560/60000 (4%)]\tLoss: 92.761826\tBCE:75.8414\tKLD:16.9204\tC_loss:0.0000\n",
      "Train Epoch: 18 [5120/60000 (9%)]\tLoss: 94.352692\tBCE:77.2350\tKLD:17.1177\tC_loss:0.0000\n",
      "Train Epoch: 18 [7680/60000 (13%)]\tLoss: 92.065811\tBCE:75.7218\tKLD:16.3440\tC_loss:0.0000\n",
      "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 96.048721\tBCE:78.9493\tKLD:17.0994\tC_loss:0.0000\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 94.013596\tBCE:77.4326\tKLD:16.5809\tC_loss:0.0000\n",
      "Train Epoch: 18 [15360/60000 (26%)]\tLoss: 93.620064\tBCE:76.7155\tKLD:16.9046\tC_loss:0.0000\n",
      "Train Epoch: 18 [17920/60000 (30%)]\tLoss: 95.277359\tBCE:77.8706\tKLD:17.4068\tC_loss:0.0000\n",
      "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 95.975990\tBCE:79.1908\tKLD:16.7852\tC_loss:0.0000\n",
      "Train Epoch: 18 [23040/60000 (38%)]\tLoss: 93.568077\tBCE:76.6273\tKLD:16.9408\tC_loss:0.0000\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 95.458267\tBCE:78.4018\tKLD:17.0565\tC_loss:0.0000\n",
      "Train Epoch: 18 [28160/60000 (47%)]\tLoss: 95.340469\tBCE:78.0893\tKLD:17.2511\tC_loss:0.0000\n",
      "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 95.790878\tBCE:78.6857\tKLD:17.1051\tC_loss:0.0000\n",
      "Train Epoch: 18 [33280/60000 (56%)]\tLoss: 94.735672\tBCE:77.6954\tKLD:17.0402\tC_loss:0.0000\n",
      "Train Epoch: 18 [35840/60000 (60%)]\tLoss: 94.399773\tBCE:77.0972\tKLD:17.3026\tC_loss:0.0000\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 94.632545\tBCE:78.0492\tKLD:16.5833\tC_loss:0.0000\n",
      "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 94.118393\tBCE:77.6216\tKLD:16.4967\tC_loss:0.0000\n",
      "Train Epoch: 18 [43520/60000 (73%)]\tLoss: 95.917740\tBCE:78.3876\tKLD:17.5301\tC_loss:0.0000\n",
      "Train Epoch: 18 [46080/60000 (77%)]\tLoss: 95.834557\tBCE:78.8116\tKLD:17.0230\tC_loss:0.0000\n",
      "Train Epoch: 18 [48640/60000 (81%)]\tLoss: 95.165756\tBCE:77.9009\tKLD:17.2649\tC_loss:0.0000\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 90.165230\tBCE:73.5357\tKLD:16.6296\tC_loss:0.0000\n",
      "Train Epoch: 18 [53760/60000 (90%)]\tLoss: 95.516388\tBCE:78.2103\tKLD:17.3061\tC_loss:0.0000\n",
      "Train Epoch: 18 [56320/60000 (94%)]\tLoss: 95.194473\tBCE:78.3553\tKLD:16.8391\tC_loss:0.0000\n",
      "Train Epoch: 18 [58880/60000 (98%)]\tLoss: 97.801765\tBCE:80.3303\tKLD:17.4714\tC_loss:0.0000\n",
      "====> Epoch: 18 Average loss: 94.5208\tClassifier Accuracy: 99.4207\n",
      "====> Test set loss: 95.1202\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 96.142387\tBCE:78.8436\tKLD:17.2987\tC_loss:0.0000\n",
      "Train Epoch: 19 [2560/60000 (4%)]\tLoss: 93.857216\tBCE:76.9266\tKLD:16.9306\tC_loss:0.0000\n",
      "Train Epoch: 19 [5120/60000 (9%)]\tLoss: 94.630554\tBCE:77.4292\tKLD:17.2014\tC_loss:0.0000\n",
      "Train Epoch: 19 [7680/60000 (13%)]\tLoss: 96.719223\tBCE:79.5218\tKLD:17.1974\tC_loss:0.0000\n",
      "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 95.679535\tBCE:78.7294\tKLD:16.9501\tC_loss:0.0000\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 94.780289\tBCE:78.0876\tKLD:16.6927\tC_loss:0.0000\n",
      "Train Epoch: 19 [15360/60000 (26%)]\tLoss: 92.205421\tBCE:75.2273\tKLD:16.9781\tC_loss:0.0000\n",
      "Train Epoch: 19 [17920/60000 (30%)]\tLoss: 94.280609\tBCE:77.3022\tKLD:16.9784\tC_loss:0.0000\n",
      "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 92.101364\tBCE:74.9993\tKLD:17.1021\tC_loss:0.0000\n",
      "Train Epoch: 19 [23040/60000 (38%)]\tLoss: 95.298325\tBCE:77.4443\tKLD:17.8540\tC_loss:0.0000\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 94.596611\tBCE:77.8506\tKLD:16.7460\tC_loss:0.0000\n",
      "Train Epoch: 19 [28160/60000 (47%)]\tLoss: 95.430298\tBCE:78.4540\tKLD:16.9763\tC_loss:0.0000\n",
      "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 90.929886\tBCE:74.2192\tKLD:16.7107\tC_loss:0.0000\n",
      "Train Epoch: 19 [33280/60000 (56%)]\tLoss: 94.860184\tBCE:78.0310\tKLD:16.8292\tC_loss:0.0000\n",
      "Train Epoch: 19 [35840/60000 (60%)]\tLoss: 93.907898\tBCE:77.2160\tKLD:16.6919\tC_loss:0.0000\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 94.613083\tBCE:77.6684\tKLD:16.9446\tC_loss:0.0000\n",
      "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 94.623917\tBCE:77.3576\tKLD:17.2663\tC_loss:0.0000\n",
      "Train Epoch: 19 [43520/60000 (73%)]\tLoss: 92.179413\tBCE:75.0686\tKLD:17.1109\tC_loss:0.0000\n",
      "Train Epoch: 19 [46080/60000 (77%)]\tLoss: 94.099030\tBCE:77.2400\tKLD:16.8590\tC_loss:0.0000\n",
      "Train Epoch: 19 [48640/60000 (81%)]\tLoss: 92.743668\tBCE:75.6991\tKLD:17.0445\tC_loss:0.0000\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 94.076729\tBCE:77.0473\tKLD:17.0294\tC_loss:0.0000\n",
      "Train Epoch: 19 [53760/60000 (90%)]\tLoss: 92.134354\tBCE:75.7314\tKLD:16.4029\tC_loss:0.0000\n",
      "Train Epoch: 19 [56320/60000 (94%)]\tLoss: 96.103424\tBCE:78.8501\tKLD:17.2533\tC_loss:0.0000\n",
      "Train Epoch: 19 [58880/60000 (98%)]\tLoss: 96.768829\tBCE:79.0388\tKLD:17.7300\tC_loss:0.0000\n",
      "====> Epoch: 19 Average loss: 94.1716\tClassifier Accuracy: 99.5209\n",
      "====> Test set loss: 95.0544\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 95.315460\tBCE:78.2292\tKLD:17.0863\tC_loss:0.0000\n",
      "Train Epoch: 20 [2560/60000 (4%)]\tLoss: 93.468689\tBCE:76.8184\tKLD:16.6503\tC_loss:0.0000\n",
      "Train Epoch: 20 [5120/60000 (9%)]\tLoss: 94.315506\tBCE:77.6782\tKLD:16.6373\tC_loss:0.0000\n",
      "Train Epoch: 20 [7680/60000 (13%)]\tLoss: 95.304825\tBCE:78.0961\tKLD:17.2087\tC_loss:0.0000\n",
      "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 91.487206\tBCE:75.2536\tKLD:16.2336\tC_loss:0.0000\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 94.331284\tBCE:77.0864\tKLD:17.2449\tC_loss:0.0000\n",
      "Train Epoch: 20 [15360/60000 (26%)]\tLoss: 96.294456\tBCE:79.0437\tKLD:17.2507\tC_loss:0.0000\n",
      "Train Epoch: 20 [17920/60000 (30%)]\tLoss: 92.728210\tBCE:75.7372\tKLD:16.9910\tC_loss:0.0000\n",
      "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 93.877708\tBCE:76.8898\tKLD:16.9879\tC_loss:0.0000\n",
      "Train Epoch: 20 [23040/60000 (38%)]\tLoss: 96.069427\tBCE:78.9298\tKLD:17.1396\tC_loss:0.0000\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 93.358452\tBCE:76.3627\tKLD:16.9958\tC_loss:0.0000\n",
      "Train Epoch: 20 [28160/60000 (47%)]\tLoss: 94.629425\tBCE:77.5241\tKLD:17.1053\tC_loss:0.0000\n",
      "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 90.529343\tBCE:74.2545\tKLD:16.2748\tC_loss:0.0000\n",
      "Train Epoch: 20 [33280/60000 (56%)]\tLoss: 91.831726\tBCE:74.8452\tKLD:16.9865\tC_loss:0.0000\n",
      "Train Epoch: 20 [35840/60000 (60%)]\tLoss: 92.493958\tBCE:75.7338\tKLD:16.7601\tC_loss:0.0000\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 93.579971\tBCE:76.5418\tKLD:17.0381\tC_loss:0.0000\n",
      "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 97.425842\tBCE:79.7942\tKLD:17.6317\tC_loss:0.0000\n",
      "Train Epoch: 20 [43520/60000 (73%)]\tLoss: 91.451759\tBCE:74.6745\tKLD:16.7773\tC_loss:0.0000\n",
      "Train Epoch: 20 [46080/60000 (77%)]\tLoss: 93.419434\tBCE:76.4940\tKLD:16.9254\tC_loss:0.0000\n",
      "Train Epoch: 20 [48640/60000 (81%)]\tLoss: 92.170197\tBCE:75.2684\tKLD:16.9018\tC_loss:0.0000\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 95.285240\tBCE:77.9698\tKLD:17.3154\tC_loss:0.0000\n",
      "Train Epoch: 20 [53760/60000 (90%)]\tLoss: 95.774170\tBCE:78.7265\tKLD:17.0477\tC_loss:0.0000\n",
      "Train Epoch: 20 [56320/60000 (94%)]\tLoss: 96.752319\tBCE:79.1670\tKLD:17.5853\tC_loss:0.0000\n",
      "Train Epoch: 20 [58880/60000 (98%)]\tLoss: 92.739540\tBCE:76.3181\tKLD:16.4214\tC_loss:0.0000\n",
      "====> Epoch: 20 Average loss: 93.8635\tClassifier Accuracy: 99.5459\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 94.5453\n",
      "Random number: 0\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 93.587128\tBCE:76.7908\tKLD:16.7964\tC_loss:0.0000\n",
      "Train Epoch: 21 [2560/60000 (4%)]\tLoss: 94.664444\tBCE:77.3476\tKLD:17.3169\tC_loss:0.0000\n",
      "Train Epoch: 21 [5120/60000 (9%)]\tLoss: 93.679298\tBCE:76.8656\tKLD:16.8137\tC_loss:0.0000\n",
      "Train Epoch: 21 [7680/60000 (13%)]\tLoss: 94.133163\tBCE:76.9067\tKLD:17.2264\tC_loss:0.0000\n",
      "Train Epoch: 21 [10240/60000 (17%)]\tLoss: 93.693390\tBCE:76.5867\tKLD:17.1066\tC_loss:0.0000\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 95.859016\tBCE:78.2892\tKLD:17.5698\tC_loss:0.0000\n",
      "Train Epoch: 21 [15360/60000 (26%)]\tLoss: 96.227295\tBCE:79.0569\tKLD:17.1703\tC_loss:0.0000\n",
      "Train Epoch: 21 [17920/60000 (30%)]\tLoss: 90.602287\tBCE:73.5608\tKLD:17.0414\tC_loss:0.0000\n",
      "Train Epoch: 21 [20480/60000 (34%)]\tLoss: 93.684532\tBCE:77.1900\tKLD:16.4946\tC_loss:0.0000\n",
      "Train Epoch: 21 [23040/60000 (38%)]\tLoss: 92.419266\tBCE:76.1079\tKLD:16.3114\tC_loss:0.0000\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 94.848404\tBCE:77.6187\tKLD:17.2297\tC_loss:0.0000\n",
      "Train Epoch: 21 [28160/60000 (47%)]\tLoss: 93.649155\tBCE:76.0983\tKLD:17.5509\tC_loss:0.0000\n",
      "Train Epoch: 21 [30720/60000 (51%)]\tLoss: 93.300728\tBCE:76.3615\tKLD:16.9392\tC_loss:0.0000\n",
      "Train Epoch: 21 [33280/60000 (56%)]\tLoss: 95.412346\tBCE:78.4119\tKLD:17.0005\tC_loss:0.0000\n",
      "Train Epoch: 21 [35840/60000 (60%)]\tLoss: 91.812119\tBCE:74.2415\tKLD:17.5706\tC_loss:0.0000\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 92.740456\tBCE:75.4975\tKLD:17.2430\tC_loss:0.0000\n",
      "Train Epoch: 21 [40960/60000 (68%)]\tLoss: 93.549637\tBCE:76.6571\tKLD:16.8925\tC_loss:0.0000\n",
      "Train Epoch: 21 [43520/60000 (73%)]\tLoss: 92.091400\tBCE:74.7518\tKLD:17.3396\tC_loss:0.0000\n",
      "Train Epoch: 21 [46080/60000 (77%)]\tLoss: 96.762390\tBCE:79.3364\tKLD:17.4259\tC_loss:0.0000\n",
      "Train Epoch: 21 [48640/60000 (81%)]\tLoss: 93.436417\tBCE:76.2136\tKLD:17.2228\tC_loss:0.0000\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 97.109917\tBCE:80.2369\tKLD:16.8730\tC_loss:0.0000\n",
      "Train Epoch: 21 [53760/60000 (90%)]\tLoss: 93.582184\tBCE:76.4829\tKLD:17.0993\tC_loss:0.0000\n",
      "Train Epoch: 21 [56320/60000 (94%)]\tLoss: 96.125023\tBCE:78.7776\tKLD:17.3474\tC_loss:0.0000\n",
      "Train Epoch: 21 [58880/60000 (98%)]\tLoss: 91.759033\tBCE:75.1482\tKLD:16.6108\tC_loss:0.0000\n",
      "====> Epoch: 21 Average loss: 93.7851\tClassifier Accuracy: 99.6461\n",
      "====> Test set loss: 94.4034\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 93.173698\tBCE:76.2362\tKLD:16.9375\tC_loss:0.0000\n",
      "Train Epoch: 22 [2560/60000 (4%)]\tLoss: 95.957932\tBCE:78.9822\tKLD:16.9757\tC_loss:0.0000\n",
      "Train Epoch: 22 [5120/60000 (9%)]\tLoss: 93.950851\tBCE:76.7341\tKLD:17.2168\tC_loss:0.0000\n",
      "Train Epoch: 22 [7680/60000 (13%)]\tLoss: 97.768768\tBCE:80.3147\tKLD:17.4540\tC_loss:0.0000\n",
      "Train Epoch: 22 [10240/60000 (17%)]\tLoss: 92.311310\tBCE:75.5078\tKLD:16.8035\tC_loss:0.0000\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 96.269890\tBCE:79.1336\tKLD:17.1363\tC_loss:0.0000\n",
      "Train Epoch: 22 [15360/60000 (26%)]\tLoss: 93.333076\tBCE:76.1952\tKLD:17.1379\tC_loss:0.0000\n",
      "Train Epoch: 22 [17920/60000 (30%)]\tLoss: 91.701256\tBCE:75.0446\tKLD:16.6567\tC_loss:0.0000\n",
      "Train Epoch: 22 [20480/60000 (34%)]\tLoss: 92.990410\tBCE:75.7737\tKLD:17.2167\tC_loss:0.0000\n",
      "Train Epoch: 22 [23040/60000 (38%)]\tLoss: 93.555908\tBCE:76.7623\tKLD:16.7936\tC_loss:0.0000\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 92.041672\tBCE:75.0425\tKLD:16.9991\tC_loss:0.0000\n",
      "Train Epoch: 22 [28160/60000 (47%)]\tLoss: 94.587051\tBCE:77.6036\tKLD:16.9835\tC_loss:0.0000\n",
      "Train Epoch: 22 [30720/60000 (51%)]\tLoss: 91.058411\tBCE:73.8929\tKLD:17.1655\tC_loss:0.0000\n",
      "Train Epoch: 22 [33280/60000 (56%)]\tLoss: 96.056320\tBCE:78.9741\tKLD:17.0822\tC_loss:0.0000\n",
      "Train Epoch: 22 [35840/60000 (60%)]\tLoss: 92.949448\tBCE:76.2189\tKLD:16.7306\tC_loss:0.0000\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 92.601868\tBCE:75.3751\tKLD:17.2268\tC_loss:0.0000\n",
      "Train Epoch: 22 [40960/60000 (68%)]\tLoss: 94.065033\tBCE:76.9952\tKLD:17.0699\tC_loss:0.0000\n",
      "Train Epoch: 22 [43520/60000 (73%)]\tLoss: 91.361076\tBCE:74.5588\tKLD:16.8023\tC_loss:0.0000\n",
      "Train Epoch: 22 [46080/60000 (77%)]\tLoss: 93.892776\tBCE:76.4797\tKLD:17.4131\tC_loss:0.0000\n",
      "Train Epoch: 22 [48640/60000 (81%)]\tLoss: 93.677910\tBCE:76.4358\tKLD:17.2421\tC_loss:0.0000\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 91.790901\tBCE:75.0916\tKLD:16.6993\tC_loss:0.0000\n",
      "Train Epoch: 22 [53760/60000 (90%)]\tLoss: 93.733978\tBCE:76.3537\tKLD:17.3803\tC_loss:0.0000\n",
      "Train Epoch: 22 [56320/60000 (94%)]\tLoss: 91.963280\tBCE:74.9126\tKLD:17.0506\tC_loss:0.0000\n",
      "Train Epoch: 22 [58880/60000 (98%)]\tLoss: 91.985764\tBCE:74.8372\tKLD:17.1485\tC_loss:0.0000\n",
      "====> Epoch: 22 Average loss: 93.3941\tClassifier Accuracy: 99.6895\n",
      "====> Test set loss: 93.9407\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 92.832626\tBCE:75.8781\tKLD:16.9545\tC_loss:0.0000\n",
      "Train Epoch: 23 [2560/60000 (4%)]\tLoss: 91.406334\tBCE:74.3304\tKLD:17.0759\tC_loss:0.0000\n",
      "Train Epoch: 23 [5120/60000 (9%)]\tLoss: 93.291611\tBCE:76.1117\tKLD:17.1799\tC_loss:0.0000\n",
      "Train Epoch: 23 [7680/60000 (13%)]\tLoss: 95.351265\tBCE:78.1794\tKLD:17.1719\tC_loss:0.0000\n",
      "Train Epoch: 23 [10240/60000 (17%)]\tLoss: 94.737404\tBCE:77.8274\tKLD:16.9100\tC_loss:0.0000\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 93.619583\tBCE:76.1545\tKLD:17.4650\tC_loss:0.0000\n",
      "Train Epoch: 23 [15360/60000 (26%)]\tLoss: 91.686798\tBCE:74.8332\tKLD:16.8535\tC_loss:0.0000\n",
      "Train Epoch: 23 [17920/60000 (30%)]\tLoss: 96.634201\tBCE:78.7487\tKLD:17.8855\tC_loss:0.0000\n",
      "Train Epoch: 23 [20480/60000 (34%)]\tLoss: 93.167160\tBCE:75.7276\tKLD:17.4395\tC_loss:0.0000\n",
      "Train Epoch: 23 [23040/60000 (38%)]\tLoss: 96.450653\tBCE:79.2339\tKLD:17.2168\tC_loss:0.0000\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 95.484016\tBCE:78.1866\tKLD:17.2974\tC_loss:0.0000\n",
      "Train Epoch: 23 [28160/60000 (47%)]\tLoss: 91.978401\tBCE:75.0722\tKLD:16.9062\tC_loss:0.0000\n",
      "Train Epoch: 23 [30720/60000 (51%)]\tLoss: 94.775177\tBCE:77.3315\tKLD:17.4437\tC_loss:0.0000\n",
      "Train Epoch: 23 [33280/60000 (56%)]\tLoss: 93.785706\tBCE:76.3234\tKLD:17.4623\tC_loss:0.0000\n",
      "Train Epoch: 23 [35840/60000 (60%)]\tLoss: 95.238770\tBCE:77.6145\tKLD:17.6243\tC_loss:0.0000\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 90.509460\tBCE:73.8797\tKLD:16.6298\tC_loss:0.0000\n",
      "Train Epoch: 23 [40960/60000 (68%)]\tLoss: 90.239624\tBCE:73.2775\tKLD:16.9621\tC_loss:0.0000\n",
      "Train Epoch: 23 [43520/60000 (73%)]\tLoss: 93.601120\tBCE:76.4486\tKLD:17.1525\tC_loss:0.0000\n",
      "Train Epoch: 23 [46080/60000 (77%)]\tLoss: 92.219025\tBCE:75.5621\tKLD:16.6569\tC_loss:0.0000\n",
      "Train Epoch: 23 [48640/60000 (81%)]\tLoss: 93.627731\tBCE:76.0543\tKLD:17.5734\tC_loss:0.0000\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 94.254120\tBCE:77.1631\tKLD:17.0910\tC_loss:0.0000\n",
      "Train Epoch: 23 [53760/60000 (90%)]\tLoss: 93.287552\tBCE:76.4220\tKLD:16.8656\tC_loss:0.0000\n",
      "Train Epoch: 23 [56320/60000 (94%)]\tLoss: 93.216705\tBCE:76.1691\tKLD:17.0476\tC_loss:0.0000\n",
      "Train Epoch: 23 [58880/60000 (98%)]\tLoss: 96.560127\tBCE:79.3655\tKLD:17.1946\tC_loss:0.0000\n",
      "====> Epoch: 23 Average loss: 93.2811\tClassifier Accuracy: 99.7346\n",
      "====> Test set loss: 93.9602\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 94.077011\tBCE:77.1066\tKLD:16.9704\tC_loss:0.0000\n",
      "Train Epoch: 24 [2560/60000 (4%)]\tLoss: 94.623383\tBCE:77.3379\tKLD:17.2854\tC_loss:0.0000\n",
      "Train Epoch: 24 [5120/60000 (9%)]\tLoss: 89.432213\tBCE:72.5224\tKLD:16.9098\tC_loss:0.0000\n",
      "Train Epoch: 24 [7680/60000 (13%)]\tLoss: 93.879227\tBCE:76.5438\tKLD:17.3355\tC_loss:0.0000\n",
      "Train Epoch: 24 [10240/60000 (17%)]\tLoss: 89.199432\tBCE:72.6450\tKLD:16.5544\tC_loss:0.0000\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 95.778122\tBCE:78.6490\tKLD:17.1291\tC_loss:0.0000\n",
      "Train Epoch: 24 [15360/60000 (26%)]\tLoss: 93.339417\tBCE:76.2414\tKLD:17.0980\tC_loss:0.0000\n",
      "Train Epoch: 24 [17920/60000 (30%)]\tLoss: 90.729446\tBCE:73.5915\tKLD:17.1380\tC_loss:0.0000\n",
      "Train Epoch: 24 [20480/60000 (34%)]\tLoss: 97.668930\tBCE:80.1142\tKLD:17.5547\tC_loss:0.0000\n",
      "Train Epoch: 24 [23040/60000 (38%)]\tLoss: 94.108856\tBCE:76.4500\tKLD:17.6589\tC_loss:0.0000\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 94.553154\tBCE:77.7630\tKLD:16.7901\tC_loss:0.0000\n",
      "Train Epoch: 24 [28160/60000 (47%)]\tLoss: 91.419022\tBCE:73.8782\tKLD:17.5408\tC_loss:0.0000\n",
      "Train Epoch: 24 [30720/60000 (51%)]\tLoss: 93.981888\tBCE:77.0499\tKLD:16.9320\tC_loss:0.0000\n",
      "Train Epoch: 24 [33280/60000 (56%)]\tLoss: 94.094772\tBCE:76.8199\tKLD:17.2749\tC_loss:0.0000\n",
      "Train Epoch: 24 [35840/60000 (60%)]\tLoss: 93.031570\tBCE:76.3452\tKLD:16.6864\tC_loss:0.0000\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 92.402008\tBCE:75.2838\tKLD:17.1182\tC_loss:0.0000\n",
      "Train Epoch: 24 [40960/60000 (68%)]\tLoss: 95.432739\tBCE:78.2085\tKLD:17.2242\tC_loss:0.0000\n",
      "Train Epoch: 24 [43520/60000 (73%)]\tLoss: 96.233437\tBCE:78.6323\tKLD:17.6011\tC_loss:0.0000\n",
      "Train Epoch: 24 [46080/60000 (77%)]\tLoss: 92.601578\tBCE:75.6495\tKLD:16.9520\tC_loss:0.0000\n",
      "Train Epoch: 24 [48640/60000 (81%)]\tLoss: 92.077515\tBCE:75.3499\tKLD:16.7276\tC_loss:0.0000\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 92.347946\tBCE:74.8710\tKLD:17.4769\tC_loss:0.0000\n",
      "Train Epoch: 24 [53760/60000 (90%)]\tLoss: 90.766907\tBCE:73.8506\tKLD:16.9163\tC_loss:0.0000\n",
      "Train Epoch: 24 [56320/60000 (94%)]\tLoss: 91.558144\tBCE:74.2435\tKLD:17.3146\tC_loss:0.0000\n",
      "Train Epoch: 24 [58880/60000 (98%)]\tLoss: 92.140869\tBCE:75.4546\tKLD:16.6863\tC_loss:0.0000\n",
      "====> Epoch: 24 Average loss: 93.0319\tClassifier Accuracy: 99.7596\n",
      "====> Test set loss: 93.5719\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 96.598038\tBCE:79.1661\tKLD:17.4319\tC_loss:0.0000\n",
      "Train Epoch: 25 [2560/60000 (4%)]\tLoss: 92.304611\tBCE:75.1615\tKLD:17.1431\tC_loss:0.0000\n",
      "Train Epoch: 25 [5120/60000 (9%)]\tLoss: 93.388107\tBCE:75.8363\tKLD:17.5518\tC_loss:0.0000\n",
      "Train Epoch: 25 [7680/60000 (13%)]\tLoss: 92.328522\tBCE:74.9942\tKLD:17.3343\tC_loss:0.0000\n",
      "Train Epoch: 25 [10240/60000 (17%)]\tLoss: 92.487976\tBCE:75.4383\tKLD:17.0497\tC_loss:0.0000\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 93.012024\tBCE:76.0573\tKLD:16.9548\tC_loss:0.0000\n",
      "Train Epoch: 25 [15360/60000 (26%)]\tLoss: 95.256226\tBCE:77.7362\tKLD:17.5200\tC_loss:0.0000\n",
      "Train Epoch: 25 [17920/60000 (30%)]\tLoss: 95.328491\tBCE:78.0022\tKLD:17.3263\tC_loss:0.0000\n",
      "Train Epoch: 25 [20480/60000 (34%)]\tLoss: 88.179657\tBCE:71.5483\tKLD:16.6314\tC_loss:0.0000\n",
      "Train Epoch: 25 [23040/60000 (38%)]\tLoss: 93.536591\tBCE:76.0164\tKLD:17.5202\tC_loss:0.0000\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 91.947357\tBCE:74.8132\tKLD:17.1342\tC_loss:0.0000\n",
      "Train Epoch: 25 [28160/60000 (47%)]\tLoss: 94.401024\tBCE:77.1059\tKLD:17.2951\tC_loss:0.0000\n",
      "Train Epoch: 25 [30720/60000 (51%)]\tLoss: 92.009758\tBCE:74.9914\tKLD:17.0184\tC_loss:0.0000\n",
      "Train Epoch: 25 [33280/60000 (56%)]\tLoss: 92.050095\tBCE:74.8375\tKLD:17.2126\tC_loss:0.0000\n",
      "Train Epoch: 25 [35840/60000 (60%)]\tLoss: 92.173508\tBCE:75.1722\tKLD:17.0013\tC_loss:0.0000\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 94.090073\tBCE:77.2163\tKLD:16.8737\tC_loss:0.0000\n",
      "Train Epoch: 25 [40960/60000 (68%)]\tLoss: 94.688904\tBCE:77.7459\tKLD:16.9430\tC_loss:0.0000\n",
      "Train Epoch: 25 [43520/60000 (73%)]\tLoss: 93.750015\tBCE:76.6220\tKLD:17.1280\tC_loss:0.0000\n",
      "Train Epoch: 25 [46080/60000 (77%)]\tLoss: 94.762665\tBCE:78.1478\tKLD:16.6148\tC_loss:0.0000\n",
      "Train Epoch: 25 [48640/60000 (81%)]\tLoss: 92.309525\tBCE:74.9090\tKLD:17.4005\tC_loss:0.0000\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 96.385880\tBCE:78.5949\tKLD:17.7909\tC_loss:0.0000\n",
      "Train Epoch: 25 [53760/60000 (90%)]\tLoss: 92.624039\tBCE:75.6036\tKLD:17.0204\tC_loss:0.0000\n",
      "Train Epoch: 25 [56320/60000 (94%)]\tLoss: 92.313538\tBCE:75.2733\tKLD:17.0402\tC_loss:0.0000\n",
      "Train Epoch: 25 [58880/60000 (98%)]\tLoss: 94.265472\tBCE:77.2876\tKLD:16.9779\tC_loss:0.0000\n",
      "====> Epoch: 25 Average loss: 92.8850\tClassifier Accuracy: 99.8214\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 93.7193\n",
      "Random number: 4\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 93.616234\tBCE:76.0456\tKLD:17.5706\tC_loss:0.0000\n",
      "Train Epoch: 26 [2560/60000 (4%)]\tLoss: 89.420631\tBCE:72.7661\tKLD:16.6545\tC_loss:0.0000\n",
      "Train Epoch: 26 [5120/60000 (9%)]\tLoss: 94.294975\tBCE:76.8339\tKLD:17.4611\tC_loss:0.0000\n",
      "Train Epoch: 26 [7680/60000 (13%)]\tLoss: 88.976959\tBCE:72.1416\tKLD:16.8354\tC_loss:0.0000\n",
      "Train Epoch: 26 [10240/60000 (17%)]\tLoss: 91.204071\tBCE:73.8150\tKLD:17.3891\tC_loss:0.0000\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 92.812317\tBCE:75.1952\tKLD:17.6171\tC_loss:0.0000\n",
      "Train Epoch: 26 [15360/60000 (26%)]\tLoss: 89.625450\tBCE:73.0750\tKLD:16.5504\tC_loss:0.0000\n",
      "Train Epoch: 26 [17920/60000 (30%)]\tLoss: 94.771774\tBCE:77.4536\tKLD:17.3182\tC_loss:0.0000\n",
      "Train Epoch: 26 [20480/60000 (34%)]\tLoss: 91.234795\tBCE:74.3393\tKLD:16.8955\tC_loss:0.0000\n",
      "Train Epoch: 26 [23040/60000 (38%)]\tLoss: 93.261337\tBCE:76.1970\tKLD:17.0644\tC_loss:0.0000\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 94.910049\tBCE:77.2597\tKLD:17.6504\tC_loss:0.0000\n",
      "Train Epoch: 26 [28160/60000 (47%)]\tLoss: 91.315742\tBCE:74.3308\tKLD:16.9849\tC_loss:0.0000\n",
      "Train Epoch: 26 [30720/60000 (51%)]\tLoss: 89.578354\tBCE:72.3383\tKLD:17.2401\tC_loss:0.0000\n",
      "Train Epoch: 26 [33280/60000 (56%)]\tLoss: 88.061966\tBCE:71.5442\tKLD:16.5177\tC_loss:0.0000\n",
      "Train Epoch: 26 [35840/60000 (60%)]\tLoss: 93.711266\tBCE:76.4356\tKLD:17.2756\tC_loss:0.0000\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 93.777893\tBCE:76.6324\tKLD:17.1455\tC_loss:0.0000\n",
      "Train Epoch: 26 [40960/60000 (68%)]\tLoss: 93.675598\tBCE:76.3044\tKLD:17.3712\tC_loss:0.0000\n",
      "Train Epoch: 26 [43520/60000 (73%)]\tLoss: 94.072853\tBCE:77.1478\tKLD:16.9250\tC_loss:0.0000\n",
      "Train Epoch: 26 [46080/60000 (77%)]\tLoss: 93.611008\tBCE:76.1812\tKLD:17.4299\tC_loss:0.0000\n",
      "Train Epoch: 26 [48640/60000 (81%)]\tLoss: 91.441574\tBCE:74.1064\tKLD:17.3351\tC_loss:0.0000\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 92.899742\tBCE:75.6021\tKLD:17.2977\tC_loss:0.0000\n",
      "Train Epoch: 26 [53760/60000 (90%)]\tLoss: 91.403992\tBCE:74.9555\tKLD:16.4485\tC_loss:0.0000\n",
      "Train Epoch: 26 [56320/60000 (94%)]\tLoss: 94.480492\tBCE:76.8809\tKLD:17.5996\tC_loss:0.0000\n",
      "Train Epoch: 26 [58880/60000 (98%)]\tLoss: 94.613403\tBCE:77.0690\tKLD:17.5444\tC_loss:0.0000\n",
      "====> Epoch: 26 Average loss: 92.6876\tClassifier Accuracy: 99.8431\n",
      "====> Test set loss: 93.4675\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 89.680374\tBCE:73.3156\tKLD:16.3648\tC_loss:0.0000\n",
      "Train Epoch: 27 [2560/60000 (4%)]\tLoss: 90.515541\tBCE:73.2600\tKLD:17.2555\tC_loss:0.0000\n",
      "Train Epoch: 27 [5120/60000 (9%)]\tLoss: 92.534325\tBCE:75.2883\tKLD:17.2461\tC_loss:0.0000\n",
      "Train Epoch: 27 [7680/60000 (13%)]\tLoss: 94.889687\tBCE:77.0675\tKLD:17.8221\tC_loss:0.0000\n",
      "Train Epoch: 27 [10240/60000 (17%)]\tLoss: 94.497025\tBCE:77.7362\tKLD:16.7608\tC_loss:0.0000\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 91.782394\tBCE:74.3335\tKLD:17.4489\tC_loss:0.0000\n",
      "Train Epoch: 27 [15360/60000 (26%)]\tLoss: 94.593445\tBCE:76.9074\tKLD:17.6860\tC_loss:0.0000\n",
      "Train Epoch: 27 [17920/60000 (30%)]\tLoss: 95.931702\tBCE:78.0022\tKLD:17.9295\tC_loss:0.0000\n",
      "Train Epoch: 27 [20480/60000 (34%)]\tLoss: 93.184204\tBCE:76.0017\tKLD:17.1825\tC_loss:0.0000\n",
      "Train Epoch: 27 [23040/60000 (38%)]\tLoss: 93.375076\tBCE:75.5972\tKLD:17.7779\tC_loss:0.0000\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 93.106552\tBCE:76.0452\tKLD:17.0613\tC_loss:0.0000\n",
      "Train Epoch: 27 [28160/60000 (47%)]\tLoss: 92.309494\tBCE:75.2068\tKLD:17.1027\tC_loss:0.0000\n",
      "Train Epoch: 27 [30720/60000 (51%)]\tLoss: 90.917313\tBCE:73.8820\tKLD:17.0353\tC_loss:0.0000\n",
      "Train Epoch: 27 [33280/60000 (56%)]\tLoss: 95.049332\tBCE:77.8541\tKLD:17.1952\tC_loss:0.0000\n",
      "Train Epoch: 27 [35840/60000 (60%)]\tLoss: 95.069168\tBCE:77.1693\tKLD:17.8999\tC_loss:0.0000\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 92.541840\tBCE:75.4016\tKLD:17.1402\tC_loss:0.0000\n",
      "Train Epoch: 27 [40960/60000 (68%)]\tLoss: 91.968796\tBCE:74.5649\tKLD:17.4039\tC_loss:0.0000\n",
      "Train Epoch: 27 [43520/60000 (73%)]\tLoss: 91.254066\tBCE:74.2364\tKLD:17.0176\tC_loss:0.0000\n",
      "Train Epoch: 27 [46080/60000 (77%)]\tLoss: 90.506790\tBCE:73.7516\tKLD:16.7552\tC_loss:0.0000\n",
      "Train Epoch: 27 [48640/60000 (81%)]\tLoss: 95.240616\tBCE:77.9082\tKLD:17.3324\tC_loss:0.0000\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 92.225479\tBCE:74.9127\tKLD:17.3128\tC_loss:0.0000\n",
      "Train Epoch: 27 [53760/60000 (90%)]\tLoss: 93.170021\tBCE:75.9255\tKLD:17.2445\tC_loss:0.0000\n",
      "Train Epoch: 27 [56320/60000 (94%)]\tLoss: 90.910316\tBCE:73.7965\tKLD:17.1138\tC_loss:0.0000\n",
      "Train Epoch: 27 [58880/60000 (98%)]\tLoss: 92.845604\tBCE:75.7964\tKLD:17.0492\tC_loss:0.0000\n",
      "====> Epoch: 27 Average loss: 92.5692\tClassifier Accuracy: 99.8464\n",
      "====> Test set loss: 92.9699\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 92.073792\tBCE:75.2837\tKLD:16.7901\tC_loss:0.0000\n",
      "Train Epoch: 28 [2560/60000 (4%)]\tLoss: 92.915283\tBCE:75.5684\tKLD:17.3469\tC_loss:0.0000\n",
      "Train Epoch: 28 [5120/60000 (9%)]\tLoss: 90.511497\tBCE:73.3309\tKLD:17.1806\tC_loss:0.0000\n",
      "Train Epoch: 28 [7680/60000 (13%)]\tLoss: 91.040390\tBCE:74.2295\tKLD:16.8108\tC_loss:0.0000\n",
      "Train Epoch: 28 [10240/60000 (17%)]\tLoss: 94.871552\tBCE:77.9267\tKLD:16.9448\tC_loss:0.0000\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 92.315857\tBCE:74.9536\tKLD:17.3623\tC_loss:0.0000\n",
      "Train Epoch: 28 [15360/60000 (26%)]\tLoss: 94.526619\tBCE:77.5672\tKLD:16.9594\tC_loss:0.0000\n",
      "Train Epoch: 28 [17920/60000 (30%)]\tLoss: 94.911423\tBCE:77.1406\tKLD:17.7708\tC_loss:0.0000\n",
      "Train Epoch: 28 [20480/60000 (34%)]\tLoss: 93.822800\tBCE:76.5917\tKLD:17.2311\tC_loss:0.0000\n",
      "Train Epoch: 28 [23040/60000 (38%)]\tLoss: 92.980957\tBCE:75.0823\tKLD:17.8987\tC_loss:0.0000\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 94.516800\tBCE:76.8629\tKLD:17.6539\tC_loss:0.0000\n",
      "Train Epoch: 28 [28160/60000 (47%)]\tLoss: 92.763687\tBCE:75.4796\tKLD:17.2841\tC_loss:0.0000\n",
      "Train Epoch: 28 [30720/60000 (51%)]\tLoss: 94.774139\tBCE:76.5659\tKLD:18.2082\tC_loss:0.0000\n",
      "Train Epoch: 28 [33280/60000 (56%)]\tLoss: 92.396484\tBCE:75.2766\tKLD:17.1199\tC_loss:0.0000\n",
      "Train Epoch: 28 [35840/60000 (60%)]\tLoss: 94.012222\tBCE:76.6988\tKLD:17.3134\tC_loss:0.0000\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 92.234283\tBCE:74.8481\tKLD:17.3861\tC_loss:0.0000\n",
      "Train Epoch: 28 [40960/60000 (68%)]\tLoss: 91.964233\tBCE:74.5749\tKLD:17.3894\tC_loss:0.0000\n",
      "Train Epoch: 28 [43520/60000 (73%)]\tLoss: 92.234665\tBCE:75.1828\tKLD:17.0519\tC_loss:0.0000\n",
      "Train Epoch: 28 [46080/60000 (77%)]\tLoss: 95.188599\tBCE:77.4071\tKLD:17.7815\tC_loss:0.0000\n",
      "Train Epoch: 28 [48640/60000 (81%)]\tLoss: 93.436874\tBCE:76.1730\tKLD:17.2639\tC_loss:0.0000\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 89.763084\tBCE:72.9167\tKLD:16.8464\tC_loss:0.0000\n",
      "Train Epoch: 28 [53760/60000 (90%)]\tLoss: 93.160042\tBCE:75.5471\tKLD:17.6129\tC_loss:0.0000\n",
      "Train Epoch: 28 [56320/60000 (94%)]\tLoss: 93.696060\tBCE:76.1514\tKLD:17.5446\tC_loss:0.0000\n",
      "Train Epoch: 28 [58880/60000 (98%)]\tLoss: 90.810631\tBCE:74.1178\tKLD:16.6928\tC_loss:0.0000\n",
      "====> Epoch: 28 Average loss: 92.3801\tClassifier Accuracy: 99.8831\n",
      "====> Test set loss: 92.6978\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 90.775505\tBCE:73.6802\tKLD:17.0953\tC_loss:0.0000\n",
      "Train Epoch: 29 [2560/60000 (4%)]\tLoss: 94.282761\tBCE:77.0503\tKLD:17.2325\tC_loss:0.0000\n",
      "Train Epoch: 29 [5120/60000 (9%)]\tLoss: 94.655113\tBCE:77.1099\tKLD:17.5452\tC_loss:0.0000\n",
      "Train Epoch: 29 [7680/60000 (13%)]\tLoss: 93.557556\tBCE:75.6925\tKLD:17.8650\tC_loss:0.0000\n",
      "Train Epoch: 29 [10240/60000 (17%)]\tLoss: 91.222946\tBCE:73.5071\tKLD:17.7159\tC_loss:0.0000\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 91.119255\tBCE:73.9741\tKLD:17.1452\tC_loss:0.0000\n",
      "Train Epoch: 29 [15360/60000 (26%)]\tLoss: 91.746826\tBCE:74.6362\tKLD:17.1107\tC_loss:0.0000\n",
      "Train Epoch: 29 [17920/60000 (30%)]\tLoss: 89.823021\tBCE:73.0842\tKLD:16.7389\tC_loss:0.0000\n",
      "Train Epoch: 29 [20480/60000 (34%)]\tLoss: 93.406143\tBCE:75.3911\tKLD:18.0150\tC_loss:0.0000\n",
      "Train Epoch: 29 [23040/60000 (38%)]\tLoss: 93.449188\tBCE:76.2840\tKLD:17.1652\tC_loss:0.0000\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 92.492516\tBCE:74.9249\tKLD:17.5677\tC_loss:0.0000\n",
      "Train Epoch: 29 [28160/60000 (47%)]\tLoss: 92.725883\tBCE:75.5059\tKLD:17.2200\tC_loss:0.0000\n",
      "Train Epoch: 29 [30720/60000 (51%)]\tLoss: 91.120865\tBCE:74.2768\tKLD:16.8440\tC_loss:0.0000\n",
      "Train Epoch: 29 [33280/60000 (56%)]\tLoss: 93.320068\tBCE:75.9573\tKLD:17.3628\tC_loss:0.0000\n",
      "Train Epoch: 29 [35840/60000 (60%)]\tLoss: 93.722183\tBCE:76.2694\tKLD:17.4528\tC_loss:0.0000\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 92.056076\tBCE:75.0984\tKLD:16.9577\tC_loss:0.0000\n",
      "Train Epoch: 29 [40960/60000 (68%)]\tLoss: 93.541931\tBCE:76.4858\tKLD:17.0562\tC_loss:0.0000\n",
      "Train Epoch: 29 [43520/60000 (73%)]\tLoss: 91.821335\tBCE:75.1424\tKLD:16.6789\tC_loss:0.0000\n",
      "Train Epoch: 29 [46080/60000 (77%)]\tLoss: 93.454842\tBCE:75.4964\tKLD:17.9585\tC_loss:0.0000\n",
      "Train Epoch: 29 [48640/60000 (81%)]\tLoss: 92.999939\tBCE:75.8884\tKLD:17.1115\tC_loss:0.0000\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 92.019264\tBCE:74.9501\tKLD:17.0691\tC_loss:0.0000\n",
      "Train Epoch: 29 [53760/60000 (90%)]\tLoss: 91.535034\tBCE:74.8350\tKLD:16.7001\tC_loss:0.0000\n",
      "Train Epoch: 29 [56320/60000 (94%)]\tLoss: 90.602722\tBCE:73.6696\tKLD:16.9331\tC_loss:0.0000\n",
      "Train Epoch: 29 [58880/60000 (98%)]\tLoss: 94.080719\tBCE:76.7523\tKLD:17.3284\tC_loss:0.0000\n",
      "====> Epoch: 29 Average loss: 92.2375\tClassifier Accuracy: 99.9282\n",
      "====> Test set loss: 92.8629\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 95.101555\tBCE:77.4250\tKLD:17.6765\tC_loss:0.0000\n",
      "Train Epoch: 30 [2560/60000 (4%)]\tLoss: 91.615723\tBCE:73.9417\tKLD:17.6740\tC_loss:0.0000\n",
      "Train Epoch: 30 [5120/60000 (9%)]\tLoss: 92.333099\tBCE:74.6326\tKLD:17.7005\tC_loss:0.0000\n",
      "Train Epoch: 30 [7680/60000 (13%)]\tLoss: 91.700005\tBCE:74.6783\tKLD:17.0217\tC_loss:0.0000\n",
      "Train Epoch: 30 [10240/60000 (17%)]\tLoss: 92.445236\tBCE:74.7071\tKLD:17.7382\tC_loss:0.0000\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 91.223305\tBCE:74.2197\tKLD:17.0036\tC_loss:0.0000\n",
      "Train Epoch: 30 [15360/60000 (26%)]\tLoss: 93.636269\tBCE:76.3200\tKLD:17.3163\tC_loss:0.0000\n",
      "Train Epoch: 30 [17920/60000 (30%)]\tLoss: 92.930336\tBCE:75.7415\tKLD:17.1888\tC_loss:0.0000\n",
      "Train Epoch: 30 [20480/60000 (34%)]\tLoss: 90.843941\tBCE:73.8034\tKLD:17.0405\tC_loss:0.0000\n",
      "Train Epoch: 30 [23040/60000 (38%)]\tLoss: 90.274696\tBCE:73.1797\tKLD:17.0949\tC_loss:0.0000\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 94.610832\tBCE:77.1180\tKLD:17.4928\tC_loss:0.0000\n",
      "Train Epoch: 30 [28160/60000 (47%)]\tLoss: 92.985260\tBCE:75.9731\tKLD:17.0122\tC_loss:0.0000\n",
      "Train Epoch: 30 [30720/60000 (51%)]\tLoss: 92.657936\tBCE:75.4526\tKLD:17.2054\tC_loss:0.0000\n",
      "Train Epoch: 30 [33280/60000 (56%)]\tLoss: 92.993332\tBCE:75.9582\tKLD:17.0351\tC_loss:0.0000\n",
      "Train Epoch: 30 [35840/60000 (60%)]\tLoss: 92.816559\tBCE:74.9786\tKLD:17.8379\tC_loss:0.0000\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 92.816864\tBCE:75.0168\tKLD:17.8001\tC_loss:0.0000\n",
      "Train Epoch: 30 [40960/60000 (68%)]\tLoss: 94.709450\tBCE:77.1955\tKLD:17.5139\tC_loss:0.0000\n",
      "Train Epoch: 30 [43520/60000 (73%)]\tLoss: 94.220436\tBCE:76.8148\tKLD:17.4056\tC_loss:0.0000\n",
      "Train Epoch: 30 [46080/60000 (77%)]\tLoss: 92.672409\tBCE:75.5060\tKLD:17.1664\tC_loss:0.0000\n",
      "Train Epoch: 30 [48640/60000 (81%)]\tLoss: 93.232208\tBCE:76.0384\tKLD:17.1938\tC_loss:0.0000\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 92.961212\tBCE:75.3982\tKLD:17.5630\tC_loss:0.0000\n",
      "Train Epoch: 30 [53760/60000 (90%)]\tLoss: 92.143639\tBCE:75.0525\tKLD:17.0911\tC_loss:0.0000\n",
      "Train Epoch: 30 [56320/60000 (94%)]\tLoss: 92.819206\tBCE:75.2924\tKLD:17.5268\tC_loss:0.0000\n",
      "Train Epoch: 30 [58880/60000 (98%)]\tLoss: 96.222145\tBCE:78.4405\tKLD:17.7817\tC_loss:0.0000\n",
      "====> Epoch: 30 Average loss: 92.0931\tClassifier Accuracy: 99.9416\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 92.7346\n",
      "Random number: 1\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 95.420250\tBCE:78.0364\tKLD:17.3838\tC_loss:0.0000\n",
      "Train Epoch: 31 [2560/60000 (4%)]\tLoss: 92.185211\tBCE:75.1120\tKLD:17.0732\tC_loss:0.0000\n",
      "Train Epoch: 31 [5120/60000 (9%)]\tLoss: 94.324837\tBCE:76.7705\tKLD:17.5544\tC_loss:0.0000\n",
      "Train Epoch: 31 [7680/60000 (13%)]\tLoss: 89.390282\tBCE:72.7070\tKLD:16.6833\tC_loss:0.0000\n",
      "Train Epoch: 31 [10240/60000 (17%)]\tLoss: 90.221649\tBCE:72.8863\tKLD:17.3354\tC_loss:0.0000\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 91.005653\tBCE:74.1252\tKLD:16.8805\tC_loss:0.0000\n",
      "Train Epoch: 31 [15360/60000 (26%)]\tLoss: 92.110214\tBCE:74.3720\tKLD:17.7382\tC_loss:0.0000\n",
      "Train Epoch: 31 [17920/60000 (30%)]\tLoss: 91.336922\tBCE:74.4744\tKLD:16.8625\tC_loss:0.0000\n",
      "Train Epoch: 31 [20480/60000 (34%)]\tLoss: 88.582245\tBCE:71.8608\tKLD:16.7214\tC_loss:0.0000\n",
      "Train Epoch: 31 [23040/60000 (38%)]\tLoss: 90.313385\tBCE:73.7853\tKLD:16.5281\tC_loss:0.0000\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 91.253799\tBCE:74.0965\tKLD:17.1573\tC_loss:0.0000\n",
      "Train Epoch: 31 [28160/60000 (47%)]\tLoss: 89.848930\tBCE:72.2748\tKLD:17.5741\tC_loss:0.0000\n",
      "Train Epoch: 31 [30720/60000 (51%)]\tLoss: 92.332886\tBCE:74.8974\tKLD:17.4355\tC_loss:0.0000\n",
      "Train Epoch: 31 [33280/60000 (56%)]\tLoss: 96.286278\tBCE:78.3653\tKLD:17.9210\tC_loss:0.0000\n",
      "Train Epoch: 31 [35840/60000 (60%)]\tLoss: 92.448929\tBCE:75.2368\tKLD:17.2121\tC_loss:0.0000\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 90.658936\tBCE:73.9474\tKLD:16.7115\tC_loss:0.0000\n",
      "Train Epoch: 31 [40960/60000 (68%)]\tLoss: 95.439758\tBCE:77.7451\tKLD:17.6947\tC_loss:0.0000\n",
      "Train Epoch: 31 [43520/60000 (73%)]\tLoss: 91.860817\tBCE:74.6858\tKLD:17.1751\tC_loss:0.0000\n",
      "Train Epoch: 31 [46080/60000 (77%)]\tLoss: 95.744698\tBCE:78.4506\tKLD:17.2941\tC_loss:0.0000\n",
      "Train Epoch: 31 [48640/60000 (81%)]\tLoss: 92.000259\tBCE:74.7804\tKLD:17.2198\tC_loss:0.0000\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 94.667770\tBCE:76.9798\tKLD:17.6880\tC_loss:0.0000\n",
      "Train Epoch: 31 [53760/60000 (90%)]\tLoss: 91.895447\tBCE:75.0436\tKLD:16.8519\tC_loss:0.0000\n",
      "Train Epoch: 31 [56320/60000 (94%)]\tLoss: 92.982979\tBCE:75.5452\tKLD:17.4378\tC_loss:0.0000\n",
      "Train Epoch: 31 [58880/60000 (98%)]\tLoss: 89.934067\tBCE:72.8401\tKLD:17.0940\tC_loss:0.0000\n",
      "====> Epoch: 31 Average loss: 92.0734\tClassifier Accuracy: 99.9583\n",
      "====> Test set loss: 92.3262\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 91.888535\tBCE:74.7916\tKLD:17.0969\tC_loss:0.0000\n",
      "Train Epoch: 32 [2560/60000 (4%)]\tLoss: 94.900185\tBCE:77.2511\tKLD:17.6490\tC_loss:0.0000\n",
      "Train Epoch: 32 [5120/60000 (9%)]\tLoss: 92.015480\tBCE:74.1611\tKLD:17.8544\tC_loss:0.0000\n",
      "Train Epoch: 32 [7680/60000 (13%)]\tLoss: 93.906830\tBCE:76.5309\tKLD:17.3759\tC_loss:0.0000\n",
      "Train Epoch: 32 [10240/60000 (17%)]\tLoss: 94.191963\tBCE:76.6134\tKLD:17.5786\tC_loss:0.0000\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 91.041046\tBCE:73.7823\tKLD:17.2587\tC_loss:0.0000\n",
      "Train Epoch: 32 [15360/60000 (26%)]\tLoss: 97.166145\tBCE:79.6133\tKLD:17.5528\tC_loss:0.0000\n",
      "Train Epoch: 32 [17920/60000 (30%)]\tLoss: 93.065430\tBCE:75.8234\tKLD:17.2420\tC_loss:0.0000\n",
      "Train Epoch: 32 [20480/60000 (34%)]\tLoss: 89.648834\tBCE:72.6848\tKLD:16.9640\tC_loss:0.0000\n",
      "Train Epoch: 32 [23040/60000 (38%)]\tLoss: 91.596603\tBCE:74.4144\tKLD:17.1822\tC_loss:0.0000\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 92.438591\tBCE:75.0944\tKLD:17.3441\tC_loss:0.0000\n",
      "Train Epoch: 32 [28160/60000 (47%)]\tLoss: 92.439331\tBCE:75.0193\tKLD:17.4200\tC_loss:0.0000\n",
      "Train Epoch: 32 [30720/60000 (51%)]\tLoss: 91.095169\tBCE:73.5000\tKLD:17.5952\tC_loss:0.0000\n",
      "Train Epoch: 32 [33280/60000 (56%)]\tLoss: 91.798363\tBCE:74.5076\tKLD:17.2908\tC_loss:0.0000\n",
      "Train Epoch: 32 [35840/60000 (60%)]\tLoss: 90.901535\tBCE:73.4515\tKLD:17.4501\tC_loss:0.0000\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 92.139679\tBCE:75.2618\tKLD:16.8779\tC_loss:0.0000\n",
      "Train Epoch: 32 [40960/60000 (68%)]\tLoss: 92.446388\tBCE:75.1565\tKLD:17.2899\tC_loss:0.0000\n",
      "Train Epoch: 32 [43520/60000 (73%)]\tLoss: 92.532516\tBCE:75.6352\tKLD:16.8974\tC_loss:0.0000\n",
      "Train Epoch: 32 [46080/60000 (77%)]\tLoss: 92.836205\tBCE:75.5041\tKLD:17.3321\tC_loss:0.0000\n",
      "Train Epoch: 32 [48640/60000 (81%)]\tLoss: 89.179596\tBCE:72.5992\tKLD:16.5804\tC_loss:0.0000\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 92.618927\tBCE:74.8761\tKLD:17.7428\tC_loss:0.0000\n",
      "Train Epoch: 32 [53760/60000 (90%)]\tLoss: 90.024292\tBCE:72.5812\tKLD:17.4430\tC_loss:0.0000\n",
      "Train Epoch: 32 [56320/60000 (94%)]\tLoss: 93.925674\tBCE:76.3735\tKLD:17.5522\tC_loss:0.0000\n",
      "Train Epoch: 32 [58880/60000 (98%)]\tLoss: 95.093102\tBCE:77.5653\tKLD:17.5278\tC_loss:0.0000\n",
      "====> Epoch: 32 Average loss: 91.8380\tClassifier Accuracy: 99.9616\n",
      "====> Test set loss: 92.5321\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 89.831711\tBCE:72.5623\tKLD:17.2694\tC_loss:0.0000\n",
      "Train Epoch: 33 [2560/60000 (4%)]\tLoss: 92.345444\tBCE:74.8985\tKLD:17.4469\tC_loss:0.0000\n",
      "Train Epoch: 33 [5120/60000 (9%)]\tLoss: 91.639763\tBCE:74.3657\tKLD:17.2741\tC_loss:0.0000\n",
      "Train Epoch: 33 [7680/60000 (13%)]\tLoss: 91.595314\tBCE:74.0982\tKLD:17.4971\tC_loss:0.0000\n",
      "Train Epoch: 33 [10240/60000 (17%)]\tLoss: 93.819916\tBCE:75.5230\tKLD:18.2969\tC_loss:0.0000\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 89.744904\tBCE:72.6514\tKLD:17.0935\tC_loss:0.0000\n",
      "Train Epoch: 33 [15360/60000 (26%)]\tLoss: 92.374939\tBCE:74.7258\tKLD:17.6491\tC_loss:0.0000\n",
      "Train Epoch: 33 [17920/60000 (30%)]\tLoss: 92.397186\tBCE:75.2412\tKLD:17.1560\tC_loss:0.0000\n",
      "Train Epoch: 33 [20480/60000 (34%)]\tLoss: 91.896027\tBCE:74.9649\tKLD:16.9311\tC_loss:0.0000\n",
      "Train Epoch: 33 [23040/60000 (38%)]\tLoss: 90.706482\tBCE:73.7607\tKLD:16.9458\tC_loss:0.0000\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 92.449997\tBCE:75.4281\tKLD:17.0219\tC_loss:0.0000\n",
      "Train Epoch: 33 [28160/60000 (47%)]\tLoss: 93.784119\tBCE:76.7511\tKLD:17.0330\tC_loss:0.0000\n",
      "Train Epoch: 33 [30720/60000 (51%)]\tLoss: 91.709763\tBCE:74.3200\tKLD:17.3897\tC_loss:0.0000\n",
      "Train Epoch: 33 [33280/60000 (56%)]\tLoss: 94.684547\tBCE:77.6024\tKLD:17.0822\tC_loss:0.0000\n",
      "Train Epoch: 33 [35840/60000 (60%)]\tLoss: 89.691254\tBCE:72.5631\tKLD:17.1281\tC_loss:0.0000\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 92.410690\tBCE:75.5015\tKLD:16.9092\tC_loss:0.0000\n",
      "Train Epoch: 33 [40960/60000 (68%)]\tLoss: 91.294556\tBCE:74.2026\tKLD:17.0919\tC_loss:0.0000\n",
      "Train Epoch: 33 [43520/60000 (73%)]\tLoss: 91.923523\tBCE:74.5596\tKLD:17.3639\tC_loss:0.0000\n",
      "Train Epoch: 33 [46080/60000 (77%)]\tLoss: 91.560829\tBCE:74.2014\tKLD:17.3594\tC_loss:0.0000\n",
      "Train Epoch: 33 [48640/60000 (81%)]\tLoss: 88.935745\tBCE:72.2707\tKLD:16.6650\tC_loss:0.0000\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 91.978828\tBCE:74.7642\tKLD:17.2147\tC_loss:0.0000\n",
      "Train Epoch: 33 [53760/60000 (90%)]\tLoss: 93.590622\tBCE:75.8289\tKLD:17.7617\tC_loss:0.0000\n",
      "Train Epoch: 33 [56320/60000 (94%)]\tLoss: 92.904160\tBCE:75.5128\tKLD:17.3914\tC_loss:0.0000\n",
      "Train Epoch: 33 [58880/60000 (98%)]\tLoss: 92.901413\tBCE:75.7712\tKLD:17.1302\tC_loss:0.0000\n",
      "====> Epoch: 33 Average loss: 91.8085\tClassifier Accuracy: 99.9633\n",
      "====> Test set loss: 92.2623\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 91.467575\tBCE:74.3051\tKLD:17.1625\tC_loss:0.0000\n",
      "Train Epoch: 34 [2560/60000 (4%)]\tLoss: 90.900208\tBCE:73.5880\tKLD:17.3122\tC_loss:0.0000\n",
      "Train Epoch: 34 [5120/60000 (9%)]\tLoss: 90.760788\tBCE:73.4427\tKLD:17.3181\tC_loss:0.0000\n",
      "Train Epoch: 34 [7680/60000 (13%)]\tLoss: 92.042183\tBCE:74.7670\tKLD:17.2752\tC_loss:0.0000\n",
      "Train Epoch: 34 [10240/60000 (17%)]\tLoss: 90.297089\tBCE:73.2274\tKLD:17.0697\tC_loss:0.0000\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 92.950600\tBCE:75.6066\tKLD:17.3440\tC_loss:0.0000\n",
      "Train Epoch: 34 [15360/60000 (26%)]\tLoss: 91.093445\tBCE:74.3202\tKLD:16.7733\tC_loss:0.0000\n",
      "Train Epoch: 34 [17920/60000 (30%)]\tLoss: 92.476501\tBCE:75.0510\tKLD:17.4255\tC_loss:0.0000\n",
      "Train Epoch: 34 [20480/60000 (34%)]\tLoss: 91.299789\tBCE:73.7614\tKLD:17.5384\tC_loss:0.0000\n",
      "Train Epoch: 34 [23040/60000 (38%)]\tLoss: 91.552345\tBCE:74.0697\tKLD:17.4827\tC_loss:0.0000\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 88.659256\tBCE:71.6121\tKLD:17.0471\tC_loss:0.0000\n",
      "Train Epoch: 34 [28160/60000 (47%)]\tLoss: 92.024483\tBCE:74.8730\tKLD:17.1515\tC_loss:0.0000\n",
      "Train Epoch: 34 [30720/60000 (51%)]\tLoss: 93.827751\tBCE:76.6753\tKLD:17.1524\tC_loss:0.0000\n",
      "Train Epoch: 34 [33280/60000 (56%)]\tLoss: 92.528732\tBCE:75.4078\tKLD:17.1210\tC_loss:0.0000\n",
      "Train Epoch: 34 [35840/60000 (60%)]\tLoss: 91.320053\tBCE:73.8286\tKLD:17.4914\tC_loss:0.0000\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 91.718048\tBCE:74.3857\tKLD:17.3323\tC_loss:0.0000\n",
      "Train Epoch: 34 [40960/60000 (68%)]\tLoss: 91.819351\tBCE:74.8630\tKLD:16.9564\tC_loss:0.0000\n",
      "Train Epoch: 34 [43520/60000 (73%)]\tLoss: 91.108131\tBCE:73.4074\tKLD:17.7007\tC_loss:0.0000\n",
      "Train Epoch: 34 [46080/60000 (77%)]\tLoss: 88.710625\tBCE:71.9046\tKLD:16.8060\tC_loss:0.0000\n",
      "Train Epoch: 34 [48640/60000 (81%)]\tLoss: 92.633232\tBCE:75.3631\tKLD:17.2701\tC_loss:0.0000\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 92.722832\tBCE:75.2147\tKLD:17.5081\tC_loss:0.0000\n",
      "Train Epoch: 34 [53760/60000 (90%)]\tLoss: 92.508316\tBCE:75.0429\tKLD:17.4654\tC_loss:0.0000\n",
      "Train Epoch: 34 [56320/60000 (94%)]\tLoss: 93.762741\tBCE:76.8442\tKLD:16.9186\tC_loss:0.0000\n",
      "Train Epoch: 34 [58880/60000 (98%)]\tLoss: 92.064392\tBCE:74.7149\tKLD:17.3495\tC_loss:0.0000\n",
      "====> Epoch: 34 Average loss: 91.5419\tClassifier Accuracy: 99.9783\n",
      "====> Test set loss: 92.5354\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 91.857269\tBCE:74.8783\tKLD:16.9790\tC_loss:0.0000\n",
      "Train Epoch: 35 [2560/60000 (4%)]\tLoss: 90.737930\tBCE:73.3699\tKLD:17.3680\tC_loss:0.0000\n",
      "Train Epoch: 35 [5120/60000 (9%)]\tLoss: 91.107269\tBCE:73.5195\tKLD:17.5878\tC_loss:0.0000\n",
      "Train Epoch: 35 [7680/60000 (13%)]\tLoss: 90.408974\tBCE:73.5839\tKLD:16.8250\tC_loss:0.0000\n",
      "Train Epoch: 35 [10240/60000 (17%)]\tLoss: 91.789719\tBCE:75.0632\tKLD:16.7265\tC_loss:0.0000\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 93.293594\tBCE:75.8201\tKLD:17.4735\tC_loss:0.0000\n",
      "Train Epoch: 35 [15360/60000 (26%)]\tLoss: 91.689468\tBCE:74.1920\tKLD:17.4975\tC_loss:0.0000\n",
      "Train Epoch: 35 [17920/60000 (30%)]\tLoss: 92.236893\tBCE:74.9479\tKLD:17.2889\tC_loss:0.0000\n",
      "Train Epoch: 35 [20480/60000 (34%)]\tLoss: 90.947327\tBCE:73.7323\tKLD:17.2151\tC_loss:0.0000\n",
      "Train Epoch: 35 [23040/60000 (38%)]\tLoss: 89.013626\tBCE:71.9357\tKLD:17.0780\tC_loss:0.0000\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 92.366600\tBCE:74.9186\tKLD:17.4480\tC_loss:0.0000\n",
      "Train Epoch: 35 [28160/60000 (47%)]\tLoss: 90.778595\tBCE:73.6837\tKLD:17.0949\tC_loss:0.0000\n",
      "Train Epoch: 35 [30720/60000 (51%)]\tLoss: 94.633286\tBCE:77.1477\tKLD:17.4856\tC_loss:0.0000\n",
      "Train Epoch: 35 [33280/60000 (56%)]\tLoss: 90.130409\tBCE:73.1179\tKLD:17.0125\tC_loss:0.0000\n",
      "Train Epoch: 35 [35840/60000 (60%)]\tLoss: 89.742416\tBCE:72.2111\tKLD:17.5313\tC_loss:0.0000\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 92.052002\tBCE:74.6241\tKLD:17.4279\tC_loss:0.0000\n",
      "Train Epoch: 35 [40960/60000 (68%)]\tLoss: 93.312576\tBCE:75.3555\tKLD:17.9571\tC_loss:0.0000\n",
      "Train Epoch: 35 [43520/60000 (73%)]\tLoss: 89.319084\tBCE:71.8627\tKLD:17.4563\tC_loss:0.0000\n",
      "Train Epoch: 35 [46080/60000 (77%)]\tLoss: 90.341370\tBCE:73.5163\tKLD:16.8251\tC_loss:0.0000\n",
      "Train Epoch: 35 [48640/60000 (81%)]\tLoss: 92.596069\tBCE:75.5477\tKLD:17.0484\tC_loss:0.0000\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 89.920441\tBCE:73.0067\tKLD:16.9137\tC_loss:0.0000\n",
      "Train Epoch: 35 [53760/60000 (90%)]\tLoss: 91.392776\tBCE:74.4451\tKLD:16.9477\tC_loss:0.0000\n",
      "Train Epoch: 35 [56320/60000 (94%)]\tLoss: 92.855713\tBCE:75.4939\tKLD:17.3618\tC_loss:0.0000\n",
      "Train Epoch: 35 [58880/60000 (98%)]\tLoss: 89.726929\tBCE:72.5497\tKLD:17.1772\tC_loss:0.0000\n",
      "====> Epoch: 35 Average loss: 91.6451\tClassifier Accuracy: 99.9800\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.9774\n",
      "Random number: 6\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 92.177078\tBCE:74.4016\tKLD:17.7755\tC_loss:0.0000\n",
      "Train Epoch: 36 [2560/60000 (4%)]\tLoss: 90.436821\tBCE:73.1350\tKLD:17.3018\tC_loss:0.0000\n",
      "Train Epoch: 36 [5120/60000 (9%)]\tLoss: 94.977814\tBCE:77.1355\tKLD:17.8423\tC_loss:0.0000\n",
      "Train Epoch: 36 [7680/60000 (13%)]\tLoss: 89.913971\tBCE:72.6017\tKLD:17.3123\tC_loss:0.0000\n",
      "Train Epoch: 36 [10240/60000 (17%)]\tLoss: 91.613716\tBCE:74.0040\tKLD:17.6097\tC_loss:0.0000\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 91.467155\tBCE:74.1234\tKLD:17.3438\tC_loss:0.0000\n",
      "Train Epoch: 36 [15360/60000 (26%)]\tLoss: 93.821304\tBCE:76.0953\tKLD:17.7260\tC_loss:0.0000\n",
      "Train Epoch: 36 [17920/60000 (30%)]\tLoss: 91.373238\tBCE:73.7978\tKLD:17.5755\tC_loss:0.0000\n",
      "Train Epoch: 36 [20480/60000 (34%)]\tLoss: 94.133324\tBCE:76.4928\tKLD:17.6405\tC_loss:0.0000\n",
      "Train Epoch: 36 [23040/60000 (38%)]\tLoss: 89.968727\tBCE:72.6073\tKLD:17.3614\tC_loss:0.0000\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 92.325775\tBCE:75.5260\tKLD:16.7997\tC_loss:0.0000\n",
      "Train Epoch: 36 [28160/60000 (47%)]\tLoss: 90.287025\tBCE:73.0755\tKLD:17.2115\tC_loss:0.0000\n",
      "Train Epoch: 36 [30720/60000 (51%)]\tLoss: 91.700729\tBCE:74.2570\tKLD:17.4437\tC_loss:0.0000\n",
      "Train Epoch: 36 [33280/60000 (56%)]\tLoss: 92.632080\tBCE:75.3793\tKLD:17.2527\tC_loss:0.0000\n",
      "Train Epoch: 36 [35840/60000 (60%)]\tLoss: 90.197220\tBCE:72.9407\tKLD:17.2565\tC_loss:0.0000\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 88.240555\tBCE:71.7064\tKLD:16.5341\tC_loss:0.0000\n",
      "Train Epoch: 36 [40960/60000 (68%)]\tLoss: 93.751099\tBCE:76.2502\tKLD:17.5009\tC_loss:0.0000\n",
      "Train Epoch: 36 [43520/60000 (73%)]\tLoss: 88.079941\tBCE:70.9880\tKLD:17.0919\tC_loss:0.0000\n",
      "Train Epoch: 36 [46080/60000 (77%)]\tLoss: 91.630081\tBCE:74.4308\tKLD:17.1992\tC_loss:0.0000\n",
      "Train Epoch: 36 [48640/60000 (81%)]\tLoss: 92.045586\tBCE:74.7640\tKLD:17.2816\tC_loss:0.0000\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 89.418556\tBCE:72.5949\tKLD:16.8237\tC_loss:0.0000\n",
      "Train Epoch: 36 [53760/60000 (90%)]\tLoss: 90.412437\tBCE:73.5977\tKLD:16.8148\tC_loss:0.0000\n",
      "Train Epoch: 36 [56320/60000 (94%)]\tLoss: 92.674095\tBCE:75.0136\tKLD:17.6605\tC_loss:0.0000\n",
      "Train Epoch: 36 [58880/60000 (98%)]\tLoss: 90.548096\tBCE:73.7684\tKLD:16.7797\tC_loss:0.0000\n",
      "====> Epoch: 36 Average loss: 91.4211\tClassifier Accuracy: 99.9716\n",
      "====> Test set loss: 92.0234\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 90.812767\tBCE:73.5086\tKLD:17.3042\tC_loss:0.0000\n",
      "Train Epoch: 37 [2560/60000 (4%)]\tLoss: 92.444939\tBCE:74.4138\tKLD:18.0311\tC_loss:0.0000\n",
      "Train Epoch: 37 [5120/60000 (9%)]\tLoss: 89.495300\tBCE:72.4018\tKLD:17.0935\tC_loss:0.0000\n",
      "Train Epoch: 37 [7680/60000 (13%)]\tLoss: 91.068314\tBCE:73.9662\tKLD:17.1022\tC_loss:0.0000\n",
      "Train Epoch: 37 [10240/60000 (17%)]\tLoss: 90.510757\tBCE:73.3605\tKLD:17.1503\tC_loss:0.0000\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 91.982857\tBCE:74.4978\tKLD:17.4851\tC_loss:0.0000\n",
      "Train Epoch: 37 [15360/60000 (26%)]\tLoss: 89.617096\tBCE:72.3163\tKLD:17.3008\tC_loss:0.0000\n",
      "Train Epoch: 37 [17920/60000 (30%)]\tLoss: 91.628273\tBCE:74.4265\tKLD:17.2017\tC_loss:0.0000\n",
      "Train Epoch: 37 [20480/60000 (34%)]\tLoss: 91.556824\tBCE:74.7099\tKLD:16.8469\tC_loss:0.0000\n",
      "Train Epoch: 37 [23040/60000 (38%)]\tLoss: 93.900902\tBCE:76.2658\tKLD:17.6351\tC_loss:0.0000\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 89.811523\tBCE:72.4981\tKLD:17.3134\tC_loss:0.0000\n",
      "Train Epoch: 37 [28160/60000 (47%)]\tLoss: 89.306580\tBCE:71.7805\tKLD:17.5261\tC_loss:0.0000\n",
      "Train Epoch: 37 [30720/60000 (51%)]\tLoss: 93.633514\tBCE:76.3319\tKLD:17.3016\tC_loss:0.0000\n",
      "Train Epoch: 37 [33280/60000 (56%)]\tLoss: 93.183319\tBCE:75.4651\tKLD:17.7183\tC_loss:0.0000\n",
      "Train Epoch: 37 [35840/60000 (60%)]\tLoss: 88.351273\tBCE:71.4024\tKLD:16.9489\tC_loss:0.0000\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 93.028000\tBCE:75.7366\tKLD:17.2914\tC_loss:0.0000\n",
      "Train Epoch: 37 [40960/60000 (68%)]\tLoss: 93.709450\tBCE:76.4905\tKLD:17.2189\tC_loss:0.0000\n",
      "Train Epoch: 37 [43520/60000 (73%)]\tLoss: 90.523941\tBCE:73.6599\tKLD:16.8640\tC_loss:0.0000\n",
      "Train Epoch: 37 [46080/60000 (77%)]\tLoss: 89.605797\tBCE:72.8767\tKLD:16.7291\tC_loss:0.0000\n",
      "Train Epoch: 37 [48640/60000 (81%)]\tLoss: 91.869904\tBCE:74.5048\tKLD:17.3651\tC_loss:0.0000\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 93.417938\tBCE:76.0194\tKLD:17.3985\tC_loss:0.0000\n",
      "Train Epoch: 37 [53760/60000 (90%)]\tLoss: 93.475311\tBCE:76.0836\tKLD:17.3917\tC_loss:0.0000\n",
      "Train Epoch: 37 [56320/60000 (94%)]\tLoss: 90.692780\tBCE:73.6764\tKLD:17.0164\tC_loss:0.0000\n",
      "Train Epoch: 37 [58880/60000 (98%)]\tLoss: 93.439804\tBCE:75.7723\tKLD:17.6675\tC_loss:0.0000\n",
      "====> Epoch: 37 Average loss: 91.3418\tClassifier Accuracy: 99.9850\n",
      "====> Test set loss: 92.4407\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 92.576248\tBCE:75.4092\tKLD:17.1670\tC_loss:0.0000\n",
      "Train Epoch: 38 [2560/60000 (4%)]\tLoss: 93.693726\tBCE:75.6714\tKLD:18.0224\tC_loss:0.0000\n",
      "Train Epoch: 38 [5120/60000 (9%)]\tLoss: 93.561295\tBCE:76.0213\tKLD:17.5400\tC_loss:0.0000\n",
      "Train Epoch: 38 [7680/60000 (13%)]\tLoss: 91.119919\tBCE:74.0415\tKLD:17.0784\tC_loss:0.0000\n",
      "Train Epoch: 38 [10240/60000 (17%)]\tLoss: 91.794708\tBCE:73.8334\tKLD:17.9613\tC_loss:0.0000\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 93.644150\tBCE:76.1635\tKLD:17.4806\tC_loss:0.0000\n",
      "Train Epoch: 38 [15360/60000 (26%)]\tLoss: 93.051773\tBCE:75.2092\tKLD:17.8426\tC_loss:0.0000\n",
      "Train Epoch: 38 [17920/60000 (30%)]\tLoss: 92.263596\tBCE:74.5405\tKLD:17.7231\tC_loss:0.0000\n",
      "Train Epoch: 38 [20480/60000 (34%)]\tLoss: 91.046967\tBCE:73.8451\tKLD:17.2019\tC_loss:0.0000\n",
      "Train Epoch: 38 [23040/60000 (38%)]\tLoss: 91.046448\tBCE:74.0452\tKLD:17.0013\tC_loss:0.0000\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 91.338165\tBCE:74.1488\tKLD:17.1893\tC_loss:0.0000\n",
      "Train Epoch: 38 [28160/60000 (47%)]\tLoss: 92.017761\tBCE:73.9693\tKLD:18.0484\tC_loss:0.0000\n",
      "Train Epoch: 38 [30720/60000 (51%)]\tLoss: 89.008720\tBCE:71.7408\tKLD:17.2679\tC_loss:0.0000\n",
      "Train Epoch: 38 [33280/60000 (56%)]\tLoss: 94.592155\tBCE:77.0911\tKLD:17.5011\tC_loss:0.0000\n",
      "Train Epoch: 38 [35840/60000 (60%)]\tLoss: 91.737526\tBCE:74.0874\tKLD:17.6502\tC_loss:0.0000\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 89.896950\tBCE:72.9496\tKLD:16.9473\tC_loss:0.0000\n",
      "Train Epoch: 38 [40960/60000 (68%)]\tLoss: 92.611710\tBCE:75.1745\tKLD:17.4372\tC_loss:0.0000\n",
      "Train Epoch: 38 [43520/60000 (73%)]\tLoss: 93.737892\tBCE:76.2855\tKLD:17.4524\tC_loss:0.0000\n",
      "Train Epoch: 38 [46080/60000 (77%)]\tLoss: 91.739334\tBCE:74.3659\tKLD:17.3734\tC_loss:0.0000\n",
      "Train Epoch: 38 [48640/60000 (81%)]\tLoss: 89.156677\tBCE:72.2435\tKLD:16.9132\tC_loss:0.0000\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 89.951736\tBCE:73.1832\tKLD:16.7685\tC_loss:0.0000\n",
      "Train Epoch: 38 [53760/60000 (90%)]\tLoss: 93.239342\tBCE:75.8007\tKLD:17.4387\tC_loss:0.0000\n",
      "Train Epoch: 38 [56320/60000 (94%)]\tLoss: 89.927658\tBCE:72.5776\tKLD:17.3501\tC_loss:0.0000\n",
      "Train Epoch: 38 [58880/60000 (98%)]\tLoss: 91.981262\tBCE:75.2088\tKLD:16.7724\tC_loss:0.0000\n",
      "====> Epoch: 38 Average loss: 91.2804\tClassifier Accuracy: 99.9783\n",
      "====> Test set loss: 93.0777\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 90.477623\tBCE:73.3345\tKLD:17.1431\tC_loss:0.0000\n",
      "Train Epoch: 39 [2560/60000 (4%)]\tLoss: 92.873123\tBCE:74.8180\tKLD:18.0551\tC_loss:0.0000\n",
      "Train Epoch: 39 [5120/60000 (9%)]\tLoss: 91.328430\tBCE:74.1387\tKLD:17.1897\tC_loss:0.0000\n",
      "Train Epoch: 39 [7680/60000 (13%)]\tLoss: 91.579521\tBCE:74.5638\tKLD:17.0158\tC_loss:0.0000\n",
      "Train Epoch: 39 [10240/60000 (17%)]\tLoss: 90.785271\tBCE:73.4394\tKLD:17.3459\tC_loss:0.0000\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 88.842316\tBCE:71.7337\tKLD:17.1086\tC_loss:0.0000\n",
      "Train Epoch: 39 [15360/60000 (26%)]\tLoss: 90.818436\tBCE:73.7695\tKLD:17.0490\tC_loss:0.0000\n",
      "Train Epoch: 39 [17920/60000 (30%)]\tLoss: 91.582809\tBCE:73.4508\tKLD:18.1320\tC_loss:0.0000\n",
      "Train Epoch: 39 [20480/60000 (34%)]\tLoss: 93.027321\tBCE:75.7555\tKLD:17.2718\tC_loss:0.0000\n",
      "Train Epoch: 39 [23040/60000 (38%)]\tLoss: 92.544518\tBCE:75.1513\tKLD:17.3932\tC_loss:0.0000\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 90.128777\tBCE:73.1142\tKLD:17.0146\tC_loss:0.0000\n",
      "Train Epoch: 39 [28160/60000 (47%)]\tLoss: 93.887184\tBCE:76.4224\tKLD:17.4648\tC_loss:0.0000\n",
      "Train Epoch: 39 [30720/60000 (51%)]\tLoss: 91.974632\tBCE:74.6361\tKLD:17.3385\tC_loss:0.0000\n",
      "Train Epoch: 39 [33280/60000 (56%)]\tLoss: 91.328346\tBCE:73.8893\tKLD:17.4391\tC_loss:0.0000\n",
      "Train Epoch: 39 [35840/60000 (60%)]\tLoss: 90.301788\tBCE:72.9593\tKLD:17.3425\tC_loss:0.0000\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 91.088226\tBCE:73.4238\tKLD:17.6644\tC_loss:0.0000\n",
      "Train Epoch: 39 [40960/60000 (68%)]\tLoss: 91.068336\tBCE:73.6965\tKLD:17.3718\tC_loss:0.0000\n",
      "Train Epoch: 39 [43520/60000 (73%)]\tLoss: 92.389328\tBCE:74.6835\tKLD:17.7059\tC_loss:0.0000\n",
      "Train Epoch: 39 [46080/60000 (77%)]\tLoss: 90.714378\tBCE:73.3871\tKLD:17.3273\tC_loss:0.0000\n",
      "Train Epoch: 39 [48640/60000 (81%)]\tLoss: 90.582504\tBCE:73.6189\tKLD:16.9636\tC_loss:0.0000\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 91.025581\tBCE:74.0475\tKLD:16.9781\tC_loss:0.0000\n",
      "Train Epoch: 39 [53760/60000 (90%)]\tLoss: 92.727608\tBCE:75.2080\tKLD:17.5196\tC_loss:0.0000\n",
      "Train Epoch: 39 [56320/60000 (94%)]\tLoss: 91.101624\tBCE:73.5950\tKLD:17.5066\tC_loss:0.0000\n",
      "Train Epoch: 39 [58880/60000 (98%)]\tLoss: 90.321365\tBCE:73.2921\tKLD:17.0293\tC_loss:0.0000\n",
      "====> Epoch: 39 Average loss: 91.1735\tClassifier Accuracy: 99.9583\n",
      "====> Test set loss: 91.9734\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 91.305344\tBCE:73.4014\tKLD:17.9040\tC_loss:0.0000\n",
      "Train Epoch: 40 [2560/60000 (4%)]\tLoss: 92.902153\tBCE:75.4997\tKLD:17.4025\tC_loss:0.0000\n",
      "Train Epoch: 40 [5120/60000 (9%)]\tLoss: 88.288567\tBCE:70.9390\tKLD:17.3495\tC_loss:0.0000\n",
      "Train Epoch: 40 [7680/60000 (13%)]\tLoss: 91.638977\tBCE:73.6923\tKLD:17.9467\tC_loss:0.0000\n",
      "Train Epoch: 40 [10240/60000 (17%)]\tLoss: 89.688751\tBCE:72.2030\tKLD:17.4858\tC_loss:0.0000\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 91.067055\tBCE:73.5064\tKLD:17.5606\tC_loss:0.0000\n",
      "Train Epoch: 40 [15360/60000 (26%)]\tLoss: 91.520775\tBCE:73.8510\tKLD:17.6698\tC_loss:0.0000\n",
      "Train Epoch: 40 [17920/60000 (30%)]\tLoss: 91.833160\tBCE:74.6021\tKLD:17.2310\tC_loss:0.0000\n",
      "Train Epoch: 40 [20480/60000 (34%)]\tLoss: 89.445297\tBCE:71.9610\tKLD:17.4843\tC_loss:0.0000\n",
      "Train Epoch: 40 [23040/60000 (38%)]\tLoss: 93.230591\tBCE:76.0414\tKLD:17.1892\tC_loss:0.0000\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 91.622147\tBCE:74.3617\tKLD:17.2604\tC_loss:0.0000\n",
      "Train Epoch: 40 [28160/60000 (47%)]\tLoss: 92.645081\tBCE:75.3798\tKLD:17.2653\tC_loss:0.0000\n",
      "Train Epoch: 40 [30720/60000 (51%)]\tLoss: 92.311455\tBCE:74.5701\tKLD:17.7413\tC_loss:0.0000\n",
      "Train Epoch: 40 [33280/60000 (56%)]\tLoss: 91.934402\tBCE:74.1952\tKLD:17.7392\tC_loss:0.0000\n",
      "Train Epoch: 40 [35840/60000 (60%)]\tLoss: 90.926224\tBCE:73.2228\tKLD:17.7034\tC_loss:0.0000\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 92.809357\tBCE:75.2523\tKLD:17.5570\tC_loss:0.0000\n",
      "Train Epoch: 40 [40960/60000 (68%)]\tLoss: 90.988503\tBCE:73.5334\tKLD:17.4551\tC_loss:0.0000\n",
      "Train Epoch: 40 [43520/60000 (73%)]\tLoss: 90.505524\tBCE:73.3645\tKLD:17.1410\tC_loss:0.0000\n",
      "Train Epoch: 40 [46080/60000 (77%)]\tLoss: 88.102707\tBCE:70.5740\tKLD:17.5287\tC_loss:0.0000\n",
      "Train Epoch: 40 [48640/60000 (81%)]\tLoss: 91.314285\tBCE:74.3889\tKLD:16.9254\tC_loss:0.0000\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 86.769638\tBCE:69.6818\tKLD:17.0878\tC_loss:0.0000\n",
      "Train Epoch: 40 [53760/60000 (90%)]\tLoss: 92.637863\tBCE:75.5967\tKLD:17.0412\tC_loss:0.0000\n",
      "Train Epoch: 40 [56320/60000 (94%)]\tLoss: 89.904510\tBCE:72.9570\tKLD:16.9475\tC_loss:0.0000\n",
      "Train Epoch: 40 [58880/60000 (98%)]\tLoss: 90.616028\tBCE:72.9816\tKLD:17.6344\tC_loss:0.0000\n",
      "====> Epoch: 40 Average loss: 90.9840\tClassifier Accuracy: 99.9199\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 92.4912\n",
      "Random number: 1\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 92.376976\tBCE:75.1059\tKLD:17.2711\tC_loss:0.0000\n",
      "Train Epoch: 41 [2560/60000 (4%)]\tLoss: 92.489883\tBCE:75.1380\tKLD:17.3519\tC_loss:0.0000\n",
      "Train Epoch: 41 [5120/60000 (9%)]\tLoss: 91.080612\tBCE:73.4125\tKLD:17.6681\tC_loss:0.0000\n",
      "Train Epoch: 41 [7680/60000 (13%)]\tLoss: 92.519318\tBCE:74.5878\tKLD:17.9315\tC_loss:0.0000\n",
      "Train Epoch: 41 [10240/60000 (17%)]\tLoss: 93.059372\tBCE:75.4351\tKLD:17.6243\tC_loss:0.0000\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 90.454842\tBCE:73.1283\tKLD:17.3265\tC_loss:0.0000\n",
      "Train Epoch: 41 [15360/60000 (26%)]\tLoss: 89.606003\tBCE:72.2173\tKLD:17.3887\tC_loss:0.0000\n",
      "Train Epoch: 41 [17920/60000 (30%)]\tLoss: 93.495567\tBCE:75.2084\tKLD:18.2871\tC_loss:0.0000\n",
      "Train Epoch: 41 [20480/60000 (34%)]\tLoss: 87.364594\tBCE:70.5830\tKLD:16.7816\tC_loss:0.0000\n",
      "Train Epoch: 41 [23040/60000 (38%)]\tLoss: 92.020599\tBCE:74.3534\tKLD:17.6672\tC_loss:0.0000\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 91.490517\tBCE:74.3612\tKLD:17.1293\tC_loss:0.0000\n",
      "Train Epoch: 41 [28160/60000 (47%)]\tLoss: 89.517014\tBCE:71.9236\tKLD:17.5934\tC_loss:0.0000\n",
      "Train Epoch: 41 [30720/60000 (51%)]\tLoss: 90.049500\tBCE:72.6320\tKLD:17.4175\tC_loss:0.0000\n",
      "Train Epoch: 41 [33280/60000 (56%)]\tLoss: 92.335007\tBCE:75.0250\tKLD:17.3100\tC_loss:0.0000\n",
      "Train Epoch: 41 [35840/60000 (60%)]\tLoss: 90.931900\tBCE:73.2217\tKLD:17.7102\tC_loss:0.0000\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 91.085739\tBCE:73.8926\tKLD:17.1931\tC_loss:0.0000\n",
      "Train Epoch: 41 [40960/60000 (68%)]\tLoss: 88.817261\tBCE:71.8284\tKLD:16.9889\tC_loss:0.0000\n",
      "Train Epoch: 41 [43520/60000 (73%)]\tLoss: 88.456177\tBCE:71.6249\tKLD:16.8312\tC_loss:0.0000\n",
      "Train Epoch: 41 [46080/60000 (77%)]\tLoss: 90.770676\tBCE:73.2263\tKLD:17.5444\tC_loss:0.0000\n",
      "Train Epoch: 41 [48640/60000 (81%)]\tLoss: 92.552467\tBCE:75.1729\tKLD:17.3796\tC_loss:0.0000\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 91.322853\tBCE:74.0238\tKLD:17.2991\tC_loss:0.0000\n",
      "Train Epoch: 41 [53760/60000 (90%)]\tLoss: 93.310577\tBCE:75.9275\tKLD:17.3831\tC_loss:0.0000\n",
      "Train Epoch: 41 [56320/60000 (94%)]\tLoss: 89.457748\tBCE:72.4781\tKLD:16.9796\tC_loss:0.0000\n",
      "Train Epoch: 41 [58880/60000 (98%)]\tLoss: 92.499481\tBCE:74.4305\tKLD:18.0690\tC_loss:0.0000\n",
      "====> Epoch: 41 Average loss: 90.8297\tClassifier Accuracy: 99.9332\n",
      "====> Test set loss: 91.9526\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 91.298409\tBCE:74.0646\tKLD:17.2338\tC_loss:0.0000\n",
      "Train Epoch: 42 [2560/60000 (4%)]\tLoss: 89.935760\tBCE:72.2963\tKLD:17.6394\tC_loss:0.0000\n",
      "Train Epoch: 42 [5120/60000 (9%)]\tLoss: 92.510239\tBCE:75.4961\tKLD:17.0141\tC_loss:0.0000\n",
      "Train Epoch: 42 [7680/60000 (13%)]\tLoss: 91.539261\tBCE:73.8380\tKLD:17.7012\tC_loss:0.0000\n",
      "Train Epoch: 42 [10240/60000 (17%)]\tLoss: 93.050186\tBCE:75.2174\tKLD:17.8328\tC_loss:0.0000\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 93.032776\tBCE:75.1164\tKLD:17.9164\tC_loss:0.0000\n",
      "Train Epoch: 42 [15360/60000 (26%)]\tLoss: 90.643631\tBCE:73.0373\tKLD:17.6063\tC_loss:0.0000\n",
      "Train Epoch: 42 [17920/60000 (30%)]\tLoss: 92.624611\tBCE:75.1604\tKLD:17.4642\tC_loss:0.0000\n",
      "Train Epoch: 42 [20480/60000 (34%)]\tLoss: 92.734619\tBCE:75.3795\tKLD:17.3551\tC_loss:0.0000\n",
      "Train Epoch: 42 [23040/60000 (38%)]\tLoss: 94.494530\tBCE:76.9981\tKLD:17.4964\tC_loss:0.0000\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 92.235130\tBCE:74.6388\tKLD:17.5963\tC_loss:0.0000\n",
      "Train Epoch: 42 [28160/60000 (47%)]\tLoss: 89.979187\tBCE:72.5001\tKLD:17.4791\tC_loss:0.0000\n",
      "Train Epoch: 42 [30720/60000 (51%)]\tLoss: 93.011917\tBCE:75.6510\tKLD:17.3609\tC_loss:0.0000\n",
      "Train Epoch: 42 [33280/60000 (56%)]\tLoss: 87.529617\tBCE:70.6318\tKLD:16.8979\tC_loss:0.0000\n",
      "Train Epoch: 42 [35840/60000 (60%)]\tLoss: 88.814438\tBCE:71.6104\tKLD:17.2040\tC_loss:0.0000\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 90.663879\tBCE:73.3181\tKLD:17.3458\tC_loss:0.0000\n",
      "Train Epoch: 42 [40960/60000 (68%)]\tLoss: 91.730774\tBCE:74.0522\tKLD:17.6786\tC_loss:0.0000\n",
      "Train Epoch: 42 [43520/60000 (73%)]\tLoss: 86.879684\tBCE:70.0323\tKLD:16.8474\tC_loss:0.0000\n",
      "Train Epoch: 42 [46080/60000 (77%)]\tLoss: 91.536148\tBCE:74.0083\tKLD:17.5278\tC_loss:0.0000\n",
      "Train Epoch: 42 [48640/60000 (81%)]\tLoss: 89.050453\tBCE:71.8444\tKLD:17.2061\tC_loss:0.0000\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 92.342834\tBCE:74.3187\tKLD:18.0241\tC_loss:0.0000\n",
      "Train Epoch: 42 [53760/60000 (90%)]\tLoss: 91.387039\tBCE:73.8983\tKLD:17.4887\tC_loss:0.0000\n",
      "Train Epoch: 42 [56320/60000 (94%)]\tLoss: 90.543335\tBCE:73.3115\tKLD:17.2318\tC_loss:0.0000\n",
      "Train Epoch: 42 [58880/60000 (98%)]\tLoss: 92.031586\tBCE:74.8367\tKLD:17.1949\tC_loss:0.0000\n",
      "====> Epoch: 42 Average loss: 90.9503\tClassifier Accuracy: 99.9983\n",
      "====> Test set loss: 91.9945\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 90.788300\tBCE:73.5051\tKLD:17.2832\tC_loss:0.0000\n",
      "Train Epoch: 43 [2560/60000 (4%)]\tLoss: 90.270760\tBCE:72.9181\tKLD:17.3526\tC_loss:0.0000\n",
      "Train Epoch: 43 [5120/60000 (9%)]\tLoss: 91.016068\tBCE:73.8015\tKLD:17.2146\tC_loss:0.0000\n",
      "Train Epoch: 43 [7680/60000 (13%)]\tLoss: 88.290588\tBCE:70.7910\tKLD:17.4996\tC_loss:0.0000\n",
      "Train Epoch: 43 [10240/60000 (17%)]\tLoss: 88.458870\tBCE:71.0860\tKLD:17.3728\tC_loss:0.0000\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 90.184456\tBCE:72.6327\tKLD:17.5518\tC_loss:0.0000\n",
      "Train Epoch: 43 [15360/60000 (26%)]\tLoss: 89.491623\tBCE:72.0585\tKLD:17.4331\tC_loss:0.0000\n",
      "Train Epoch: 43 [17920/60000 (30%)]\tLoss: 93.387054\tBCE:76.2740\tKLD:17.1131\tC_loss:0.0000\n",
      "Train Epoch: 43 [20480/60000 (34%)]\tLoss: 87.646454\tBCE:70.7594\tKLD:16.8870\tC_loss:0.0000\n",
      "Train Epoch: 43 [23040/60000 (38%)]\tLoss: 88.099045\tBCE:71.4014\tKLD:16.6977\tC_loss:0.0000\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 88.273796\tBCE:71.3343\tKLD:16.9395\tC_loss:0.0000\n",
      "Train Epoch: 43 [28160/60000 (47%)]\tLoss: 91.244629\tBCE:74.0965\tKLD:17.1481\tC_loss:0.0000\n",
      "Train Epoch: 43 [30720/60000 (51%)]\tLoss: 91.086105\tBCE:73.8404\tKLD:17.2457\tC_loss:0.0000\n",
      "Train Epoch: 43 [33280/60000 (56%)]\tLoss: 92.244781\tBCE:74.7184\tKLD:17.5263\tC_loss:0.0000\n",
      "Train Epoch: 43 [35840/60000 (60%)]\tLoss: 92.497208\tBCE:75.2248\tKLD:17.2724\tC_loss:0.0000\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 93.888618\tBCE:76.3675\tKLD:17.5212\tC_loss:0.0000\n",
      "Train Epoch: 43 [40960/60000 (68%)]\tLoss: 92.972748\tBCE:75.7479\tKLD:17.2248\tC_loss:0.0000\n",
      "Train Epoch: 43 [43520/60000 (73%)]\tLoss: 90.106262\tBCE:72.8238\tKLD:17.2825\tC_loss:0.0000\n",
      "Train Epoch: 43 [46080/60000 (77%)]\tLoss: 88.544113\tBCE:71.3245\tKLD:17.2197\tC_loss:0.0000\n",
      "Train Epoch: 43 [48640/60000 (81%)]\tLoss: 89.154793\tBCE:71.8178\tKLD:17.3370\tC_loss:0.0000\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 93.022919\tBCE:75.7091\tKLD:17.3138\tC_loss:0.0000\n",
      "Train Epoch: 43 [53760/60000 (90%)]\tLoss: 92.121872\tBCE:74.7065\tKLD:17.4153\tC_loss:0.0000\n",
      "Train Epoch: 43 [56320/60000 (94%)]\tLoss: 90.478973\tBCE:73.2355\tKLD:17.2435\tC_loss:0.0000\n",
      "Train Epoch: 43 [58880/60000 (98%)]\tLoss: 93.263588\tBCE:76.0475\tKLD:17.2161\tC_loss:0.0000\n",
      "====> Epoch: 43 Average loss: 90.7688\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.7333\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 93.392868\tBCE:75.9286\tKLD:17.4642\tC_loss:0.0000\n",
      "Train Epoch: 44 [2560/60000 (4%)]\tLoss: 95.379822\tBCE:77.4072\tKLD:17.9727\tC_loss:0.0000\n",
      "Train Epoch: 44 [5120/60000 (9%)]\tLoss: 92.432236\tBCE:75.3204\tKLD:17.1119\tC_loss:0.0000\n",
      "Train Epoch: 44 [7680/60000 (13%)]\tLoss: 90.562500\tBCE:73.2648\tKLD:17.2977\tC_loss:0.0000\n",
      "Train Epoch: 44 [10240/60000 (17%)]\tLoss: 92.484970\tBCE:75.0279\tKLD:17.4571\tC_loss:0.0000\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 91.584641\tBCE:74.5727\tKLD:17.0119\tC_loss:0.0000\n",
      "Train Epoch: 44 [15360/60000 (26%)]\tLoss: 89.702728\tBCE:72.4282\tKLD:17.2745\tC_loss:0.0000\n",
      "Train Epoch: 44 [17920/60000 (30%)]\tLoss: 90.023087\tBCE:72.4188\tKLD:17.6042\tC_loss:0.0000\n",
      "Train Epoch: 44 [20480/60000 (34%)]\tLoss: 88.086723\tBCE:70.9478\tKLD:17.1390\tC_loss:0.0000\n",
      "Train Epoch: 44 [23040/60000 (38%)]\tLoss: 86.910759\tBCE:69.9979\tKLD:16.9128\tC_loss:0.0000\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 92.535713\tBCE:74.8775\tKLD:17.6582\tC_loss:0.0000\n",
      "Train Epoch: 44 [28160/60000 (47%)]\tLoss: 90.931610\tBCE:73.3461\tKLD:17.5856\tC_loss:0.0000\n",
      "Train Epoch: 44 [30720/60000 (51%)]\tLoss: 90.217323\tBCE:72.6079\tKLD:17.6094\tC_loss:0.0000\n",
      "Train Epoch: 44 [33280/60000 (56%)]\tLoss: 90.925629\tBCE:73.1839\tKLD:17.7417\tC_loss:0.0000\n",
      "Train Epoch: 44 [35840/60000 (60%)]\tLoss: 89.503670\tBCE:72.1869\tKLD:17.3168\tC_loss:0.0000\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 92.592880\tBCE:75.0102\tKLD:17.5827\tC_loss:0.0000\n",
      "Train Epoch: 44 [40960/60000 (68%)]\tLoss: 90.841232\tBCE:73.2016\tKLD:17.6397\tC_loss:0.0000\n",
      "Train Epoch: 44 [43520/60000 (73%)]\tLoss: 90.224014\tBCE:73.2158\tKLD:17.0082\tC_loss:0.0000\n",
      "Train Epoch: 44 [46080/60000 (77%)]\tLoss: 91.789566\tBCE:74.3803\tKLD:17.4093\tC_loss:0.0000\n",
      "Train Epoch: 44 [48640/60000 (81%)]\tLoss: 90.165474\tBCE:72.9358\tKLD:17.2297\tC_loss:0.0000\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 89.310410\tBCE:72.0684\tKLD:17.2420\tC_loss:0.0000\n",
      "Train Epoch: 44 [53760/60000 (90%)]\tLoss: 90.763405\tBCE:73.3540\tKLD:17.4094\tC_loss:0.0000\n",
      "Train Epoch: 44 [56320/60000 (94%)]\tLoss: 90.468544\tBCE:73.4202\tKLD:17.0483\tC_loss:0.0000\n",
      "Train Epoch: 44 [58880/60000 (98%)]\tLoss: 93.002998\tBCE:75.3551\tKLD:17.6479\tC_loss:0.0000\n",
      "====> Epoch: 44 Average loss: 90.6831\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 92.1931\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 88.093407\tBCE:71.0987\tKLD:16.9947\tC_loss:0.0000\n",
      "Train Epoch: 45 [2560/60000 (4%)]\tLoss: 92.250145\tBCE:75.0488\tKLD:17.2013\tC_loss:0.0000\n",
      "Train Epoch: 45 [5120/60000 (9%)]\tLoss: 90.165627\tBCE:72.5981\tKLD:17.5675\tC_loss:0.0000\n",
      "Train Epoch: 45 [7680/60000 (13%)]\tLoss: 88.660683\tBCE:71.9047\tKLD:16.7560\tC_loss:0.0000\n",
      "Train Epoch: 45 [10240/60000 (17%)]\tLoss: 91.631187\tBCE:73.9799\tKLD:17.6513\tC_loss:0.0000\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 88.116150\tBCE:70.7574\tKLD:17.3587\tC_loss:0.0000\n",
      "Train Epoch: 45 [15360/60000 (26%)]\tLoss: 88.525879\tBCE:71.8638\tKLD:16.6621\tC_loss:0.0000\n",
      "Train Epoch: 45 [17920/60000 (30%)]\tLoss: 91.437279\tBCE:73.7313\tKLD:17.7060\tC_loss:0.0000\n",
      "Train Epoch: 45 [20480/60000 (34%)]\tLoss: 90.505783\tBCE:72.5834\tKLD:17.9224\tC_loss:0.0000\n",
      "Train Epoch: 45 [23040/60000 (38%)]\tLoss: 87.575668\tBCE:70.5223\tKLD:17.0534\tC_loss:0.0000\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 90.523582\tBCE:72.9918\tKLD:17.5318\tC_loss:0.0000\n",
      "Train Epoch: 45 [28160/60000 (47%)]\tLoss: 92.643417\tBCE:75.2085\tKLD:17.4350\tC_loss:0.0000\n",
      "Train Epoch: 45 [30720/60000 (51%)]\tLoss: 94.157349\tBCE:75.9636\tKLD:18.1938\tC_loss:0.0000\n",
      "Train Epoch: 45 [33280/60000 (56%)]\tLoss: 88.598236\tBCE:71.6080\tKLD:16.9903\tC_loss:0.0000\n",
      "Train Epoch: 45 [35840/60000 (60%)]\tLoss: 89.478241\tBCE:72.1048\tKLD:17.3735\tC_loss:0.0000\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 88.628357\tBCE:71.0446\tKLD:17.5838\tC_loss:0.0000\n",
      "Train Epoch: 45 [40960/60000 (68%)]\tLoss: 91.251022\tBCE:73.7800\tKLD:17.4710\tC_loss:0.0000\n",
      "Train Epoch: 45 [43520/60000 (73%)]\tLoss: 93.138084\tBCE:75.3732\tKLD:17.7649\tC_loss:0.0000\n",
      "Train Epoch: 45 [46080/60000 (77%)]\tLoss: 91.014214\tBCE:73.5896\tKLD:17.4246\tC_loss:0.0000\n",
      "Train Epoch: 45 [48640/60000 (81%)]\tLoss: 93.496979\tBCE:75.0809\tKLD:18.4161\tC_loss:0.0000\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 93.127190\tBCE:75.1227\tKLD:18.0045\tC_loss:0.0000\n",
      "Train Epoch: 45 [53760/60000 (90%)]\tLoss: 94.462921\tBCE:77.0572\tKLD:17.4057\tC_loss:0.0000\n",
      "Train Epoch: 45 [56320/60000 (94%)]\tLoss: 90.737938\tBCE:73.2513\tKLD:17.4867\tC_loss:0.0000\n",
      "Train Epoch: 45 [58880/60000 (98%)]\tLoss: 90.858528\tBCE:73.0388\tKLD:17.8197\tC_loss:0.0000\n",
      "====> Epoch: 45 Average loss: 90.4723\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.6672\n",
      "Random number: 9\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 90.210770\tBCE:73.1777\tKLD:17.0331\tC_loss:0.0000\n",
      "Train Epoch: 46 [2560/60000 (4%)]\tLoss: 90.803391\tBCE:73.4431\tKLD:17.3603\tC_loss:0.0000\n",
      "Train Epoch: 46 [5120/60000 (9%)]\tLoss: 90.320923\tBCE:73.3941\tKLD:16.9268\tC_loss:0.0000\n",
      "Train Epoch: 46 [7680/60000 (13%)]\tLoss: 90.550247\tBCE:73.0315\tKLD:17.5187\tC_loss:0.0000\n",
      "Train Epoch: 46 [10240/60000 (17%)]\tLoss: 90.241043\tBCE:72.4655\tKLD:17.7756\tC_loss:0.0000\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 91.967834\tBCE:74.1656\tKLD:17.8022\tC_loss:0.0000\n",
      "Train Epoch: 46 [15360/60000 (26%)]\tLoss: 88.638245\tBCE:70.8554\tKLD:17.7829\tC_loss:0.0000\n",
      "Train Epoch: 46 [17920/60000 (30%)]\tLoss: 87.163658\tBCE:70.6793\tKLD:16.4844\tC_loss:0.0000\n",
      "Train Epoch: 46 [20480/60000 (34%)]\tLoss: 90.446922\tBCE:73.2334\tKLD:17.2135\tC_loss:0.0000\n",
      "Train Epoch: 46 [23040/60000 (38%)]\tLoss: 92.692833\tBCE:75.0558\tKLD:17.6371\tC_loss:0.0000\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 93.644638\tBCE:76.1830\tKLD:17.4617\tC_loss:0.0000\n",
      "Train Epoch: 46 [28160/60000 (47%)]\tLoss: 90.271492\tBCE:72.4721\tKLD:17.7994\tC_loss:0.0000\n",
      "Train Epoch: 46 [30720/60000 (51%)]\tLoss: 91.122757\tBCE:73.5813\tKLD:17.5414\tC_loss:0.0000\n",
      "Train Epoch: 46 [33280/60000 (56%)]\tLoss: 90.463829\tBCE:73.0818\tKLD:17.3820\tC_loss:0.0000\n",
      "Train Epoch: 46 [35840/60000 (60%)]\tLoss: 91.691635\tBCE:74.0256\tKLD:17.6660\tC_loss:0.0000\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 92.363998\tBCE:74.8452\tKLD:17.5188\tC_loss:0.0000\n",
      "Train Epoch: 46 [40960/60000 (68%)]\tLoss: 91.285423\tBCE:74.5461\tKLD:16.7393\tC_loss:0.0000\n",
      "Train Epoch: 46 [43520/60000 (73%)]\tLoss: 90.510391\tBCE:73.2390\tKLD:17.2714\tC_loss:0.0000\n",
      "Train Epoch: 46 [46080/60000 (77%)]\tLoss: 90.688126\tBCE:72.8695\tKLD:17.8186\tC_loss:0.0000\n",
      "Train Epoch: 46 [48640/60000 (81%)]\tLoss: 93.697861\tBCE:75.9667\tKLD:17.7312\tC_loss:0.0000\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 90.145393\tBCE:73.0460\tKLD:17.0994\tC_loss:0.0000\n",
      "Train Epoch: 46 [53760/60000 (90%)]\tLoss: 93.306519\tBCE:75.2782\tKLD:18.0283\tC_loss:0.0000\n",
      "Train Epoch: 46 [56320/60000 (94%)]\tLoss: 88.337830\tBCE:71.0830\tKLD:17.2548\tC_loss:0.0000\n",
      "Train Epoch: 46 [58880/60000 (98%)]\tLoss: 90.079292\tBCE:72.8688\tKLD:17.2105\tC_loss:0.0000\n",
      "====> Epoch: 46 Average loss: 90.6731\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.8367\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 87.763000\tBCE:70.8741\tKLD:16.8889\tC_loss:0.0000\n",
      "Train Epoch: 47 [2560/60000 (4%)]\tLoss: 89.477142\tBCE:72.4153\tKLD:17.0618\tC_loss:0.0000\n",
      "Train Epoch: 47 [5120/60000 (9%)]\tLoss: 93.024681\tBCE:75.5565\tKLD:17.4682\tC_loss:0.0000\n",
      "Train Epoch: 47 [7680/60000 (13%)]\tLoss: 91.685097\tBCE:73.9526\tKLD:17.7325\tC_loss:0.0000\n",
      "Train Epoch: 47 [10240/60000 (17%)]\tLoss: 89.856415\tBCE:72.8400\tKLD:17.0164\tC_loss:0.0000\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 90.278702\tBCE:72.7514\tKLD:17.5274\tC_loss:0.0000\n",
      "Train Epoch: 47 [15360/60000 (26%)]\tLoss: 87.798004\tBCE:71.1142\tKLD:16.6838\tC_loss:0.0000\n",
      "Train Epoch: 47 [17920/60000 (30%)]\tLoss: 90.728516\tBCE:73.2058\tKLD:17.5227\tC_loss:0.0000\n",
      "Train Epoch: 47 [20480/60000 (34%)]\tLoss: 87.784386\tBCE:70.6163\tKLD:17.1681\tC_loss:0.0000\n",
      "Train Epoch: 47 [23040/60000 (38%)]\tLoss: 91.514549\tBCE:73.5744\tKLD:17.9401\tC_loss:0.0000\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 90.833687\tBCE:73.2624\tKLD:17.5713\tC_loss:0.0000\n",
      "Train Epoch: 47 [28160/60000 (47%)]\tLoss: 92.013329\tBCE:74.4109\tKLD:17.6024\tC_loss:0.0000\n",
      "Train Epoch: 47 [30720/60000 (51%)]\tLoss: 89.755287\tBCE:72.3971\tKLD:17.3581\tC_loss:0.0000\n",
      "Train Epoch: 47 [33280/60000 (56%)]\tLoss: 93.530220\tBCE:75.8330\tKLD:17.6972\tC_loss:0.0000\n",
      "Train Epoch: 47 [35840/60000 (60%)]\tLoss: 89.198883\tBCE:71.7118\tKLD:17.4870\tC_loss:0.0000\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 93.282288\tBCE:75.8092\tKLD:17.4731\tC_loss:0.0000\n",
      "Train Epoch: 47 [40960/60000 (68%)]\tLoss: 92.689667\tBCE:74.8195\tKLD:17.8702\tC_loss:0.0000\n",
      "Train Epoch: 47 [43520/60000 (73%)]\tLoss: 88.158905\tBCE:70.9671\tKLD:17.1918\tC_loss:0.0000\n",
      "Train Epoch: 47 [46080/60000 (77%)]\tLoss: 92.765465\tBCE:74.9675\tKLD:17.7980\tC_loss:0.0000\n",
      "Train Epoch: 47 [48640/60000 (81%)]\tLoss: 93.388710\tBCE:75.4114\tKLD:17.9773\tC_loss:0.0000\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 91.060257\tBCE:73.4633\tKLD:17.5970\tC_loss:0.0000\n",
      "Train Epoch: 47 [53760/60000 (90%)]\tLoss: 92.493416\tBCE:75.0401\tKLD:17.4533\tC_loss:0.0000\n",
      "Train Epoch: 47 [56320/60000 (94%)]\tLoss: 91.584969\tBCE:74.0885\tKLD:17.4965\tC_loss:0.0000\n",
      "Train Epoch: 47 [58880/60000 (98%)]\tLoss: 91.977585\tBCE:74.2915\tKLD:17.6861\tC_loss:0.0000\n",
      "====> Epoch: 47 Average loss: 90.4998\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.7820\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 89.763458\tBCE:72.2486\tKLD:17.5148\tC_loss:0.0000\n",
      "Train Epoch: 48 [2560/60000 (4%)]\tLoss: 90.629662\tBCE:72.7908\tKLD:17.8389\tC_loss:0.0000\n",
      "Train Epoch: 48 [5120/60000 (9%)]\tLoss: 90.401672\tBCE:73.4728\tKLD:16.9289\tC_loss:0.0000\n",
      "Train Epoch: 48 [7680/60000 (13%)]\tLoss: 92.280792\tBCE:74.7053\tKLD:17.5755\tC_loss:0.0000\n",
      "Train Epoch: 48 [10240/60000 (17%)]\tLoss: 93.872688\tBCE:76.1095\tKLD:17.7632\tC_loss:0.0000\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 92.117218\tBCE:74.4779\tKLD:17.6393\tC_loss:0.0000\n",
      "Train Epoch: 48 [15360/60000 (26%)]\tLoss: 88.759232\tBCE:71.3014\tKLD:17.4579\tC_loss:0.0000\n",
      "Train Epoch: 48 [17920/60000 (30%)]\tLoss: 90.803474\tBCE:73.0574\tKLD:17.7460\tC_loss:0.0000\n",
      "Train Epoch: 48 [20480/60000 (34%)]\tLoss: 90.646652\tBCE:73.2531\tKLD:17.3935\tC_loss:0.0000\n",
      "Train Epoch: 48 [23040/60000 (38%)]\tLoss: 90.325867\tBCE:73.5284\tKLD:16.7975\tC_loss:0.0000\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 90.989525\tBCE:73.1472\tKLD:17.8423\tC_loss:0.0000\n",
      "Train Epoch: 48 [28160/60000 (47%)]\tLoss: 91.908752\tBCE:74.7798\tKLD:17.1290\tC_loss:0.0000\n",
      "Train Epoch: 48 [30720/60000 (51%)]\tLoss: 90.069321\tBCE:72.1655\tKLD:17.9038\tC_loss:0.0000\n",
      "Train Epoch: 48 [33280/60000 (56%)]\tLoss: 86.956779\tBCE:69.9510\tKLD:17.0058\tC_loss:0.0000\n",
      "Train Epoch: 48 [35840/60000 (60%)]\tLoss: 89.640358\tBCE:72.0356\tKLD:17.6048\tC_loss:0.0000\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 89.061348\tBCE:71.9545\tKLD:17.1068\tC_loss:0.0000\n",
      "Train Epoch: 48 [40960/60000 (68%)]\tLoss: 94.389160\tBCE:76.5629\tKLD:17.8263\tC_loss:0.0000\n",
      "Train Epoch: 48 [43520/60000 (73%)]\tLoss: 90.074753\tBCE:72.3909\tKLD:17.6839\tC_loss:0.0000\n",
      "Train Epoch: 48 [46080/60000 (77%)]\tLoss: 94.277527\tBCE:77.0666\tKLD:17.2110\tC_loss:0.0000\n",
      "Train Epoch: 48 [48640/60000 (81%)]\tLoss: 87.839333\tBCE:70.6163\tKLD:17.2230\tC_loss:0.0000\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 89.755280\tBCE:72.4101\tKLD:17.3451\tC_loss:0.0000\n",
      "Train Epoch: 48 [53760/60000 (90%)]\tLoss: 88.583443\tBCE:71.5618\tKLD:17.0216\tC_loss:0.0000\n",
      "Train Epoch: 48 [56320/60000 (94%)]\tLoss: 89.864700\tBCE:72.7776\tKLD:17.0871\tC_loss:0.0000\n",
      "Train Epoch: 48 [58880/60000 (98%)]\tLoss: 90.883530\tBCE:73.4677\tKLD:17.4158\tC_loss:0.0000\n",
      "====> Epoch: 48 Average loss: 90.4948\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.9545\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 91.577652\tBCE:74.4191\tKLD:17.1586\tC_loss:0.0000\n",
      "Train Epoch: 49 [2560/60000 (4%)]\tLoss: 89.834778\tBCE:72.0012\tKLD:17.8335\tC_loss:0.0000\n",
      "Train Epoch: 49 [5120/60000 (9%)]\tLoss: 88.098160\tBCE:70.8525\tKLD:17.2457\tC_loss:0.0000\n",
      "Train Epoch: 49 [7680/60000 (13%)]\tLoss: 88.450043\tBCE:71.3733\tKLD:17.0768\tC_loss:0.0000\n",
      "Train Epoch: 49 [10240/60000 (17%)]\tLoss: 90.678246\tBCE:73.2980\tKLD:17.3803\tC_loss:0.0000\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 89.568832\tBCE:71.9821\tKLD:17.5868\tC_loss:0.0000\n",
      "Train Epoch: 49 [15360/60000 (26%)]\tLoss: 90.033600\tBCE:72.6585\tKLD:17.3751\tC_loss:0.0000\n",
      "Train Epoch: 49 [17920/60000 (30%)]\tLoss: 91.854065\tBCE:74.3263\tKLD:17.5278\tC_loss:0.0000\n",
      "Train Epoch: 49 [20480/60000 (34%)]\tLoss: 88.993126\tBCE:71.6769\tKLD:17.3162\tC_loss:0.0000\n",
      "Train Epoch: 49 [23040/60000 (38%)]\tLoss: 92.704071\tBCE:74.8036\tKLD:17.9005\tC_loss:0.0000\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 90.938324\tBCE:73.3597\tKLD:17.5786\tC_loss:0.0000\n",
      "Train Epoch: 49 [28160/60000 (47%)]\tLoss: 90.866875\tBCE:73.3565\tKLD:17.5104\tC_loss:0.0000\n",
      "Train Epoch: 49 [30720/60000 (51%)]\tLoss: 90.675034\tBCE:73.1897\tKLD:17.4854\tC_loss:0.0000\n",
      "Train Epoch: 49 [33280/60000 (56%)]\tLoss: 90.126022\tBCE:72.4873\tKLD:17.6387\tC_loss:0.0000\n",
      "Train Epoch: 49 [35840/60000 (60%)]\tLoss: 91.383865\tBCE:73.7374\tKLD:17.6465\tC_loss:0.0000\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 93.603516\tBCE:76.0540\tKLD:17.5495\tC_loss:0.0000\n",
      "Train Epoch: 49 [40960/60000 (68%)]\tLoss: 86.666428\tBCE:69.3966\tKLD:17.2698\tC_loss:0.0000\n",
      "Train Epoch: 49 [43520/60000 (73%)]\tLoss: 90.705795\tBCE:73.2330\tKLD:17.4728\tC_loss:0.0000\n",
      "Train Epoch: 49 [46080/60000 (77%)]\tLoss: 91.184105\tBCE:73.8439\tKLD:17.3402\tC_loss:0.0000\n",
      "Train Epoch: 49 [48640/60000 (81%)]\tLoss: 91.472908\tBCE:73.7848\tKLD:17.6881\tC_loss:0.0000\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 91.811172\tBCE:74.5481\tKLD:17.2631\tC_loss:0.0000\n",
      "Train Epoch: 49 [53760/60000 (90%)]\tLoss: 87.062256\tBCE:70.2171\tKLD:16.8451\tC_loss:0.0000\n",
      "Train Epoch: 49 [56320/60000 (94%)]\tLoss: 91.273758\tBCE:73.8957\tKLD:17.3781\tC_loss:0.0000\n",
      "Train Epoch: 49 [58880/60000 (98%)]\tLoss: 90.884033\tBCE:73.1844\tKLD:17.6996\tC_loss:0.0000\n",
      "====> Epoch: 49 Average loss: 90.2820\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.7436\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 89.587639\tBCE:72.1101\tKLD:17.4775\tC_loss:0.0000\n",
      "Train Epoch: 50 [2560/60000 (4%)]\tLoss: 90.705368\tBCE:73.2428\tKLD:17.4625\tC_loss:0.0000\n",
      "Train Epoch: 50 [5120/60000 (9%)]\tLoss: 91.967285\tBCE:74.2883\tKLD:17.6790\tC_loss:0.0000\n",
      "Train Epoch: 50 [7680/60000 (13%)]\tLoss: 91.890945\tBCE:74.2695\tKLD:17.6215\tC_loss:0.0000\n",
      "Train Epoch: 50 [10240/60000 (17%)]\tLoss: 90.091194\tBCE:72.7633\tKLD:17.3279\tC_loss:0.0000\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 90.032944\tBCE:72.6988\tKLD:17.3341\tC_loss:0.0000\n",
      "Train Epoch: 50 [15360/60000 (26%)]\tLoss: 91.842674\tBCE:73.9100\tKLD:17.9327\tC_loss:0.0000\n",
      "Train Epoch: 50 [17920/60000 (30%)]\tLoss: 90.140991\tBCE:72.4769\tKLD:17.6641\tC_loss:0.0000\n",
      "Train Epoch: 50 [20480/60000 (34%)]\tLoss: 91.339729\tBCE:74.5644\tKLD:16.7753\tC_loss:0.0000\n",
      "Train Epoch: 50 [23040/60000 (38%)]\tLoss: 91.558365\tBCE:74.1641\tKLD:17.3942\tC_loss:0.0000\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 90.555420\tBCE:73.4695\tKLD:17.0859\tC_loss:0.0000\n",
      "Train Epoch: 50 [28160/60000 (47%)]\tLoss: 90.894730\tBCE:73.4126\tKLD:17.4821\tC_loss:0.0000\n",
      "Train Epoch: 50 [30720/60000 (51%)]\tLoss: 92.033051\tBCE:74.2110\tKLD:17.8220\tC_loss:0.0000\n",
      "Train Epoch: 50 [33280/60000 (56%)]\tLoss: 92.765045\tBCE:75.7664\tKLD:16.9986\tC_loss:0.0000\n",
      "Train Epoch: 50 [35840/60000 (60%)]\tLoss: 91.509499\tBCE:73.4580\tKLD:18.0515\tC_loss:0.0000\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 93.915970\tBCE:76.3242\tKLD:17.5918\tC_loss:0.0000\n",
      "Train Epoch: 50 [40960/60000 (68%)]\tLoss: 88.073456\tBCE:70.5099\tKLD:17.5636\tC_loss:0.0000\n",
      "Train Epoch: 50 [43520/60000 (73%)]\tLoss: 90.456825\tBCE:72.8682\tKLD:17.5887\tC_loss:0.0000\n",
      "Train Epoch: 50 [46080/60000 (77%)]\tLoss: 88.995926\tBCE:71.4816\tKLD:17.5143\tC_loss:0.0000\n",
      "Train Epoch: 50 [48640/60000 (81%)]\tLoss: 90.466553\tBCE:72.9165\tKLD:17.5501\tC_loss:0.0000\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 91.183777\tBCE:73.6364\tKLD:17.5474\tC_loss:0.0000\n",
      "Train Epoch: 50 [53760/60000 (90%)]\tLoss: 90.548752\tBCE:73.3971\tKLD:17.1516\tC_loss:0.0000\n",
      "Train Epoch: 50 [56320/60000 (94%)]\tLoss: 92.582695\tBCE:74.6515\tKLD:17.9312\tC_loss:0.0000\n",
      "Train Epoch: 50 [58880/60000 (98%)]\tLoss: 87.706017\tBCE:71.0019\tKLD:16.7042\tC_loss:0.0000\n",
      "====> Epoch: 50 Average loss: 90.2214\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.6655\n",
      "Random number: 6\n",
      "Train Epoch: 51 [0/60000 (0%)]\tLoss: 88.917442\tBCE:71.5450\tKLD:17.3725\tC_loss:0.0000\n",
      "Train Epoch: 51 [2560/60000 (4%)]\tLoss: 88.897308\tBCE:71.8662\tKLD:17.0311\tC_loss:0.0000\n",
      "Train Epoch: 51 [5120/60000 (9%)]\tLoss: 88.979385\tBCE:71.3209\tKLD:17.6585\tC_loss:0.0000\n",
      "Train Epoch: 51 [7680/60000 (13%)]\tLoss: 87.656143\tBCE:70.7204\tKLD:16.9358\tC_loss:0.0000\n",
      "Train Epoch: 51 [10240/60000 (17%)]\tLoss: 90.605804\tBCE:72.7743\tKLD:17.8315\tC_loss:0.0000\n",
      "Train Epoch: 51 [12800/60000 (21%)]\tLoss: 91.869644\tBCE:73.9480\tKLD:17.9216\tC_loss:0.0000\n",
      "Train Epoch: 51 [15360/60000 (26%)]\tLoss: 87.404358\tBCE:70.4105\tKLD:16.9939\tC_loss:0.0000\n",
      "Train Epoch: 51 [17920/60000 (30%)]\tLoss: 87.950562\tBCE:70.4118\tKLD:17.5388\tC_loss:0.0000\n",
      "Train Epoch: 51 [20480/60000 (34%)]\tLoss: 92.980194\tBCE:75.4709\tKLD:17.5093\tC_loss:0.0000\n",
      "Train Epoch: 51 [23040/60000 (38%)]\tLoss: 90.020409\tBCE:72.6147\tKLD:17.4057\tC_loss:0.0000\n",
      "Train Epoch: 51 [25600/60000 (43%)]\tLoss: 89.493851\tBCE:72.1624\tKLD:17.3314\tC_loss:0.0000\n",
      "Train Epoch: 51 [28160/60000 (47%)]\tLoss: 91.514206\tBCE:73.6836\tKLD:17.8306\tC_loss:0.0000\n",
      "Train Epoch: 51 [30720/60000 (51%)]\tLoss: 89.520294\tBCE:72.4533\tKLD:17.0670\tC_loss:0.0000\n",
      "Train Epoch: 51 [33280/60000 (56%)]\tLoss: 88.163239\tBCE:71.1334\tKLD:17.0299\tC_loss:0.0000\n",
      "Train Epoch: 51 [35840/60000 (60%)]\tLoss: 89.213776\tBCE:72.2127\tKLD:17.0011\tC_loss:0.0000\n",
      "Train Epoch: 51 [38400/60000 (64%)]\tLoss: 90.472321\tBCE:72.6100\tKLD:17.8623\tC_loss:0.0000\n",
      "Train Epoch: 51 [40960/60000 (68%)]\tLoss: 89.275597\tBCE:72.1714\tKLD:17.1042\tC_loss:0.0000\n",
      "Train Epoch: 51 [43520/60000 (73%)]\tLoss: 89.430275\tBCE:72.1454\tKLD:17.2849\tC_loss:0.0000\n",
      "Train Epoch: 51 [46080/60000 (77%)]\tLoss: 91.252167\tBCE:73.2423\tKLD:18.0099\tC_loss:0.0000\n",
      "Train Epoch: 51 [48640/60000 (81%)]\tLoss: 90.806778\tBCE:73.0033\tKLD:17.8035\tC_loss:0.0000\n",
      "Train Epoch: 51 [51200/60000 (85%)]\tLoss: 87.415398\tBCE:70.3348\tKLD:17.0806\tC_loss:0.0000\n",
      "Train Epoch: 51 [53760/60000 (90%)]\tLoss: 89.933678\tBCE:72.6774\tKLD:17.2563\tC_loss:0.0000\n",
      "Train Epoch: 51 [56320/60000 (94%)]\tLoss: 89.118927\tBCE:71.7156\tKLD:17.4033\tC_loss:0.0000\n",
      "Train Epoch: 51 [58880/60000 (98%)]\tLoss: 91.413651\tBCE:73.4174\tKLD:17.9963\tC_loss:0.0000\n",
      "====> Epoch: 51 Average loss: 90.1536\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.6484\n",
      "Train Epoch: 52 [0/60000 (0%)]\tLoss: 92.054634\tBCE:74.6122\tKLD:17.4425\tC_loss:0.0000\n",
      "Train Epoch: 52 [2560/60000 (4%)]\tLoss: 89.113403\tBCE:72.2267\tKLD:16.8867\tC_loss:0.0000\n",
      "Train Epoch: 52 [5120/60000 (9%)]\tLoss: 88.885849\tBCE:71.6679\tKLD:17.2180\tC_loss:0.0000\n",
      "Train Epoch: 52 [7680/60000 (13%)]\tLoss: 91.546120\tBCE:74.2559\tKLD:17.2902\tC_loss:0.0000\n",
      "Train Epoch: 52 [10240/60000 (17%)]\tLoss: 89.279289\tBCE:71.9506\tKLD:17.3287\tC_loss:0.0000\n",
      "Train Epoch: 52 [12800/60000 (21%)]\tLoss: 90.645447\tBCE:73.4147\tKLD:17.2308\tC_loss:0.0000\n",
      "Train Epoch: 52 [15360/60000 (26%)]\tLoss: 93.682831\tBCE:75.7576\tKLD:17.9253\tC_loss:0.0000\n",
      "Train Epoch: 52 [17920/60000 (30%)]\tLoss: 88.539993\tBCE:71.1300\tKLD:17.4100\tC_loss:0.0000\n",
      "Train Epoch: 52 [20480/60000 (34%)]\tLoss: 90.488449\tBCE:73.4111\tKLD:17.0773\tC_loss:0.0000\n",
      "Train Epoch: 52 [23040/60000 (38%)]\tLoss: 90.914696\tBCE:73.3091\tKLD:17.6056\tC_loss:0.0000\n",
      "Train Epoch: 52 [25600/60000 (43%)]\tLoss: 89.801781\tBCE:72.4247\tKLD:17.3771\tC_loss:0.0000\n",
      "Train Epoch: 52 [28160/60000 (47%)]\tLoss: 91.457397\tBCE:73.2616\tKLD:18.1958\tC_loss:0.0000\n",
      "Train Epoch: 52 [30720/60000 (51%)]\tLoss: 88.609444\tBCE:71.5706\tKLD:17.0389\tC_loss:0.0000\n",
      "Train Epoch: 52 [33280/60000 (56%)]\tLoss: 91.121819\tBCE:73.2958\tKLD:17.8261\tC_loss:0.0000\n",
      "Train Epoch: 52 [35840/60000 (60%)]\tLoss: 89.694984\tBCE:72.0863\tKLD:17.6087\tC_loss:0.0000\n",
      "Train Epoch: 52 [38400/60000 (64%)]\tLoss: 89.250084\tBCE:72.3153\tKLD:16.9347\tC_loss:0.0000\n",
      "Train Epoch: 52 [40960/60000 (68%)]\tLoss: 89.794296\tBCE:71.8979\tKLD:17.8964\tC_loss:0.0000\n",
      "Train Epoch: 52 [43520/60000 (73%)]\tLoss: 88.614471\tBCE:71.4882\tKLD:17.1263\tC_loss:0.0000\n",
      "Train Epoch: 52 [46080/60000 (77%)]\tLoss: 88.804474\tBCE:70.8879\tKLD:17.9166\tC_loss:0.0000\n",
      "Train Epoch: 52 [48640/60000 (81%)]\tLoss: 89.353470\tBCE:72.2029\tKLD:17.1505\tC_loss:0.0000\n",
      "Train Epoch: 52 [51200/60000 (85%)]\tLoss: 88.707542\tBCE:71.7180\tKLD:16.9895\tC_loss:0.0000\n",
      "Train Epoch: 52 [53760/60000 (90%)]\tLoss: 89.720108\tBCE:71.9095\tKLD:17.8106\tC_loss:0.0000\n",
      "Train Epoch: 52 [56320/60000 (94%)]\tLoss: 93.122826\tBCE:75.4654\tKLD:17.6574\tC_loss:0.0000\n",
      "Train Epoch: 52 [58880/60000 (98%)]\tLoss: 91.055191\tBCE:73.7391\tKLD:17.3161\tC_loss:0.0000\n",
      "====> Epoch: 52 Average loss: 90.0112\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.7528\n",
      "Train Epoch: 53 [0/60000 (0%)]\tLoss: 88.201729\tBCE:70.1381\tKLD:18.0636\tC_loss:0.0000\n",
      "Train Epoch: 53 [2560/60000 (4%)]\tLoss: 88.511230\tBCE:71.5994\tKLD:16.9118\tC_loss:0.0000\n",
      "Train Epoch: 53 [5120/60000 (9%)]\tLoss: 89.643837\tBCE:72.3037\tKLD:17.3402\tC_loss:0.0000\n",
      "Train Epoch: 53 [7680/60000 (13%)]\tLoss: 88.313393\tBCE:70.8544\tKLD:17.4590\tC_loss:0.0000\n",
      "Train Epoch: 53 [10240/60000 (17%)]\tLoss: 92.882385\tBCE:75.0696\tKLD:17.8128\tC_loss:0.0000\n",
      "Train Epoch: 53 [12800/60000 (21%)]\tLoss: 91.371460\tBCE:74.1517\tKLD:17.2197\tC_loss:0.0000\n",
      "Train Epoch: 53 [15360/60000 (26%)]\tLoss: 90.963715\tBCE:73.3111\tKLD:17.6526\tC_loss:0.0000\n",
      "Train Epoch: 53 [17920/60000 (30%)]\tLoss: 89.763008\tBCE:72.5368\tKLD:17.2262\tC_loss:0.0000\n",
      "Train Epoch: 53 [20480/60000 (34%)]\tLoss: 91.878372\tBCE:73.8881\tKLD:17.9903\tC_loss:0.0000\n",
      "Train Epoch: 53 [23040/60000 (38%)]\tLoss: 90.908028\tBCE:73.4858\tKLD:17.4222\tC_loss:0.0000\n",
      "Train Epoch: 53 [25600/60000 (43%)]\tLoss: 91.022255\tBCE:73.3293\tKLD:17.6929\tC_loss:0.0000\n",
      "Train Epoch: 53 [28160/60000 (47%)]\tLoss: 89.043564\tBCE:72.2413\tKLD:16.8022\tC_loss:0.0000\n",
      "Train Epoch: 53 [30720/60000 (51%)]\tLoss: 91.496078\tBCE:73.7808\tKLD:17.7153\tC_loss:0.0000\n",
      "Train Epoch: 53 [33280/60000 (56%)]\tLoss: 86.930092\tBCE:69.6734\tKLD:17.2567\tC_loss:0.0000\n",
      "Train Epoch: 53 [35840/60000 (60%)]\tLoss: 87.327614\tBCE:70.5868\tKLD:16.7408\tC_loss:0.0000\n",
      "Train Epoch: 53 [38400/60000 (64%)]\tLoss: 91.095871\tBCE:73.4500\tKLD:17.6459\tC_loss:0.0000\n",
      "Train Epoch: 53 [40960/60000 (68%)]\tLoss: 90.338501\tBCE:73.0773\tKLD:17.2612\tC_loss:0.0000\n",
      "Train Epoch: 53 [43520/60000 (73%)]\tLoss: 89.742035\tBCE:72.3393\tKLD:17.4027\tC_loss:0.0000\n",
      "Train Epoch: 53 [46080/60000 (77%)]\tLoss: 90.972862\tBCE:73.8429\tKLD:17.1300\tC_loss:0.0000\n",
      "Train Epoch: 53 [48640/60000 (81%)]\tLoss: 91.920731\tBCE:74.1803\tKLD:17.7405\tC_loss:0.0000\n",
      "Train Epoch: 53 [51200/60000 (85%)]\tLoss: 91.493340\tBCE:74.0999\tKLD:17.3934\tC_loss:0.0000\n",
      "Train Epoch: 53 [53760/60000 (90%)]\tLoss: 87.712578\tBCE:70.7761\tKLD:16.9365\tC_loss:0.0000\n",
      "Train Epoch: 53 [56320/60000 (94%)]\tLoss: 91.506622\tBCE:73.8683\tKLD:17.6383\tC_loss:0.0000\n",
      "Train Epoch: 53 [58880/60000 (98%)]\tLoss: 89.002373\tBCE:71.3108\tKLD:17.6915\tC_loss:0.0000\n",
      "====> Epoch: 53 Average loss: 90.1422\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.5441\n",
      "Train Epoch: 54 [0/60000 (0%)]\tLoss: 88.852280\tBCE:71.8836\tKLD:16.9687\tC_loss:0.0000\n",
      "Train Epoch: 54 [2560/60000 (4%)]\tLoss: 90.510910\tBCE:73.3013\tKLD:17.2096\tC_loss:0.0000\n",
      "Train Epoch: 54 [5120/60000 (9%)]\tLoss: 89.868263\tBCE:72.3420\tKLD:17.5263\tC_loss:0.0000\n",
      "Train Epoch: 54 [7680/60000 (13%)]\tLoss: 92.347824\tBCE:75.0034\tKLD:17.3444\tC_loss:0.0000\n",
      "Train Epoch: 54 [10240/60000 (17%)]\tLoss: 88.973625\tBCE:72.0800\tKLD:16.8937\tC_loss:0.0000\n",
      "Train Epoch: 54 [12800/60000 (21%)]\tLoss: 88.166740\tBCE:71.2465\tKLD:16.9202\tC_loss:0.0000\n",
      "Train Epoch: 54 [15360/60000 (26%)]\tLoss: 88.441833\tBCE:70.8784\tKLD:17.5634\tC_loss:0.0000\n",
      "Train Epoch: 54 [17920/60000 (30%)]\tLoss: 91.090027\tBCE:73.4490\tKLD:17.6411\tC_loss:0.0000\n",
      "Train Epoch: 54 [20480/60000 (34%)]\tLoss: 90.763992\tBCE:73.0203\tKLD:17.7437\tC_loss:0.0000\n",
      "Train Epoch: 54 [23040/60000 (38%)]\tLoss: 89.626038\tBCE:72.6946\tKLD:16.9314\tC_loss:0.0000\n",
      "Train Epoch: 54 [25600/60000 (43%)]\tLoss: 88.691856\tBCE:71.2646\tKLD:17.4272\tC_loss:0.0000\n",
      "Train Epoch: 54 [28160/60000 (47%)]\tLoss: 90.325607\tBCE:72.2981\tKLD:18.0275\tC_loss:0.0000\n",
      "Train Epoch: 54 [30720/60000 (51%)]\tLoss: 89.539490\tBCE:72.3227\tKLD:17.2167\tC_loss:0.0000\n",
      "Train Epoch: 54 [33280/60000 (56%)]\tLoss: 93.627312\tBCE:75.3304\tKLD:18.2969\tC_loss:0.0000\n",
      "Train Epoch: 54 [35840/60000 (60%)]\tLoss: 88.987381\tBCE:71.6132\tKLD:17.3741\tC_loss:0.0000\n",
      "Train Epoch: 54 [38400/60000 (64%)]\tLoss: 90.031494\tBCE:72.6426\tKLD:17.3889\tC_loss:0.0000\n",
      "Train Epoch: 54 [40960/60000 (68%)]\tLoss: 90.516441\tBCE:73.1286\tKLD:17.3878\tC_loss:0.0000\n",
      "Train Epoch: 54 [43520/60000 (73%)]\tLoss: 92.590668\tBCE:74.6787\tKLD:17.9119\tC_loss:0.0000\n",
      "Train Epoch: 54 [46080/60000 (77%)]\tLoss: 88.864532\tBCE:71.6275\tKLD:17.2370\tC_loss:0.0000\n",
      "Train Epoch: 54 [48640/60000 (81%)]\tLoss: 88.329201\tBCE:71.0554\tKLD:17.2738\tC_loss:0.0000\n",
      "Train Epoch: 54 [51200/60000 (85%)]\tLoss: 90.115189\tBCE:72.6026\tKLD:17.5126\tC_loss:0.0000\n",
      "Train Epoch: 54 [53760/60000 (90%)]\tLoss: 90.448288\tBCE:72.8539\tKLD:17.5944\tC_loss:0.0000\n",
      "Train Epoch: 54 [56320/60000 (94%)]\tLoss: 90.151787\tBCE:72.6109\tKLD:17.5409\tC_loss:0.0000\n",
      "Train Epoch: 54 [58880/60000 (98%)]\tLoss: 88.183640\tBCE:71.1556\tKLD:17.0280\tC_loss:0.0000\n",
      "====> Epoch: 54 Average loss: 89.9864\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.7499\n",
      "Train Epoch: 55 [0/60000 (0%)]\tLoss: 90.345459\tBCE:73.0408\tKLD:17.3047\tC_loss:0.0000\n",
      "Train Epoch: 55 [2560/60000 (4%)]\tLoss: 88.864540\tBCE:71.5007\tKLD:17.3639\tC_loss:0.0000\n",
      "Train Epoch: 55 [5120/60000 (9%)]\tLoss: 91.764038\tBCE:73.7086\tKLD:18.0554\tC_loss:0.0000\n",
      "Train Epoch: 55 [7680/60000 (13%)]\tLoss: 91.345009\tBCE:73.7014\tKLD:17.6436\tC_loss:0.0000\n",
      "Train Epoch: 55 [10240/60000 (17%)]\tLoss: 89.126678\tBCE:71.8125\tKLD:17.3142\tC_loss:0.0000\n",
      "Train Epoch: 55 [12800/60000 (21%)]\tLoss: 91.091141\tBCE:73.5350\tKLD:17.5562\tC_loss:0.0000\n",
      "Train Epoch: 55 [15360/60000 (26%)]\tLoss: 88.897514\tBCE:71.3210\tKLD:17.5765\tC_loss:0.0000\n",
      "Train Epoch: 55 [17920/60000 (30%)]\tLoss: 90.135048\tBCE:72.2853\tKLD:17.8498\tC_loss:0.0000\n",
      "Train Epoch: 55 [20480/60000 (34%)]\tLoss: 88.749733\tBCE:71.4603\tKLD:17.2895\tC_loss:0.0000\n",
      "Train Epoch: 55 [23040/60000 (38%)]\tLoss: 91.044022\tBCE:73.2000\tKLD:17.8441\tC_loss:0.0000\n",
      "Train Epoch: 55 [25600/60000 (43%)]\tLoss: 89.625168\tBCE:71.9358\tKLD:17.6893\tC_loss:0.0000\n",
      "Train Epoch: 55 [28160/60000 (47%)]\tLoss: 92.018402\tBCE:74.1560\tKLD:17.8624\tC_loss:0.0000\n",
      "Train Epoch: 55 [30720/60000 (51%)]\tLoss: 89.368942\tBCE:71.9878\tKLD:17.3811\tC_loss:0.0000\n",
      "Train Epoch: 55 [33280/60000 (56%)]\tLoss: 87.943161\tBCE:70.4032\tKLD:17.5400\tC_loss:0.0000\n",
      "Train Epoch: 55 [35840/60000 (60%)]\tLoss: 89.458664\tBCE:71.5618\tKLD:17.8969\tC_loss:0.0000\n",
      "Train Epoch: 55 [38400/60000 (64%)]\tLoss: 87.127647\tBCE:70.4471\tKLD:16.6805\tC_loss:0.0000\n",
      "Train Epoch: 55 [40960/60000 (68%)]\tLoss: 92.629387\tBCE:74.7532\tKLD:17.8762\tC_loss:0.0000\n",
      "Train Epoch: 55 [43520/60000 (73%)]\tLoss: 90.430801\tBCE:72.8806\tKLD:17.5502\tC_loss:0.0000\n",
      "Train Epoch: 55 [46080/60000 (77%)]\tLoss: 90.922508\tBCE:73.5347\tKLD:17.3878\tC_loss:0.0000\n",
      "Train Epoch: 55 [48640/60000 (81%)]\tLoss: 88.490486\tBCE:71.1951\tKLD:17.2954\tC_loss:0.0000\n",
      "Train Epoch: 55 [51200/60000 (85%)]\tLoss: 88.091171\tBCE:70.7460\tKLD:17.3452\tC_loss:0.0000\n",
      "Train Epoch: 55 [53760/60000 (90%)]\tLoss: 92.965935\tBCE:74.9363\tKLD:18.0296\tC_loss:0.0000\n",
      "Train Epoch: 55 [56320/60000 (94%)]\tLoss: 90.501381\tBCE:73.1379\tKLD:17.3634\tC_loss:0.0000\n",
      "Train Epoch: 55 [58880/60000 (98%)]\tLoss: 91.775238\tBCE:74.2211\tKLD:17.5541\tC_loss:0.0000\n",
      "====> Epoch: 55 Average loss: 90.0188\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 92.0322\n",
      "Random number: 3\n",
      "Train Epoch: 56 [0/60000 (0%)]\tLoss: 91.142761\tBCE:73.1424\tKLD:18.0004\tC_loss:0.0000\n",
      "Train Epoch: 56 [2560/60000 (4%)]\tLoss: 89.253677\tBCE:72.0515\tKLD:17.2022\tC_loss:0.0000\n",
      "Train Epoch: 56 [5120/60000 (9%)]\tLoss: 89.926476\tBCE:72.5502\tKLD:17.3763\tC_loss:0.0000\n",
      "Train Epoch: 56 [7680/60000 (13%)]\tLoss: 89.424294\tBCE:71.7646\tKLD:17.6597\tC_loss:0.0000\n",
      "Train Epoch: 56 [10240/60000 (17%)]\tLoss: 92.621925\tBCE:74.9244\tKLD:17.6976\tC_loss:0.0000\n",
      "Train Epoch: 56 [12800/60000 (21%)]\tLoss: 92.146698\tBCE:74.7311\tKLD:17.4156\tC_loss:0.0000\n",
      "Train Epoch: 56 [15360/60000 (26%)]\tLoss: 89.214951\tBCE:71.7642\tKLD:17.4507\tC_loss:0.0000\n",
      "Train Epoch: 56 [17920/60000 (30%)]\tLoss: 92.187393\tBCE:74.6298\tKLD:17.5576\tC_loss:0.0000\n",
      "Train Epoch: 56 [20480/60000 (34%)]\tLoss: 91.177086\tBCE:73.4116\tKLD:17.7655\tC_loss:0.0000\n",
      "Train Epoch: 56 [23040/60000 (38%)]\tLoss: 91.277206\tBCE:73.1396\tKLD:18.1376\tC_loss:0.0000\n",
      "Train Epoch: 56 [25600/60000 (43%)]\tLoss: 90.586578\tBCE:73.0453\tKLD:17.5412\tC_loss:0.0000\n",
      "Train Epoch: 56 [28160/60000 (47%)]\tLoss: 89.521622\tBCE:71.7190\tKLD:17.8026\tC_loss:0.0000\n",
      "Train Epoch: 56 [30720/60000 (51%)]\tLoss: 90.727837\tBCE:73.0210\tKLD:17.7068\tC_loss:0.0000\n",
      "Train Epoch: 56 [33280/60000 (56%)]\tLoss: 89.422546\tBCE:71.9763\tKLD:17.4463\tC_loss:0.0000\n",
      "Train Epoch: 56 [35840/60000 (60%)]\tLoss: 87.899368\tBCE:71.1804\tKLD:16.7190\tC_loss:0.0000\n",
      "Train Epoch: 56 [38400/60000 (64%)]\tLoss: 89.554062\tBCE:72.3255\tKLD:17.2286\tC_loss:0.0000\n",
      "Train Epoch: 56 [40960/60000 (68%)]\tLoss: 91.410446\tBCE:73.8073\tKLD:17.6031\tC_loss:0.0000\n",
      "Train Epoch: 56 [43520/60000 (73%)]\tLoss: 92.750572\tBCE:74.6256\tKLD:18.1250\tC_loss:0.0000\n",
      "Train Epoch: 56 [46080/60000 (77%)]\tLoss: 91.583008\tBCE:74.4048\tKLD:17.1782\tC_loss:0.0000\n",
      "Train Epoch: 56 [48640/60000 (81%)]\tLoss: 90.950089\tBCE:72.9722\tKLD:17.9779\tC_loss:0.0000\n",
      "Train Epoch: 56 [51200/60000 (85%)]\tLoss: 88.867165\tBCE:71.8733\tKLD:16.9939\tC_loss:0.0000\n",
      "Train Epoch: 56 [53760/60000 (90%)]\tLoss: 87.861900\tBCE:70.8034\tKLD:17.0585\tC_loss:0.0000\n",
      "Train Epoch: 56 [56320/60000 (94%)]\tLoss: 90.229546\tBCE:73.0338\tKLD:17.1958\tC_loss:0.0000\n",
      "Train Epoch: 56 [58880/60000 (98%)]\tLoss: 88.453720\tBCE:70.6435\tKLD:17.8102\tC_loss:0.0000\n",
      "====> Epoch: 56 Average loss: 89.8537\tClassifier Accuracy: 99.8631\n",
      "====> Test set loss: 91.5840\n",
      "Train Epoch: 57 [0/60000 (0%)]\tLoss: 90.451828\tBCE:73.1627\tKLD:17.2891\tC_loss:0.0000\n",
      "Train Epoch: 57 [2560/60000 (4%)]\tLoss: 86.849640\tBCE:69.4283\tKLD:17.4214\tC_loss:0.0000\n",
      "Train Epoch: 57 [5120/60000 (9%)]\tLoss: 88.495865\tBCE:71.1957\tKLD:17.3001\tC_loss:0.0000\n",
      "Train Epoch: 57 [7680/60000 (13%)]\tLoss: 89.307388\tBCE:71.5709\tKLD:17.7365\tC_loss:0.0000\n",
      "Train Epoch: 57 [10240/60000 (17%)]\tLoss: 89.254028\tBCE:71.3860\tKLD:17.8680\tC_loss:0.0000\n",
      "Train Epoch: 57 [12800/60000 (21%)]\tLoss: 86.731865\tBCE:69.6275\tKLD:17.1044\tC_loss:0.0000\n",
      "Train Epoch: 57 [15360/60000 (26%)]\tLoss: 89.058678\tBCE:71.7681\tKLD:17.2906\tC_loss:0.0000\n",
      "Train Epoch: 57 [17920/60000 (30%)]\tLoss: 91.021088\tBCE:74.0343\tKLD:16.9868\tC_loss:0.0000\n",
      "Train Epoch: 57 [20480/60000 (34%)]\tLoss: 88.049545\tBCE:70.3800\tKLD:17.6695\tC_loss:0.0000\n",
      "Train Epoch: 57 [23040/60000 (38%)]\tLoss: 91.860413\tBCE:74.2747\tKLD:17.5857\tC_loss:0.0000\n",
      "Train Epoch: 57 [25600/60000 (43%)]\tLoss: 91.798775\tBCE:74.1017\tKLD:17.6970\tC_loss:0.0000\n",
      "Train Epoch: 57 [28160/60000 (47%)]\tLoss: 92.585846\tBCE:74.8723\tKLD:17.7135\tC_loss:0.0000\n",
      "Train Epoch: 57 [30720/60000 (51%)]\tLoss: 91.997543\tBCE:74.3432\tKLD:17.6543\tC_loss:0.0000\n",
      "Train Epoch: 57 [33280/60000 (56%)]\tLoss: 90.358086\tBCE:72.7062\tKLD:17.6519\tC_loss:0.0000\n",
      "Train Epoch: 57 [35840/60000 (60%)]\tLoss: 88.473083\tBCE:70.8555\tKLD:17.6176\tC_loss:0.0000\n",
      "Train Epoch: 57 [38400/60000 (64%)]\tLoss: 89.447144\tBCE:71.9476\tKLD:17.4995\tC_loss:0.0000\n",
      "Train Epoch: 57 [40960/60000 (68%)]\tLoss: 88.177666\tBCE:71.3206\tKLD:16.8571\tC_loss:0.0000\n",
      "Train Epoch: 57 [43520/60000 (73%)]\tLoss: 90.823433\tBCE:73.2820\tKLD:17.5414\tC_loss:0.0000\n",
      "Train Epoch: 57 [46080/60000 (77%)]\tLoss: 89.245522\tBCE:72.2624\tKLD:16.9831\tC_loss:0.0000\n",
      "Train Epoch: 57 [48640/60000 (81%)]\tLoss: 87.691917\tBCE:70.7001\tKLD:16.9918\tC_loss:0.0000\n",
      "Train Epoch: 57 [51200/60000 (85%)]\tLoss: 91.667137\tBCE:73.7268\tKLD:17.9403\tC_loss:0.0000\n",
      "Train Epoch: 57 [53760/60000 (90%)]\tLoss: 90.809280\tBCE:73.1211\tKLD:17.6882\tC_loss:0.0000\n",
      "Train Epoch: 57 [56320/60000 (94%)]\tLoss: 89.405869\tBCE:72.2649\tKLD:17.1410\tC_loss:0.0000\n",
      "Train Epoch: 57 [58880/60000 (98%)]\tLoss: 92.576294\tBCE:74.8171\tKLD:17.7592\tC_loss:0.0000\n",
      "====> Epoch: 57 Average loss: 89.8847\tClassifier Accuracy: 99.5509\n",
      "====> Test set loss: 91.3096\n",
      "Train Epoch: 58 [0/60000 (0%)]\tLoss: 87.261581\tBCE:70.2247\tKLD:17.0369\tC_loss:0.0000\n",
      "Train Epoch: 58 [2560/60000 (4%)]\tLoss: 92.067368\tBCE:74.0798\tKLD:17.9875\tC_loss:0.0000\n",
      "Train Epoch: 58 [5120/60000 (9%)]\tLoss: 90.709396\tBCE:73.1440\tKLD:17.5654\tC_loss:0.0000\n",
      "Train Epoch: 58 [7680/60000 (13%)]\tLoss: 87.262451\tBCE:69.9818\tKLD:17.2807\tC_loss:0.0000\n",
      "Train Epoch: 58 [10240/60000 (17%)]\tLoss: 89.501884\tBCE:72.4720\tKLD:17.0298\tC_loss:0.0000\n",
      "Train Epoch: 58 [12800/60000 (21%)]\tLoss: 90.244766\tBCE:72.5683\tKLD:17.6765\tC_loss:0.0000\n",
      "Train Epoch: 58 [15360/60000 (26%)]\tLoss: 90.445847\tBCE:73.1745\tKLD:17.2714\tC_loss:0.0000\n",
      "Train Epoch: 58 [17920/60000 (30%)]\tLoss: 90.543663\tBCE:72.9109\tKLD:17.6327\tC_loss:0.0000\n",
      "Train Epoch: 58 [20480/60000 (34%)]\tLoss: 89.341003\tBCE:71.7974\tKLD:17.5436\tC_loss:0.0000\n",
      "Train Epoch: 58 [23040/60000 (38%)]\tLoss: 89.460838\tBCE:72.4438\tKLD:17.0171\tC_loss:0.0000\n",
      "Train Epoch: 58 [25600/60000 (43%)]\tLoss: 86.248581\tBCE:69.5091\tKLD:16.7394\tC_loss:0.0000\n",
      "Train Epoch: 58 [28160/60000 (47%)]\tLoss: 87.743317\tBCE:70.6201\tKLD:17.1232\tC_loss:0.0000\n",
      "Train Epoch: 58 [30720/60000 (51%)]\tLoss: 90.159119\tBCE:72.3767\tKLD:17.7824\tC_loss:0.0000\n",
      "Train Epoch: 58 [33280/60000 (56%)]\tLoss: 92.117233\tBCE:73.9103\tKLD:18.2070\tC_loss:0.0000\n",
      "Train Epoch: 58 [35840/60000 (60%)]\tLoss: 87.887291\tBCE:70.7292\tKLD:17.1581\tC_loss:0.0000\n",
      "Train Epoch: 58 [38400/60000 (64%)]\tLoss: 89.540237\tBCE:72.0896\tKLD:17.4507\tC_loss:0.0000\n",
      "Train Epoch: 58 [40960/60000 (68%)]\tLoss: 87.609955\tBCE:70.3213\tKLD:17.2887\tC_loss:0.0000\n",
      "Train Epoch: 58 [43520/60000 (73%)]\tLoss: 90.485535\tBCE:73.1016\tKLD:17.3840\tC_loss:0.0000\n",
      "Train Epoch: 58 [46080/60000 (77%)]\tLoss: 89.506432\tBCE:71.9786\tKLD:17.5279\tC_loss:0.0000\n",
      "Train Epoch: 58 [48640/60000 (81%)]\tLoss: 91.615997\tBCE:73.8711\tKLD:17.7449\tC_loss:0.0000\n",
      "Train Epoch: 58 [51200/60000 (85%)]\tLoss: 90.569382\tBCE:73.0091\tKLD:17.5602\tC_loss:0.0000\n",
      "Train Epoch: 58 [53760/60000 (90%)]\tLoss: 87.915237\tBCE:71.0332\tKLD:16.8820\tC_loss:0.0000\n",
      "Train Epoch: 58 [56320/60000 (94%)]\tLoss: 89.971962\tBCE:71.6852\tKLD:18.2868\tC_loss:0.0000\n",
      "Train Epoch: 58 [58880/60000 (98%)]\tLoss: 86.910240\tBCE:70.2105\tKLD:16.6998\tC_loss:0.0000\n",
      "====> Epoch: 58 Average loss: 89.6626\tClassifier Accuracy: 99.9733\n",
      "====> Test set loss: 91.3441\n",
      "Train Epoch: 59 [0/60000 (0%)]\tLoss: 88.651886\tBCE:71.2795\tKLD:17.3724\tC_loss:0.0000\n",
      "Train Epoch: 59 [2560/60000 (4%)]\tLoss: 91.093071\tBCE:73.3412\tKLD:17.7519\tC_loss:0.0000\n",
      "Train Epoch: 59 [5120/60000 (9%)]\tLoss: 90.919327\tBCE:73.4540\tKLD:17.4653\tC_loss:0.0000\n",
      "Train Epoch: 59 [7680/60000 (13%)]\tLoss: 86.702347\tBCE:69.5684\tKLD:17.1339\tC_loss:0.0000\n",
      "Train Epoch: 59 [10240/60000 (17%)]\tLoss: 88.438850\tBCE:71.0130\tKLD:17.4259\tC_loss:0.0000\n",
      "Train Epoch: 59 [12800/60000 (21%)]\tLoss: 88.040558\tBCE:70.4769\tKLD:17.5636\tC_loss:0.0000\n",
      "Train Epoch: 59 [15360/60000 (26%)]\tLoss: 87.783493\tBCE:70.4904\tKLD:17.2931\tC_loss:0.0000\n",
      "Train Epoch: 59 [17920/60000 (30%)]\tLoss: 90.754318\tBCE:72.7866\tKLD:17.9677\tC_loss:0.0000\n",
      "Train Epoch: 59 [20480/60000 (34%)]\tLoss: 90.019547\tBCE:72.5141\tKLD:17.5055\tC_loss:0.0000\n",
      "Train Epoch: 59 [23040/60000 (38%)]\tLoss: 89.982628\tBCE:72.2363\tKLD:17.7463\tC_loss:0.0000\n",
      "Train Epoch: 59 [25600/60000 (43%)]\tLoss: 89.635658\tBCE:72.6007\tKLD:17.0349\tC_loss:0.0000\n",
      "Train Epoch: 59 [28160/60000 (47%)]\tLoss: 91.899033\tBCE:74.0907\tKLD:17.8084\tC_loss:0.0000\n",
      "Train Epoch: 59 [30720/60000 (51%)]\tLoss: 87.640030\tBCE:70.4054\tKLD:17.2346\tC_loss:0.0000\n",
      "Train Epoch: 59 [33280/60000 (56%)]\tLoss: 87.919167\tBCE:70.4085\tKLD:17.5107\tC_loss:0.0000\n",
      "Train Epoch: 59 [35840/60000 (60%)]\tLoss: 89.779564\tBCE:72.4741\tKLD:17.3055\tC_loss:0.0000\n",
      "Train Epoch: 59 [38400/60000 (64%)]\tLoss: 90.679871\tBCE:73.0810\tKLD:17.5989\tC_loss:0.0000\n",
      "Train Epoch: 59 [40960/60000 (68%)]\tLoss: 87.752670\tBCE:70.2216\tKLD:17.5311\tC_loss:0.0000\n",
      "Train Epoch: 59 [43520/60000 (73%)]\tLoss: 88.936615\tBCE:71.6827\tKLD:17.2540\tC_loss:0.0000\n",
      "Train Epoch: 59 [46080/60000 (77%)]\tLoss: 88.159149\tBCE:70.8440\tKLD:17.3151\tC_loss:0.0000\n",
      "Train Epoch: 59 [48640/60000 (81%)]\tLoss: 88.415833\tBCE:71.3764\tKLD:17.0394\tC_loss:0.0000\n",
      "Train Epoch: 59 [51200/60000 (85%)]\tLoss: 91.213524\tBCE:73.5291\tKLD:17.6845\tC_loss:0.0000\n",
      "Train Epoch: 59 [53760/60000 (90%)]\tLoss: 87.879608\tBCE:70.3868\tKLD:17.4928\tC_loss:0.0000\n",
      "Train Epoch: 59 [56320/60000 (94%)]\tLoss: 91.211617\tBCE:74.0259\tKLD:17.1857\tC_loss:0.0000\n",
      "Train Epoch: 59 [58880/60000 (98%)]\tLoss: 89.485962\tBCE:72.6900\tKLD:16.7959\tC_loss:0.0000\n",
      "====> Epoch: 59 Average loss: 89.6375\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.5024\n",
      "Train Epoch: 60 [0/60000 (0%)]\tLoss: 89.594635\tBCE:72.4789\tKLD:17.1157\tC_loss:0.0000\n",
      "Train Epoch: 60 [2560/60000 (4%)]\tLoss: 87.854027\tBCE:70.3673\tKLD:17.4867\tC_loss:0.0000\n",
      "Train Epoch: 60 [5120/60000 (9%)]\tLoss: 88.755516\tBCE:71.4203\tKLD:17.3352\tC_loss:0.0000\n",
      "Train Epoch: 60 [7680/60000 (13%)]\tLoss: 90.022232\tBCE:72.5844\tKLD:17.4378\tC_loss:0.0000\n",
      "Train Epoch: 60 [10240/60000 (17%)]\tLoss: 91.089516\tBCE:73.0536\tKLD:18.0359\tC_loss:0.0000\n",
      "Train Epoch: 60 [12800/60000 (21%)]\tLoss: 89.777756\tBCE:72.1622\tKLD:17.6156\tC_loss:0.0000\n",
      "Train Epoch: 60 [15360/60000 (26%)]\tLoss: 88.621620\tBCE:71.5921\tKLD:17.0295\tC_loss:0.0000\n",
      "Train Epoch: 60 [17920/60000 (30%)]\tLoss: 90.068619\tBCE:72.8345\tKLD:17.2341\tC_loss:0.0000\n",
      "Train Epoch: 60 [20480/60000 (34%)]\tLoss: 94.354019\tBCE:76.5596\tKLD:17.7944\tC_loss:0.0000\n",
      "Train Epoch: 60 [23040/60000 (38%)]\tLoss: 91.122993\tBCE:73.7624\tKLD:17.3606\tC_loss:0.0000\n",
      "Train Epoch: 60 [25600/60000 (43%)]\tLoss: 87.772194\tBCE:70.1208\tKLD:17.6514\tC_loss:0.0000\n",
      "Train Epoch: 60 [28160/60000 (47%)]\tLoss: 90.237091\tBCE:72.8175\tKLD:17.4196\tC_loss:0.0000\n",
      "Train Epoch: 60 [30720/60000 (51%)]\tLoss: 89.279892\tBCE:71.7140\tKLD:17.5659\tC_loss:0.0000\n",
      "Train Epoch: 60 [33280/60000 (56%)]\tLoss: 89.879517\tBCE:72.3535\tKLD:17.5260\tC_loss:0.0000\n",
      "Train Epoch: 60 [35840/60000 (60%)]\tLoss: 92.194061\tBCE:74.5428\tKLD:17.6513\tC_loss:0.0000\n",
      "Train Epoch: 60 [38400/60000 (64%)]\tLoss: 91.387810\tBCE:73.7912\tKLD:17.5966\tC_loss:0.0000\n",
      "Train Epoch: 60 [40960/60000 (68%)]\tLoss: 91.612793\tBCE:73.7400\tKLD:17.8728\tC_loss:0.0000\n",
      "Train Epoch: 60 [43520/60000 (73%)]\tLoss: 88.529633\tBCE:71.7222\tKLD:16.8074\tC_loss:0.0000\n",
      "Train Epoch: 60 [46080/60000 (77%)]\tLoss: 90.639114\tBCE:73.0265\tKLD:17.6127\tC_loss:0.0000\n",
      "Train Epoch: 60 [48640/60000 (81%)]\tLoss: 87.927048\tBCE:70.7928\tKLD:17.1342\tC_loss:0.0000\n",
      "Train Epoch: 60 [51200/60000 (85%)]\tLoss: 90.394745\tBCE:73.1074\tKLD:17.2874\tC_loss:0.0000\n",
      "Train Epoch: 60 [53760/60000 (90%)]\tLoss: 88.686981\tBCE:71.3255\tKLD:17.3615\tC_loss:0.0000\n",
      "Train Epoch: 60 [56320/60000 (94%)]\tLoss: 91.582214\tBCE:73.9233\tKLD:17.6589\tC_loss:0.0000\n",
      "Train Epoch: 60 [58880/60000 (98%)]\tLoss: 90.662056\tBCE:72.7955\tKLD:17.8666\tC_loss:0.0000\n",
      "====> Epoch: 60 Average loss: 89.6680\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.0500\n",
      "Random number: 4\n",
      "Train Epoch: 61 [0/60000 (0%)]\tLoss: 88.822617\tBCE:71.7661\tKLD:17.0565\tC_loss:0.0000\n",
      "Train Epoch: 61 [2560/60000 (4%)]\tLoss: 91.792084\tBCE:73.9074\tKLD:17.8847\tC_loss:0.0000\n",
      "Train Epoch: 61 [5120/60000 (9%)]\tLoss: 90.419525\tBCE:72.9758\tKLD:17.4437\tC_loss:0.0000\n",
      "Train Epoch: 61 [7680/60000 (13%)]\tLoss: 87.656479\tBCE:69.8766\tKLD:17.7799\tC_loss:0.0000\n",
      "Train Epoch: 61 [10240/60000 (17%)]\tLoss: 90.677368\tBCE:72.9008\tKLD:17.7766\tC_loss:0.0000\n",
      "Train Epoch: 61 [12800/60000 (21%)]\tLoss: 89.732246\tBCE:72.3282\tKLD:17.4040\tC_loss:0.0000\n",
      "Train Epoch: 61 [15360/60000 (26%)]\tLoss: 86.659958\tBCE:69.4450\tKLD:17.2149\tC_loss:0.0000\n",
      "Train Epoch: 61 [17920/60000 (30%)]\tLoss: 91.927605\tBCE:74.5893\tKLD:17.3383\tC_loss:0.0000\n",
      "Train Epoch: 61 [20480/60000 (34%)]\tLoss: 89.865761\tBCE:72.3123\tKLD:17.5534\tC_loss:0.0000\n",
      "Train Epoch: 61 [23040/60000 (38%)]\tLoss: 90.496552\tBCE:72.7234\tKLD:17.7731\tC_loss:0.0000\n",
      "Train Epoch: 61 [25600/60000 (43%)]\tLoss: 90.459091\tBCE:72.9940\tKLD:17.4651\tC_loss:0.0000\n",
      "Train Epoch: 61 [28160/60000 (47%)]\tLoss: 90.708862\tBCE:73.4132\tKLD:17.2956\tC_loss:0.0000\n",
      "Train Epoch: 61 [30720/60000 (51%)]\tLoss: 89.568161\tBCE:72.0558\tKLD:17.5124\tC_loss:0.0000\n",
      "Train Epoch: 61 [33280/60000 (56%)]\tLoss: 91.079628\tBCE:73.5632\tKLD:17.5164\tC_loss:0.0000\n",
      "Train Epoch: 61 [35840/60000 (60%)]\tLoss: 86.871384\tBCE:69.6169\tKLD:17.2545\tC_loss:0.0000\n",
      "Train Epoch: 61 [38400/60000 (64%)]\tLoss: 89.636864\tBCE:71.9429\tKLD:17.6940\tC_loss:0.0000\n",
      "Train Epoch: 61 [40960/60000 (68%)]\tLoss: 89.327477\tBCE:71.9911\tKLD:17.3363\tC_loss:0.0000\n",
      "Train Epoch: 61 [43520/60000 (73%)]\tLoss: 90.712532\tBCE:73.2106\tKLD:17.5019\tC_loss:0.0000\n",
      "Train Epoch: 61 [46080/60000 (77%)]\tLoss: 86.930283\tBCE:69.5817\tKLD:17.3485\tC_loss:0.0000\n",
      "Train Epoch: 61 [48640/60000 (81%)]\tLoss: 90.799210\tBCE:73.4870\tKLD:17.3122\tC_loss:0.0000\n",
      "Train Epoch: 61 [51200/60000 (85%)]\tLoss: 91.312378\tBCE:73.7447\tKLD:17.5677\tC_loss:0.0000\n",
      "Train Epoch: 61 [53760/60000 (90%)]\tLoss: 90.469040\tBCE:72.8321\tKLD:17.6369\tC_loss:0.0000\n",
      "Train Epoch: 61 [56320/60000 (94%)]\tLoss: 89.117676\tBCE:71.8241\tKLD:17.2936\tC_loss:0.0000\n",
      "Train Epoch: 61 [58880/60000 (98%)]\tLoss: 91.395454\tBCE:73.2604\tKLD:18.1351\tC_loss:0.0000\n",
      "====> Epoch: 61 Average loss: 89.5943\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.6507\n",
      "Train Epoch: 62 [0/60000 (0%)]\tLoss: 88.540642\tBCE:71.0882\tKLD:17.4525\tC_loss:0.0000\n",
      "Train Epoch: 62 [2560/60000 (4%)]\tLoss: 89.646255\tBCE:71.9993\tKLD:17.6469\tC_loss:0.0000\n",
      "Train Epoch: 62 [5120/60000 (9%)]\tLoss: 89.149162\tBCE:71.7217\tKLD:17.4275\tC_loss:0.0000\n",
      "Train Epoch: 62 [7680/60000 (13%)]\tLoss: 90.100296\tBCE:72.4903\tKLD:17.6100\tC_loss:0.0000\n",
      "Train Epoch: 62 [10240/60000 (17%)]\tLoss: 87.984512\tBCE:70.7001\tKLD:17.2844\tC_loss:0.0000\n",
      "Train Epoch: 62 [12800/60000 (21%)]\tLoss: 89.242096\tBCE:71.6258\tKLD:17.6163\tC_loss:0.0000\n",
      "Train Epoch: 62 [15360/60000 (26%)]\tLoss: 88.882149\tBCE:71.5329\tKLD:17.3493\tC_loss:0.0000\n",
      "Train Epoch: 62 [17920/60000 (30%)]\tLoss: 90.319229\tBCE:72.7276\tKLD:17.5916\tC_loss:0.0000\n",
      "Train Epoch: 62 [20480/60000 (34%)]\tLoss: 89.822067\tBCE:72.3557\tKLD:17.4664\tC_loss:0.0000\n",
      "Train Epoch: 62 [23040/60000 (38%)]\tLoss: 91.025101\tBCE:73.1169\tKLD:17.9082\tC_loss:0.0000\n",
      "Train Epoch: 62 [25600/60000 (43%)]\tLoss: 88.147102\tBCE:70.8233\tKLD:17.3238\tC_loss:0.0000\n",
      "Train Epoch: 62 [28160/60000 (47%)]\tLoss: 89.174309\tBCE:71.7026\tKLD:17.4717\tC_loss:0.0000\n",
      "Train Epoch: 62 [30720/60000 (51%)]\tLoss: 89.589355\tBCE:72.3292\tKLD:17.2602\tC_loss:0.0000\n",
      "Train Epoch: 62 [33280/60000 (56%)]\tLoss: 86.295975\tBCE:69.2734\tKLD:17.0226\tC_loss:0.0000\n",
      "Train Epoch: 62 [35840/60000 (60%)]\tLoss: 89.654617\tBCE:72.6597\tKLD:16.9950\tC_loss:0.0000\n",
      "Train Epoch: 62 [38400/60000 (64%)]\tLoss: 87.602760\tBCE:70.4598\tKLD:17.1430\tC_loss:0.0000\n",
      "Train Epoch: 62 [40960/60000 (68%)]\tLoss: 88.190521\tBCE:70.8688\tKLD:17.3217\tC_loss:0.0000\n",
      "Train Epoch: 62 [43520/60000 (73%)]\tLoss: 90.542923\tBCE:72.9286\tKLD:17.6143\tC_loss:0.0000\n",
      "Train Epoch: 62 [46080/60000 (77%)]\tLoss: 89.841988\tBCE:72.8974\tKLD:16.9446\tC_loss:0.0000\n",
      "Train Epoch: 62 [48640/60000 (81%)]\tLoss: 90.690094\tBCE:72.7669\tKLD:17.9232\tC_loss:0.0000\n",
      "Train Epoch: 62 [51200/60000 (85%)]\tLoss: 84.803009\tBCE:67.7974\tKLD:17.0056\tC_loss:0.0000\n",
      "Train Epoch: 62 [53760/60000 (90%)]\tLoss: 88.812485\tBCE:71.4457\tKLD:17.3668\tC_loss:0.0000\n",
      "Train Epoch: 62 [56320/60000 (94%)]\tLoss: 90.534325\tBCE:72.8921\tKLD:17.6422\tC_loss:0.0000\n",
      "Train Epoch: 62 [58880/60000 (98%)]\tLoss: 88.723907\tBCE:71.5947\tKLD:17.1292\tC_loss:0.0000\n",
      "====> Epoch: 62 Average loss: 89.5462\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0394\n",
      "Train Epoch: 63 [0/60000 (0%)]\tLoss: 89.406235\tBCE:72.3237\tKLD:17.0826\tC_loss:0.0000\n",
      "Train Epoch: 63 [2560/60000 (4%)]\tLoss: 91.042053\tBCE:73.0175\tKLD:18.0245\tC_loss:0.0000\n",
      "Train Epoch: 63 [5120/60000 (9%)]\tLoss: 89.277611\tBCE:71.4671\tKLD:17.8105\tC_loss:0.0000\n",
      "Train Epoch: 63 [7680/60000 (13%)]\tLoss: 88.365036\tBCE:71.2459\tKLD:17.1192\tC_loss:0.0000\n",
      "Train Epoch: 63 [10240/60000 (17%)]\tLoss: 88.938568\tBCE:71.4837\tKLD:17.4549\tC_loss:0.0000\n",
      "Train Epoch: 63 [12800/60000 (21%)]\tLoss: 89.288788\tBCE:71.6624\tKLD:17.6264\tC_loss:0.0000\n",
      "Train Epoch: 63 [15360/60000 (26%)]\tLoss: 90.182938\tBCE:72.9371\tKLD:17.2458\tC_loss:0.0000\n",
      "Train Epoch: 63 [17920/60000 (30%)]\tLoss: 88.137283\tBCE:71.3471\tKLD:16.7902\tC_loss:0.0000\n",
      "Train Epoch: 63 [20480/60000 (34%)]\tLoss: 88.897079\tBCE:71.8705\tKLD:17.0266\tC_loss:0.0000\n",
      "Train Epoch: 63 [23040/60000 (38%)]\tLoss: 90.766602\tBCE:73.0053\tKLD:17.7613\tC_loss:0.0000\n",
      "Train Epoch: 63 [25600/60000 (43%)]\tLoss: 87.758331\tBCE:70.2464\tKLD:17.5119\tC_loss:0.0000\n",
      "Train Epoch: 63 [28160/60000 (47%)]\tLoss: 89.829926\tBCE:72.4278\tKLD:17.4021\tC_loss:0.0000\n",
      "Train Epoch: 63 [30720/60000 (51%)]\tLoss: 89.508095\tBCE:71.8187\tKLD:17.6894\tC_loss:0.0000\n",
      "Train Epoch: 63 [33280/60000 (56%)]\tLoss: 91.777084\tBCE:73.6328\tKLD:18.1443\tC_loss:0.0000\n",
      "Train Epoch: 63 [35840/60000 (60%)]\tLoss: 88.061699\tBCE:70.4358\tKLD:17.6259\tC_loss:0.0000\n",
      "Train Epoch: 63 [38400/60000 (64%)]\tLoss: 93.737259\tBCE:75.3142\tKLD:18.4231\tC_loss:0.0000\n",
      "Train Epoch: 63 [40960/60000 (68%)]\tLoss: 90.248016\tBCE:72.1112\tKLD:18.1368\tC_loss:0.0000\n",
      "Train Epoch: 63 [43520/60000 (73%)]\tLoss: 89.352303\tBCE:71.8707\tKLD:17.4816\tC_loss:0.0000\n",
      "Train Epoch: 63 [46080/60000 (77%)]\tLoss: 88.576355\tBCE:71.3198\tKLD:17.2566\tC_loss:0.0000\n",
      "Train Epoch: 63 [48640/60000 (81%)]\tLoss: 89.914894\tBCE:72.6612\tKLD:17.2537\tC_loss:0.0000\n",
      "Train Epoch: 63 [51200/60000 (85%)]\tLoss: 89.227554\tBCE:71.8589\tKLD:17.3686\tC_loss:0.0000\n",
      "Train Epoch: 63 [53760/60000 (90%)]\tLoss: 89.738434\tBCE:72.1911\tKLD:17.5473\tC_loss:0.0000\n",
      "Train Epoch: 63 [56320/60000 (94%)]\tLoss: 89.043037\tBCE:71.4798\tKLD:17.5633\tC_loss:0.0000\n",
      "Train Epoch: 63 [58880/60000 (98%)]\tLoss: 89.387329\tBCE:72.0214\tKLD:17.3659\tC_loss:0.0000\n",
      "====> Epoch: 63 Average loss: 89.5736\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.6313\n",
      "Train Epoch: 64 [0/60000 (0%)]\tLoss: 90.060455\tBCE:72.3433\tKLD:17.7172\tC_loss:0.0000\n",
      "Train Epoch: 64 [2560/60000 (4%)]\tLoss: 89.286400\tBCE:71.4737\tKLD:17.8127\tC_loss:0.0000\n",
      "Train Epoch: 64 [5120/60000 (9%)]\tLoss: 91.729637\tBCE:73.9445\tKLD:17.7852\tC_loss:0.0000\n",
      "Train Epoch: 64 [7680/60000 (13%)]\tLoss: 88.965988\tBCE:71.9689\tKLD:16.9971\tC_loss:0.0000\n",
      "Train Epoch: 64 [10240/60000 (17%)]\tLoss: 90.755524\tBCE:72.6292\tKLD:18.1263\tC_loss:0.0000\n",
      "Train Epoch: 64 [12800/60000 (21%)]\tLoss: 87.091812\tBCE:70.0162\tKLD:17.0756\tC_loss:0.0000\n",
      "Train Epoch: 64 [15360/60000 (26%)]\tLoss: 89.732117\tBCE:72.3500\tKLD:17.3821\tC_loss:0.0000\n",
      "Train Epoch: 64 [17920/60000 (30%)]\tLoss: 88.908195\tBCE:71.3098\tKLD:17.5984\tC_loss:0.0000\n",
      "Train Epoch: 64 [20480/60000 (34%)]\tLoss: 89.983047\tBCE:72.8238\tKLD:17.1592\tC_loss:0.0000\n",
      "Train Epoch: 64 [23040/60000 (38%)]\tLoss: 87.270470\tBCE:69.4602\tKLD:17.8103\tC_loss:0.0000\n",
      "Train Epoch: 64 [25600/60000 (43%)]\tLoss: 87.988007\tBCE:70.9292\tKLD:17.0588\tC_loss:0.0000\n",
      "Train Epoch: 64 [28160/60000 (47%)]\tLoss: 88.272064\tBCE:70.6932\tKLD:17.5789\tC_loss:0.0000\n",
      "Train Epoch: 64 [30720/60000 (51%)]\tLoss: 90.898682\tBCE:73.2370\tKLD:17.6617\tC_loss:0.0000\n",
      "Train Epoch: 64 [33280/60000 (56%)]\tLoss: 86.670593\tBCE:69.0064\tKLD:17.6642\tC_loss:0.0000\n",
      "Train Epoch: 64 [35840/60000 (60%)]\tLoss: 86.972015\tBCE:69.9777\tKLD:16.9943\tC_loss:0.0000\n",
      "Train Epoch: 64 [38400/60000 (64%)]\tLoss: 88.333603\tBCE:70.9954\tKLD:17.3382\tC_loss:0.0000\n",
      "Train Epoch: 64 [40960/60000 (68%)]\tLoss: 91.458878\tBCE:73.8368\tKLD:17.6221\tC_loss:0.0000\n",
      "Train Epoch: 64 [43520/60000 (73%)]\tLoss: 89.019051\tBCE:71.6902\tKLD:17.3288\tC_loss:0.0000\n",
      "Train Epoch: 64 [46080/60000 (77%)]\tLoss: 91.737679\tBCE:73.7999\tKLD:17.9378\tC_loss:0.0000\n",
      "Train Epoch: 64 [48640/60000 (81%)]\tLoss: 87.592072\tBCE:70.5297\tKLD:17.0623\tC_loss:0.0000\n",
      "Train Epoch: 64 [51200/60000 (85%)]\tLoss: 88.436218\tBCE:70.7841\tKLD:17.6521\tC_loss:0.0000\n",
      "Train Epoch: 64 [53760/60000 (90%)]\tLoss: 90.812057\tBCE:73.1458\tKLD:17.6663\tC_loss:0.0000\n",
      "Train Epoch: 64 [56320/60000 (94%)]\tLoss: 86.266899\tBCE:69.3452\tKLD:16.9217\tC_loss:0.0000\n",
      "Train Epoch: 64 [58880/60000 (98%)]\tLoss: 90.018257\tBCE:72.9815\tKLD:17.0367\tC_loss:0.0000\n",
      "====> Epoch: 64 Average loss: 89.4408\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.9642\n",
      "Train Epoch: 65 [0/60000 (0%)]\tLoss: 89.493141\tBCE:71.8009\tKLD:17.6922\tC_loss:0.0000\n",
      "Train Epoch: 65 [2560/60000 (4%)]\tLoss: 87.622215\tBCE:70.3792\tKLD:17.2430\tC_loss:0.0000\n",
      "Train Epoch: 65 [5120/60000 (9%)]\tLoss: 89.355896\tBCE:72.1099\tKLD:17.2460\tC_loss:0.0000\n",
      "Train Epoch: 65 [7680/60000 (13%)]\tLoss: 90.232498\tBCE:72.7394\tKLD:17.4931\tC_loss:0.0000\n",
      "Train Epoch: 65 [10240/60000 (17%)]\tLoss: 88.821259\tBCE:71.0123\tKLD:17.8090\tC_loss:0.0000\n",
      "Train Epoch: 65 [12800/60000 (21%)]\tLoss: 91.271439\tBCE:73.5683\tKLD:17.7032\tC_loss:0.0000\n",
      "Train Epoch: 65 [15360/60000 (26%)]\tLoss: 89.476517\tBCE:71.8260\tKLD:17.6505\tC_loss:0.0000\n",
      "Train Epoch: 65 [17920/60000 (30%)]\tLoss: 90.805779\tBCE:72.8469\tKLD:17.9589\tC_loss:0.0000\n",
      "Train Epoch: 65 [20480/60000 (34%)]\tLoss: 90.786751\tBCE:73.2175\tKLD:17.5692\tC_loss:0.0000\n",
      "Train Epoch: 65 [23040/60000 (38%)]\tLoss: 89.694519\tBCE:72.1403\tKLD:17.5543\tC_loss:0.0000\n",
      "Train Epoch: 65 [25600/60000 (43%)]\tLoss: 88.052780\tBCE:70.6749\tKLD:17.3779\tC_loss:0.0000\n",
      "Train Epoch: 65 [28160/60000 (47%)]\tLoss: 91.507019\tBCE:73.8463\tKLD:17.6607\tC_loss:0.0000\n",
      "Train Epoch: 65 [30720/60000 (51%)]\tLoss: 90.760635\tBCE:73.7095\tKLD:17.0512\tC_loss:0.0000\n",
      "Train Epoch: 65 [33280/60000 (56%)]\tLoss: 88.086197\tBCE:71.0219\tKLD:17.0643\tC_loss:0.0000\n",
      "Train Epoch: 65 [35840/60000 (60%)]\tLoss: 88.830803\tBCE:71.3037\tKLD:17.5271\tC_loss:0.0000\n",
      "Train Epoch: 65 [38400/60000 (64%)]\tLoss: 89.401451\tBCE:71.7596\tKLD:17.6418\tC_loss:0.0000\n",
      "Train Epoch: 65 [40960/60000 (68%)]\tLoss: 88.274658\tBCE:70.6183\tKLD:17.6564\tC_loss:0.0000\n",
      "Train Epoch: 65 [43520/60000 (73%)]\tLoss: 89.893974\tBCE:72.1357\tKLD:17.7582\tC_loss:0.0000\n",
      "Train Epoch: 65 [46080/60000 (77%)]\tLoss: 91.116379\tBCE:74.1702\tKLD:16.9462\tC_loss:0.0000\n",
      "Train Epoch: 65 [48640/60000 (81%)]\tLoss: 87.033783\tBCE:69.8522\tKLD:17.1816\tC_loss:0.0000\n",
      "Train Epoch: 65 [51200/60000 (85%)]\tLoss: 91.156090\tBCE:73.3284\tKLD:17.8277\tC_loss:0.0000\n",
      "Train Epoch: 65 [53760/60000 (90%)]\tLoss: 90.727448\tBCE:72.4948\tKLD:18.2326\tC_loss:0.0000\n",
      "Train Epoch: 65 [56320/60000 (94%)]\tLoss: 89.107666\tBCE:71.4524\tKLD:17.6553\tC_loss:0.0000\n",
      "Train Epoch: 65 [58880/60000 (98%)]\tLoss: 88.641479\tBCE:71.1328\tKLD:17.5087\tC_loss:0.0000\n",
      "====> Epoch: 65 Average loss: 89.3525\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.8282\n",
      "Random number: 6\n",
      "Train Epoch: 66 [0/60000 (0%)]\tLoss: 87.363266\tBCE:70.5947\tKLD:16.7685\tC_loss:0.0000\n",
      "Train Epoch: 66 [2560/60000 (4%)]\tLoss: 88.582443\tBCE:71.2376\tKLD:17.3449\tC_loss:0.0000\n",
      "Train Epoch: 66 [5120/60000 (9%)]\tLoss: 92.003677\tBCE:74.3931\tKLD:17.6106\tC_loss:0.0000\n",
      "Train Epoch: 66 [7680/60000 (13%)]\tLoss: 92.482651\tBCE:74.7807\tKLD:17.7020\tC_loss:0.0000\n",
      "Train Epoch: 66 [10240/60000 (17%)]\tLoss: 88.963287\tBCE:71.4280\tKLD:17.5353\tC_loss:0.0000\n",
      "Train Epoch: 66 [12800/60000 (21%)]\tLoss: 88.546432\tBCE:71.1898\tKLD:17.3566\tC_loss:0.0000\n",
      "Train Epoch: 66 [15360/60000 (26%)]\tLoss: 86.167740\tBCE:69.4752\tKLD:16.6926\tC_loss:0.0000\n",
      "Train Epoch: 66 [17920/60000 (30%)]\tLoss: 86.665131\tBCE:69.3499\tKLD:17.3152\tC_loss:0.0000\n",
      "Train Epoch: 66 [20480/60000 (34%)]\tLoss: 88.929214\tBCE:71.3788\tKLD:17.5504\tC_loss:0.0000\n",
      "Train Epoch: 66 [23040/60000 (38%)]\tLoss: 87.156876\tBCE:70.0828\tKLD:17.0741\tC_loss:0.0000\n",
      "Train Epoch: 66 [25600/60000 (43%)]\tLoss: 90.272720\tBCE:72.4259\tKLD:17.8468\tC_loss:0.0000\n",
      "Train Epoch: 66 [28160/60000 (47%)]\tLoss: 86.629204\tBCE:69.5412\tKLD:17.0880\tC_loss:0.0000\n",
      "Train Epoch: 66 [30720/60000 (51%)]\tLoss: 91.275177\tBCE:73.7468\tKLD:17.5284\tC_loss:0.0000\n",
      "Train Epoch: 66 [33280/60000 (56%)]\tLoss: 87.285721\tBCE:69.8072\tKLD:17.4786\tC_loss:0.0000\n",
      "Train Epoch: 66 [35840/60000 (60%)]\tLoss: 87.550705\tBCE:70.5116\tKLD:17.0392\tC_loss:0.0000\n",
      "Train Epoch: 66 [38400/60000 (64%)]\tLoss: 89.878304\tBCE:72.4534\tKLD:17.4249\tC_loss:0.0000\n",
      "Train Epoch: 66 [40960/60000 (68%)]\tLoss: 87.871544\tBCE:70.3580\tKLD:17.5135\tC_loss:0.0000\n",
      "Train Epoch: 66 [43520/60000 (73%)]\tLoss: 88.302673\tBCE:71.0272\tKLD:17.2754\tC_loss:0.0000\n",
      "Train Epoch: 66 [46080/60000 (77%)]\tLoss: 87.902939\tBCE:70.7938\tKLD:17.1092\tC_loss:0.0000\n",
      "Train Epoch: 66 [48640/60000 (81%)]\tLoss: 88.975616\tBCE:71.6812\tKLD:17.2944\tC_loss:0.0000\n",
      "Train Epoch: 66 [51200/60000 (85%)]\tLoss: 91.576538\tBCE:73.8491\tKLD:17.7274\tC_loss:0.0000\n",
      "Train Epoch: 66 [53760/60000 (90%)]\tLoss: 90.650314\tBCE:72.9800\tKLD:17.6703\tC_loss:0.0000\n",
      "Train Epoch: 66 [56320/60000 (94%)]\tLoss: 89.455734\tBCE:71.8604\tKLD:17.5953\tC_loss:0.0000\n",
      "Train Epoch: 66 [58880/60000 (98%)]\tLoss: 91.014809\tBCE:73.4924\tKLD:17.5224\tC_loss:0.0000\n",
      "====> Epoch: 66 Average loss: 89.4246\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0396\n",
      "Train Epoch: 67 [0/60000 (0%)]\tLoss: 86.461800\tBCE:69.0513\tKLD:17.4105\tC_loss:0.0000\n",
      "Train Epoch: 67 [2560/60000 (4%)]\tLoss: 91.290520\tBCE:73.6695\tKLD:17.6211\tC_loss:0.0000\n",
      "Train Epoch: 67 [5120/60000 (9%)]\tLoss: 90.166000\tBCE:72.6934\tKLD:17.4726\tC_loss:0.0000\n",
      "Train Epoch: 67 [7680/60000 (13%)]\tLoss: 88.215332\tBCE:70.6356\tKLD:17.5797\tC_loss:0.0000\n",
      "Train Epoch: 67 [10240/60000 (17%)]\tLoss: 89.656303\tBCE:72.4066\tKLD:17.2497\tC_loss:0.0000\n",
      "Train Epoch: 67 [12800/60000 (21%)]\tLoss: 88.195648\tBCE:70.6116\tKLD:17.5841\tC_loss:0.0000\n",
      "Train Epoch: 67 [15360/60000 (26%)]\tLoss: 86.846390\tBCE:69.4466\tKLD:17.3998\tC_loss:0.0000\n",
      "Train Epoch: 67 [17920/60000 (30%)]\tLoss: 89.621307\tBCE:71.8697\tKLD:17.7516\tC_loss:0.0000\n",
      "Train Epoch: 67 [20480/60000 (34%)]\tLoss: 87.174164\tBCE:69.9225\tKLD:17.2517\tC_loss:0.0000\n",
      "Train Epoch: 67 [23040/60000 (38%)]\tLoss: 91.083176\tBCE:73.7956\tKLD:17.2875\tC_loss:0.0000\n",
      "Train Epoch: 67 [25600/60000 (43%)]\tLoss: 88.784149\tBCE:71.0372\tKLD:17.7470\tC_loss:0.0000\n",
      "Train Epoch: 67 [28160/60000 (47%)]\tLoss: 91.467728\tBCE:73.6939\tKLD:17.7738\tC_loss:0.0000\n",
      "Train Epoch: 67 [30720/60000 (51%)]\tLoss: 89.370850\tBCE:71.9048\tKLD:17.4660\tC_loss:0.0000\n",
      "Train Epoch: 67 [33280/60000 (56%)]\tLoss: 89.529404\tBCE:71.8470\tKLD:17.6824\tC_loss:0.0000\n",
      "Train Epoch: 67 [35840/60000 (60%)]\tLoss: 90.444923\tBCE:72.6441\tKLD:17.8008\tC_loss:0.0000\n",
      "Train Epoch: 67 [38400/60000 (64%)]\tLoss: 87.985756\tBCE:70.4134\tKLD:17.5724\tC_loss:0.0000\n",
      "Train Epoch: 67 [40960/60000 (68%)]\tLoss: 89.688690\tBCE:72.5560\tKLD:17.1327\tC_loss:0.0000\n",
      "Train Epoch: 67 [43520/60000 (73%)]\tLoss: 90.054192\tBCE:72.4303\tKLD:17.6239\tC_loss:0.0000\n",
      "Train Epoch: 67 [46080/60000 (77%)]\tLoss: 87.163506\tBCE:69.9645\tKLD:17.1990\tC_loss:0.0000\n",
      "Train Epoch: 67 [48640/60000 (81%)]\tLoss: 88.846481\tBCE:71.6530\tKLD:17.1935\tC_loss:0.0000\n",
      "Train Epoch: 67 [51200/60000 (85%)]\tLoss: 88.872940\tBCE:71.3483\tKLD:17.5246\tC_loss:0.0000\n",
      "Train Epoch: 67 [53760/60000 (90%)]\tLoss: 90.012650\tBCE:72.6148\tKLD:17.3978\tC_loss:0.0000\n",
      "Train Epoch: 67 [56320/60000 (94%)]\tLoss: 90.263618\tBCE:72.6643\tKLD:17.5994\tC_loss:0.0000\n",
      "Train Epoch: 67 [58880/60000 (98%)]\tLoss: 88.344482\tBCE:70.5332\tKLD:17.8112\tC_loss:0.0000\n",
      "====> Epoch: 67 Average loss: 89.2805\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.1890\n",
      "Train Epoch: 68 [0/60000 (0%)]\tLoss: 89.428970\tBCE:71.2419\tKLD:18.1871\tC_loss:0.0000\n",
      "Train Epoch: 68 [2560/60000 (4%)]\tLoss: 89.160728\tBCE:71.6931\tKLD:17.4676\tC_loss:0.0000\n",
      "Train Epoch: 68 [5120/60000 (9%)]\tLoss: 86.937378\tBCE:69.5440\tKLD:17.3933\tC_loss:0.0000\n",
      "Train Epoch: 68 [7680/60000 (13%)]\tLoss: 90.743423\tBCE:73.6787\tKLD:17.0648\tC_loss:0.0000\n",
      "Train Epoch: 68 [10240/60000 (17%)]\tLoss: 89.063553\tBCE:71.4934\tKLD:17.5701\tC_loss:0.0000\n",
      "Train Epoch: 68 [12800/60000 (21%)]\tLoss: 91.218079\tBCE:73.3002\tKLD:17.9179\tC_loss:0.0000\n",
      "Train Epoch: 68 [15360/60000 (26%)]\tLoss: 89.037827\tBCE:72.0599\tKLD:16.9779\tC_loss:0.0000\n",
      "Train Epoch: 68 [17920/60000 (30%)]\tLoss: 88.638306\tBCE:70.9722\tKLD:17.6661\tC_loss:0.0000\n",
      "Train Epoch: 68 [20480/60000 (34%)]\tLoss: 89.645966\tBCE:72.3557\tKLD:17.2903\tC_loss:0.0000\n",
      "Train Epoch: 68 [23040/60000 (38%)]\tLoss: 88.205818\tBCE:70.8659\tKLD:17.3400\tC_loss:0.0000\n",
      "Train Epoch: 68 [25600/60000 (43%)]\tLoss: 89.489532\tBCE:71.7365\tKLD:17.7531\tC_loss:0.0000\n",
      "Train Epoch: 68 [28160/60000 (47%)]\tLoss: 91.552544\tBCE:73.5378\tKLD:18.0147\tC_loss:0.0000\n",
      "Train Epoch: 68 [30720/60000 (51%)]\tLoss: 87.504417\tBCE:70.0379\tKLD:17.4665\tC_loss:0.0000\n",
      "Train Epoch: 68 [33280/60000 (56%)]\tLoss: 88.883308\tBCE:71.0633\tKLD:17.8200\tC_loss:0.0000\n",
      "Train Epoch: 68 [35840/60000 (60%)]\tLoss: 88.778481\tBCE:71.3061\tKLD:17.4724\tC_loss:0.0000\n",
      "Train Epoch: 68 [38400/60000 (64%)]\tLoss: 89.495041\tBCE:71.4729\tKLD:18.0221\tC_loss:0.0000\n",
      "Train Epoch: 68 [40960/60000 (68%)]\tLoss: 88.698547\tBCE:71.3786\tKLD:17.3200\tC_loss:0.0000\n",
      "Train Epoch: 68 [43520/60000 (73%)]\tLoss: 89.003822\tBCE:71.2922\tKLD:17.7116\tC_loss:0.0000\n",
      "Train Epoch: 68 [46080/60000 (77%)]\tLoss: 93.176224\tBCE:74.8553\tKLD:18.3209\tC_loss:0.0000\n",
      "Train Epoch: 68 [48640/60000 (81%)]\tLoss: 89.820290\tBCE:72.2993\tKLD:17.5210\tC_loss:0.0000\n",
      "Train Epoch: 68 [51200/60000 (85%)]\tLoss: 87.525604\tBCE:70.4875\tKLD:17.0381\tC_loss:0.0000\n",
      "Train Epoch: 68 [53760/60000 (90%)]\tLoss: 87.151619\tBCE:70.2181\tKLD:16.9335\tC_loss:0.0000\n",
      "Train Epoch: 68 [56320/60000 (94%)]\tLoss: 91.198792\tBCE:73.1673\tKLD:18.0315\tC_loss:0.0000\n",
      "Train Epoch: 68 [58880/60000 (98%)]\tLoss: 86.610497\tBCE:69.6086\tKLD:17.0019\tC_loss:0.0000\n",
      "====> Epoch: 68 Average loss: 89.1784\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.9621\n",
      "Train Epoch: 69 [0/60000 (0%)]\tLoss: 87.113403\tBCE:70.0415\tKLD:17.0719\tC_loss:0.0000\n",
      "Train Epoch: 69 [2560/60000 (4%)]\tLoss: 88.957657\tBCE:71.0295\tKLD:17.9282\tC_loss:0.0000\n",
      "Train Epoch: 69 [5120/60000 (9%)]\tLoss: 89.761833\tBCE:72.1730\tKLD:17.5888\tC_loss:0.0000\n",
      "Train Epoch: 69 [7680/60000 (13%)]\tLoss: 89.398308\tBCE:71.7406\tKLD:17.6577\tC_loss:0.0000\n",
      "Train Epoch: 69 [10240/60000 (17%)]\tLoss: 90.723846\tBCE:72.8661\tKLD:17.8577\tC_loss:0.0000\n",
      "Train Epoch: 69 [12800/60000 (21%)]\tLoss: 91.030907\tBCE:72.9227\tKLD:18.1082\tC_loss:0.0000\n",
      "Train Epoch: 69 [15360/60000 (26%)]\tLoss: 86.198639\tBCE:68.8949\tKLD:17.3038\tC_loss:0.0000\n",
      "Train Epoch: 69 [17920/60000 (30%)]\tLoss: 89.536026\tBCE:72.2934\tKLD:17.2426\tC_loss:0.0000\n",
      "Train Epoch: 69 [20480/60000 (34%)]\tLoss: 88.579636\tBCE:71.2315\tKLD:17.3482\tC_loss:0.0000\n",
      "Train Epoch: 69 [23040/60000 (38%)]\tLoss: 91.379044\tBCE:73.7177\tKLD:17.6613\tC_loss:0.0000\n",
      "Train Epoch: 69 [25600/60000 (43%)]\tLoss: 89.590057\tBCE:72.4915\tKLD:17.0985\tC_loss:0.0000\n",
      "Train Epoch: 69 [28160/60000 (47%)]\tLoss: 93.855690\tBCE:75.5210\tKLD:18.3347\tC_loss:0.0000\n",
      "Train Epoch: 69 [30720/60000 (51%)]\tLoss: 86.400475\tBCE:69.6298\tKLD:16.7707\tC_loss:0.0000\n",
      "Train Epoch: 69 [33280/60000 (56%)]\tLoss: 87.914719\tBCE:70.5227\tKLD:17.3920\tC_loss:0.0000\n",
      "Train Epoch: 69 [35840/60000 (60%)]\tLoss: 92.153793\tBCE:74.0962\tKLD:18.0576\tC_loss:0.0000\n",
      "Train Epoch: 69 [38400/60000 (64%)]\tLoss: 88.447990\tBCE:70.9861\tKLD:17.4619\tC_loss:0.0000\n",
      "Train Epoch: 69 [40960/60000 (68%)]\tLoss: 89.250427\tBCE:72.1232\tKLD:17.1272\tC_loss:0.0000\n",
      "Train Epoch: 69 [43520/60000 (73%)]\tLoss: 92.330276\tBCE:74.1759\tKLD:18.1543\tC_loss:0.0000\n",
      "Train Epoch: 69 [46080/60000 (77%)]\tLoss: 89.795197\tBCE:72.4367\tKLD:17.3585\tC_loss:0.0000\n",
      "Train Epoch: 69 [48640/60000 (81%)]\tLoss: 89.707344\tBCE:71.8423\tKLD:17.8650\tC_loss:0.0000\n",
      "Train Epoch: 69 [51200/60000 (85%)]\tLoss: 87.993080\tBCE:70.8659\tKLD:17.1272\tC_loss:0.0000\n",
      "Train Epoch: 69 [53760/60000 (90%)]\tLoss: 87.977493\tBCE:70.3811\tKLD:17.5964\tC_loss:0.0000\n",
      "Train Epoch: 69 [56320/60000 (94%)]\tLoss: 91.365570\tBCE:73.6553\tKLD:17.7103\tC_loss:0.0000\n",
      "Train Epoch: 69 [58880/60000 (98%)]\tLoss: 88.937210\tBCE:71.5748\tKLD:17.3624\tC_loss:0.0000\n",
      "====> Epoch: 69 Average loss: 89.2682\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.5771\n",
      "Train Epoch: 70 [0/60000 (0%)]\tLoss: 88.768402\tBCE:70.6583\tKLD:18.1101\tC_loss:0.0000\n",
      "Train Epoch: 70 [2560/60000 (4%)]\tLoss: 91.610107\tBCE:73.8034\tKLD:17.8067\tC_loss:0.0000\n",
      "Train Epoch: 70 [5120/60000 (9%)]\tLoss: 87.428749\tBCE:70.4230\tKLD:17.0057\tC_loss:0.0000\n",
      "Train Epoch: 70 [7680/60000 (13%)]\tLoss: 91.196075\tBCE:73.3475\tKLD:17.8486\tC_loss:0.0000\n",
      "Train Epoch: 70 [10240/60000 (17%)]\tLoss: 89.269463\tBCE:71.6756\tKLD:17.5938\tC_loss:0.0000\n",
      "Train Epoch: 70 [12800/60000 (21%)]\tLoss: 86.006523\tBCE:68.6224\tKLD:17.3841\tC_loss:0.0000\n",
      "Train Epoch: 70 [15360/60000 (26%)]\tLoss: 91.593948\tBCE:73.7804\tKLD:17.8136\tC_loss:0.0000\n",
      "Train Epoch: 70 [17920/60000 (30%)]\tLoss: 87.016830\tBCE:69.5826\tKLD:17.4342\tC_loss:0.0000\n",
      "Train Epoch: 70 [20480/60000 (34%)]\tLoss: 91.561226\tBCE:73.5436\tKLD:18.0176\tC_loss:0.0000\n",
      "Train Epoch: 70 [23040/60000 (38%)]\tLoss: 87.907463\tBCE:70.0465\tKLD:17.8610\tC_loss:0.0000\n",
      "Train Epoch: 70 [25600/60000 (43%)]\tLoss: 89.270065\tBCE:71.4575\tKLD:17.8126\tC_loss:0.0000\n",
      "Train Epoch: 70 [28160/60000 (47%)]\tLoss: 91.535408\tBCE:73.5825\tKLD:17.9529\tC_loss:0.0000\n",
      "Train Epoch: 70 [30720/60000 (51%)]\tLoss: 87.664139\tBCE:70.3153\tKLD:17.3489\tC_loss:0.0000\n",
      "Train Epoch: 70 [33280/60000 (56%)]\tLoss: 90.303024\tBCE:72.4926\tKLD:17.8105\tC_loss:0.0000\n",
      "Train Epoch: 70 [35840/60000 (60%)]\tLoss: 90.606262\tBCE:72.8616\tKLD:17.7447\tC_loss:0.0000\n",
      "Train Epoch: 70 [38400/60000 (64%)]\tLoss: 89.722260\tBCE:71.9485\tKLD:17.7737\tC_loss:0.0000\n",
      "Train Epoch: 70 [40960/60000 (68%)]\tLoss: 87.424599\tBCE:70.1669\tKLD:17.2577\tC_loss:0.0000\n",
      "Train Epoch: 70 [43520/60000 (73%)]\tLoss: 90.109337\tBCE:72.4952\tKLD:17.6141\tC_loss:0.0000\n",
      "Train Epoch: 70 [46080/60000 (77%)]\tLoss: 88.468819\tBCE:70.7046\tKLD:17.7642\tC_loss:0.0000\n",
      "Train Epoch: 70 [48640/60000 (81%)]\tLoss: 88.938530\tBCE:71.5508\tKLD:17.3877\tC_loss:0.0000\n",
      "Train Epoch: 70 [51200/60000 (85%)]\tLoss: 90.768005\tBCE:72.6960\tKLD:18.0720\tC_loss:0.0000\n",
      "Train Epoch: 70 [53760/60000 (90%)]\tLoss: 92.034515\tBCE:74.0925\tKLD:17.9420\tC_loss:0.0000\n",
      "Train Epoch: 70 [56320/60000 (94%)]\tLoss: 88.084465\tBCE:70.6592\tKLD:17.4253\tC_loss:0.0000\n",
      "Train Epoch: 70 [58880/60000 (98%)]\tLoss: 89.021225\tBCE:71.3508\tKLD:17.6705\tC_loss:0.0000\n",
      "====> Epoch: 70 Average loss: 89.1849\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.1712\n",
      "Random number: 8\n",
      "Train Epoch: 71 [0/60000 (0%)]\tLoss: 88.735947\tBCE:71.5587\tKLD:17.1773\tC_loss:0.0000\n",
      "Train Epoch: 71 [2560/60000 (4%)]\tLoss: 86.876762\tBCE:69.4161\tKLD:17.4607\tC_loss:0.0000\n",
      "Train Epoch: 71 [5120/60000 (9%)]\tLoss: 91.266754\tBCE:73.3135\tKLD:17.9532\tC_loss:0.0000\n",
      "Train Epoch: 71 [7680/60000 (13%)]\tLoss: 88.470825\tBCE:71.5050\tKLD:16.9659\tC_loss:0.0000\n",
      "Train Epoch: 71 [10240/60000 (17%)]\tLoss: 89.123566\tBCE:71.8486\tKLD:17.2750\tC_loss:0.0000\n",
      "Train Epoch: 71 [12800/60000 (21%)]\tLoss: 89.383888\tBCE:71.5376\tKLD:17.8463\tC_loss:0.0000\n",
      "Train Epoch: 71 [15360/60000 (26%)]\tLoss: 91.102524\tBCE:73.8142\tKLD:17.2884\tC_loss:0.0000\n",
      "Train Epoch: 71 [17920/60000 (30%)]\tLoss: 94.462082\tBCE:75.2730\tKLD:19.1891\tC_loss:0.0000\n",
      "Train Epoch: 71 [20480/60000 (34%)]\tLoss: 90.714401\tBCE:73.7819\tKLD:16.9325\tC_loss:0.0000\n",
      "Train Epoch: 71 [23040/60000 (38%)]\tLoss: 88.008606\tBCE:70.7351\tKLD:17.2735\tC_loss:0.0000\n",
      "Train Epoch: 71 [25600/60000 (43%)]\tLoss: 87.880005\tBCE:70.4370\tKLD:17.4430\tC_loss:0.0000\n",
      "Train Epoch: 71 [28160/60000 (47%)]\tLoss: 89.604897\tBCE:72.3805\tKLD:17.2244\tC_loss:0.0000\n",
      "Train Epoch: 71 [30720/60000 (51%)]\tLoss: 88.730164\tBCE:70.9371\tKLD:17.7931\tC_loss:0.0000\n",
      "Train Epoch: 71 [33280/60000 (56%)]\tLoss: 87.328552\tBCE:69.8784\tKLD:17.4502\tC_loss:0.0000\n",
      "Train Epoch: 71 [35840/60000 (60%)]\tLoss: 88.216682\tBCE:70.6260\tKLD:17.5906\tC_loss:0.0000\n",
      "Train Epoch: 71 [38400/60000 (64%)]\tLoss: 89.688736\tBCE:71.9723\tKLD:17.7165\tC_loss:0.0000\n",
      "Train Epoch: 71 [40960/60000 (68%)]\tLoss: 91.575096\tBCE:73.9470\tKLD:17.6281\tC_loss:0.0000\n",
      "Train Epoch: 71 [43520/60000 (73%)]\tLoss: 89.816986\tBCE:71.9111\tKLD:17.9059\tC_loss:0.0000\n",
      "Train Epoch: 71 [46080/60000 (77%)]\tLoss: 87.675690\tBCE:70.8484\tKLD:16.8273\tC_loss:0.0000\n",
      "Train Epoch: 71 [48640/60000 (81%)]\tLoss: 89.145752\tBCE:71.4451\tKLD:17.7006\tC_loss:0.0000\n",
      "Train Epoch: 71 [51200/60000 (85%)]\tLoss: 88.564888\tBCE:71.0052\tKLD:17.5597\tC_loss:0.0000\n",
      "Train Epoch: 71 [53760/60000 (90%)]\tLoss: 89.984436\tBCE:72.4764\tKLD:17.5081\tC_loss:0.0000\n",
      "Train Epoch: 71 [56320/60000 (94%)]\tLoss: 87.270210\tBCE:69.8965\tKLD:17.3737\tC_loss:0.0000\n",
      "Train Epoch: 71 [58880/60000 (98%)]\tLoss: 89.985649\tBCE:72.6097\tKLD:17.3759\tC_loss:0.0000\n",
      "====> Epoch: 71 Average loss: 89.2175\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.2041\n",
      "Train Epoch: 72 [0/60000 (0%)]\tLoss: 88.698265\tBCE:70.9306\tKLD:17.7677\tC_loss:0.0000\n",
      "Train Epoch: 72 [2560/60000 (4%)]\tLoss: 90.195045\tBCE:72.5286\tKLD:17.6665\tC_loss:0.0000\n",
      "Train Epoch: 72 [5120/60000 (9%)]\tLoss: 91.451843\tBCE:73.9138\tKLD:17.5380\tC_loss:0.0000\n",
      "Train Epoch: 72 [7680/60000 (13%)]\tLoss: 89.218414\tBCE:72.0638\tKLD:17.1546\tC_loss:0.0000\n",
      "Train Epoch: 72 [10240/60000 (17%)]\tLoss: 91.508606\tBCE:73.4561\tKLD:18.0525\tC_loss:0.0000\n",
      "Train Epoch: 72 [12800/60000 (21%)]\tLoss: 90.247124\tBCE:72.4662\tKLD:17.7809\tC_loss:0.0000\n",
      "Train Epoch: 72 [15360/60000 (26%)]\tLoss: 88.409149\tBCE:70.6442\tKLD:17.7649\tC_loss:0.0000\n",
      "Train Epoch: 72 [17920/60000 (30%)]\tLoss: 90.378662\tBCE:71.9690\tKLD:18.4097\tC_loss:0.0000\n",
      "Train Epoch: 72 [20480/60000 (34%)]\tLoss: 88.893997\tBCE:71.1472\tKLD:17.7468\tC_loss:0.0000\n",
      "Train Epoch: 72 [23040/60000 (38%)]\tLoss: 88.732948\tBCE:71.0650\tKLD:17.6680\tC_loss:0.0000\n",
      "Train Epoch: 72 [25600/60000 (43%)]\tLoss: 88.827179\tBCE:71.4669\tKLD:17.3603\tC_loss:0.0000\n",
      "Train Epoch: 72 [28160/60000 (47%)]\tLoss: 87.219215\tBCE:70.2103\tKLD:17.0089\tC_loss:0.0000\n",
      "Train Epoch: 72 [30720/60000 (51%)]\tLoss: 92.165939\tBCE:73.7697\tKLD:18.3962\tC_loss:0.0000\n",
      "Train Epoch: 72 [33280/60000 (56%)]\tLoss: 87.306656\tBCE:69.9363\tKLD:17.3703\tC_loss:0.0000\n",
      "Train Epoch: 72 [35840/60000 (60%)]\tLoss: 88.420547\tBCE:70.9763\tKLD:17.4442\tC_loss:0.0000\n",
      "Train Epoch: 72 [38400/60000 (64%)]\tLoss: 88.212517\tBCE:70.9342\tKLD:17.2783\tC_loss:0.0000\n",
      "Train Epoch: 72 [40960/60000 (68%)]\tLoss: 90.149574\tBCE:71.9115\tKLD:18.2381\tC_loss:0.0000\n",
      "Train Epoch: 72 [43520/60000 (73%)]\tLoss: 87.726540\tBCE:70.4765\tKLD:17.2500\tC_loss:0.0000\n",
      "Train Epoch: 72 [46080/60000 (77%)]\tLoss: 85.967384\tBCE:68.8271\tKLD:17.1403\tC_loss:0.0000\n",
      "Train Epoch: 72 [48640/60000 (81%)]\tLoss: 88.108688\tBCE:70.6475\tKLD:17.4612\tC_loss:0.0000\n",
      "Train Epoch: 72 [51200/60000 (85%)]\tLoss: 91.570747\tBCE:74.0024\tKLD:17.5683\tC_loss:0.0000\n",
      "Train Epoch: 72 [53760/60000 (90%)]\tLoss: 88.722900\tBCE:71.1261\tKLD:17.5968\tC_loss:0.0000\n",
      "Train Epoch: 72 [56320/60000 (94%)]\tLoss: 88.945801\tBCE:71.2616\tKLD:17.6842\tC_loss:0.0000\n",
      "Train Epoch: 72 [58880/60000 (98%)]\tLoss: 87.451706\tBCE:69.9490\tKLD:17.5027\tC_loss:0.0000\n",
      "====> Epoch: 72 Average loss: 89.0672\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.8267\n",
      "Train Epoch: 73 [0/60000 (0%)]\tLoss: 88.724846\tBCE:71.5417\tKLD:17.1832\tC_loss:0.0000\n",
      "Train Epoch: 73 [2560/60000 (4%)]\tLoss: 88.437614\tBCE:70.8487\tKLD:17.5889\tC_loss:0.0000\n",
      "Train Epoch: 73 [5120/60000 (9%)]\tLoss: 88.882072\tBCE:70.9551\tKLD:17.9270\tC_loss:0.0000\n",
      "Train Epoch: 73 [7680/60000 (13%)]\tLoss: 87.176498\tBCE:70.2428\tKLD:16.9337\tC_loss:0.0000\n",
      "Train Epoch: 73 [10240/60000 (17%)]\tLoss: 89.531738\tBCE:71.6132\tKLD:17.9186\tC_loss:0.0000\n",
      "Train Epoch: 73 [12800/60000 (21%)]\tLoss: 89.201721\tBCE:71.9144\tKLD:17.2873\tC_loss:0.0000\n",
      "Train Epoch: 73 [15360/60000 (26%)]\tLoss: 89.537201\tBCE:71.9925\tKLD:17.5447\tC_loss:0.0000\n",
      "Train Epoch: 73 [17920/60000 (30%)]\tLoss: 87.431709\tBCE:69.8514\tKLD:17.5803\tC_loss:0.0000\n",
      "Train Epoch: 73 [20480/60000 (34%)]\tLoss: 87.785591\tBCE:70.5595\tKLD:17.2261\tC_loss:0.0000\n",
      "Train Epoch: 73 [23040/60000 (38%)]\tLoss: 88.985977\tBCE:71.4256\tKLD:17.5604\tC_loss:0.0000\n",
      "Train Epoch: 73 [25600/60000 (43%)]\tLoss: 88.419922\tBCE:70.4798\tKLD:17.9401\tC_loss:0.0000\n",
      "Train Epoch: 73 [28160/60000 (47%)]\tLoss: 91.473068\tBCE:73.4644\tKLD:18.0087\tC_loss:0.0000\n",
      "Train Epoch: 73 [30720/60000 (51%)]\tLoss: 90.393311\tBCE:72.7520\tKLD:17.6413\tC_loss:0.0000\n",
      "Train Epoch: 73 [33280/60000 (56%)]\tLoss: 88.452515\tBCE:70.9341\tKLD:17.5184\tC_loss:0.0000\n",
      "Train Epoch: 73 [35840/60000 (60%)]\tLoss: 88.432106\tBCE:70.6946\tKLD:17.7375\tC_loss:0.0000\n",
      "Train Epoch: 73 [38400/60000 (64%)]\tLoss: 90.696365\tBCE:72.7411\tKLD:17.9553\tC_loss:0.0000\n",
      "Train Epoch: 73 [40960/60000 (68%)]\tLoss: 88.058624\tBCE:70.7051\tKLD:17.3535\tC_loss:0.0000\n",
      "Train Epoch: 73 [43520/60000 (73%)]\tLoss: 89.089249\tBCE:71.2086\tKLD:17.8806\tC_loss:0.0000\n",
      "Train Epoch: 73 [46080/60000 (77%)]\tLoss: 88.647278\tBCE:70.9967\tKLD:17.6505\tC_loss:0.0000\n",
      "Train Epoch: 73 [48640/60000 (81%)]\tLoss: 90.579979\tBCE:72.6630\tKLD:17.9169\tC_loss:0.0000\n",
      "Train Epoch: 73 [51200/60000 (85%)]\tLoss: 88.059265\tBCE:71.0229\tKLD:17.0364\tC_loss:0.0000\n",
      "Train Epoch: 73 [53760/60000 (90%)]\tLoss: 89.721306\tBCE:72.1931\tKLD:17.5282\tC_loss:0.0000\n",
      "Train Epoch: 73 [56320/60000 (94%)]\tLoss: 89.108467\tBCE:71.5502\tKLD:17.5583\tC_loss:0.0000\n",
      "Train Epoch: 73 [58880/60000 (98%)]\tLoss: 89.575211\tBCE:71.9574\tKLD:17.6178\tC_loss:0.0000\n",
      "====> Epoch: 73 Average loss: 89.1021\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0418\n",
      "Train Epoch: 74 [0/60000 (0%)]\tLoss: 89.777962\tBCE:71.7224\tKLD:18.0556\tC_loss:0.0000\n",
      "Train Epoch: 74 [2560/60000 (4%)]\tLoss: 88.979538\tBCE:71.4250\tKLD:17.5546\tC_loss:0.0000\n",
      "Train Epoch: 74 [5120/60000 (9%)]\tLoss: 88.467255\tBCE:70.5219\tKLD:17.9453\tC_loss:0.0000\n",
      "Train Epoch: 74 [7680/60000 (13%)]\tLoss: 87.826126\tBCE:70.4604\tKLD:17.3657\tC_loss:0.0000\n",
      "Train Epoch: 74 [10240/60000 (17%)]\tLoss: 88.855881\tBCE:71.5190\tKLD:17.3369\tC_loss:0.0000\n",
      "Train Epoch: 74 [12800/60000 (21%)]\tLoss: 90.283524\tBCE:72.7119\tKLD:17.5716\tC_loss:0.0000\n",
      "Train Epoch: 74 [15360/60000 (26%)]\tLoss: 90.803444\tBCE:72.0865\tKLD:18.7170\tC_loss:0.0000\n",
      "Train Epoch: 74 [17920/60000 (30%)]\tLoss: 90.673729\tBCE:72.6841\tKLD:17.9896\tC_loss:0.0000\n",
      "Train Epoch: 74 [20480/60000 (34%)]\tLoss: 89.233704\tBCE:72.1705\tKLD:17.0632\tC_loss:0.0000\n",
      "Train Epoch: 74 [23040/60000 (38%)]\tLoss: 87.135422\tBCE:69.8413\tKLD:17.2941\tC_loss:0.0000\n",
      "Train Epoch: 74 [25600/60000 (43%)]\tLoss: 86.478294\tBCE:69.2600\tKLD:17.2183\tC_loss:0.0000\n",
      "Train Epoch: 74 [28160/60000 (47%)]\tLoss: 90.091331\tBCE:72.1807\tKLD:17.9107\tC_loss:0.0000\n",
      "Train Epoch: 74 [30720/60000 (51%)]\tLoss: 91.778229\tBCE:73.4511\tKLD:18.3272\tC_loss:0.0000\n",
      "Train Epoch: 74 [33280/60000 (56%)]\tLoss: 87.255959\tBCE:69.8411\tKLD:17.4149\tC_loss:0.0000\n",
      "Train Epoch: 74 [35840/60000 (60%)]\tLoss: 89.385559\tBCE:72.0910\tKLD:17.2946\tC_loss:0.0000\n",
      "Train Epoch: 74 [38400/60000 (64%)]\tLoss: 89.171356\tBCE:71.6920\tKLD:17.4794\tC_loss:0.0000\n",
      "Train Epoch: 74 [40960/60000 (68%)]\tLoss: 88.156586\tBCE:70.8789\tKLD:17.2777\tC_loss:0.0000\n",
      "Train Epoch: 74 [43520/60000 (73%)]\tLoss: 88.899261\tBCE:71.5162\tKLD:17.3831\tC_loss:0.0000\n",
      "Train Epoch: 74 [46080/60000 (77%)]\tLoss: 90.372971\tBCE:73.0897\tKLD:17.2833\tC_loss:0.0000\n",
      "Train Epoch: 74 [48640/60000 (81%)]\tLoss: 88.978043\tBCE:70.9663\tKLD:18.0117\tC_loss:0.0000\n",
      "Train Epoch: 74 [51200/60000 (85%)]\tLoss: 89.509186\tBCE:71.9006\tKLD:17.6086\tC_loss:0.0000\n",
      "Train Epoch: 74 [53760/60000 (90%)]\tLoss: 89.296509\tBCE:71.8044\tKLD:17.4922\tC_loss:0.0000\n",
      "Train Epoch: 74 [56320/60000 (94%)]\tLoss: 91.872459\tBCE:73.7649\tKLD:18.1075\tC_loss:0.0000\n",
      "Train Epoch: 74 [58880/60000 (98%)]\tLoss: 88.966187\tBCE:71.8814\tKLD:17.0848\tC_loss:0.0000\n",
      "====> Epoch: 74 Average loss: 89.0224\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.2236\n",
      "Train Epoch: 75 [0/60000 (0%)]\tLoss: 89.491287\tBCE:71.2841\tKLD:18.2072\tC_loss:0.0000\n",
      "Train Epoch: 75 [2560/60000 (4%)]\tLoss: 89.267059\tBCE:71.4952\tKLD:17.7719\tC_loss:0.0000\n",
      "Train Epoch: 75 [5120/60000 (9%)]\tLoss: 90.408417\tBCE:72.4271\tKLD:17.9813\tC_loss:0.0000\n",
      "Train Epoch: 75 [7680/60000 (13%)]\tLoss: 88.436394\tBCE:71.3375\tKLD:17.0989\tC_loss:0.0000\n",
      "Train Epoch: 75 [10240/60000 (17%)]\tLoss: 88.646492\tBCE:70.7419\tKLD:17.9046\tC_loss:0.0000\n",
      "Train Epoch: 75 [12800/60000 (21%)]\tLoss: 87.033020\tBCE:69.3008\tKLD:17.7322\tC_loss:0.0000\n",
      "Train Epoch: 75 [15360/60000 (26%)]\tLoss: 88.519516\tBCE:71.5807\tKLD:16.9388\tC_loss:0.0000\n",
      "Train Epoch: 75 [17920/60000 (30%)]\tLoss: 88.836578\tBCE:70.4591\tKLD:18.3775\tC_loss:0.0000\n",
      "Train Epoch: 75 [20480/60000 (34%)]\tLoss: 90.538544\tBCE:72.9013\tKLD:17.6372\tC_loss:0.0000\n",
      "Train Epoch: 75 [23040/60000 (38%)]\tLoss: 88.510994\tBCE:70.6543\tKLD:17.8567\tC_loss:0.0000\n",
      "Train Epoch: 75 [25600/60000 (43%)]\tLoss: 90.774643\tBCE:72.8224\tKLD:17.9522\tC_loss:0.0000\n",
      "Train Epoch: 75 [28160/60000 (47%)]\tLoss: 87.998108\tBCE:70.6553\tKLD:17.3428\tC_loss:0.0000\n",
      "Train Epoch: 75 [30720/60000 (51%)]\tLoss: 88.442139\tBCE:71.1874\tKLD:17.2547\tC_loss:0.0000\n",
      "Train Epoch: 75 [33280/60000 (56%)]\tLoss: 88.724854\tBCE:71.3615\tKLD:17.3633\tC_loss:0.0000\n",
      "Train Epoch: 75 [35840/60000 (60%)]\tLoss: 90.548492\tBCE:72.2323\tKLD:18.3162\tC_loss:0.0000\n",
      "Train Epoch: 75 [38400/60000 (64%)]\tLoss: 88.749588\tBCE:71.6611\tKLD:17.0885\tC_loss:0.0000\n",
      "Train Epoch: 75 [40960/60000 (68%)]\tLoss: 87.197815\tBCE:69.8744\tKLD:17.3235\tC_loss:0.0000\n",
      "Train Epoch: 75 [43520/60000 (73%)]\tLoss: 89.052444\tBCE:71.2578\tKLD:17.7947\tC_loss:0.0000\n",
      "Train Epoch: 75 [46080/60000 (77%)]\tLoss: 88.593628\tBCE:71.1572\tKLD:17.4364\tC_loss:0.0000\n",
      "Train Epoch: 75 [48640/60000 (81%)]\tLoss: 89.487274\tBCE:71.7257\tKLD:17.7616\tC_loss:0.0000\n",
      "Train Epoch: 75 [51200/60000 (85%)]\tLoss: 91.086952\tBCE:73.1626\tKLD:17.9244\tC_loss:0.0000\n",
      "Train Epoch: 75 [53760/60000 (90%)]\tLoss: 88.192818\tBCE:70.6489\tKLD:17.5439\tC_loss:0.0000\n",
      "Train Epoch: 75 [56320/60000 (94%)]\tLoss: 89.582672\tBCE:71.9353\tKLD:17.6474\tC_loss:0.0000\n",
      "Train Epoch: 75 [58880/60000 (98%)]\tLoss: 87.517441\tBCE:70.0268\tKLD:17.4906\tC_loss:0.0000\n",
      "====> Epoch: 75 Average loss: 88.9328\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.5145\n",
      "Random number: 6\n",
      "Train Epoch: 76 [0/60000 (0%)]\tLoss: 91.352676\tBCE:72.9327\tKLD:18.4199\tC_loss:0.0000\n",
      "Train Epoch: 76 [2560/60000 (4%)]\tLoss: 89.315468\tBCE:71.9474\tKLD:17.3681\tC_loss:0.0000\n",
      "Train Epoch: 76 [5120/60000 (9%)]\tLoss: 91.582916\tBCE:74.1922\tKLD:17.3907\tC_loss:0.0000\n",
      "Train Epoch: 76 [7680/60000 (13%)]\tLoss: 90.036781\tBCE:72.4924\tKLD:17.5444\tC_loss:0.0000\n",
      "Train Epoch: 76 [10240/60000 (17%)]\tLoss: 90.119957\tBCE:72.4042\tKLD:17.7157\tC_loss:0.0000\n",
      "Train Epoch: 76 [12800/60000 (21%)]\tLoss: 89.286064\tBCE:72.0301\tKLD:17.2560\tC_loss:0.0000\n",
      "Train Epoch: 76 [15360/60000 (26%)]\tLoss: 87.234940\tBCE:69.7792\tKLD:17.4558\tC_loss:0.0000\n",
      "Train Epoch: 76 [17920/60000 (30%)]\tLoss: 89.021385\tBCE:71.5570\tKLD:17.4644\tC_loss:0.0000\n",
      "Train Epoch: 76 [20480/60000 (34%)]\tLoss: 91.170975\tBCE:73.5194\tKLD:17.6516\tC_loss:0.0000\n",
      "Train Epoch: 76 [23040/60000 (38%)]\tLoss: 87.219002\tBCE:69.6072\tKLD:17.6118\tC_loss:0.0000\n",
      "Train Epoch: 76 [25600/60000 (43%)]\tLoss: 89.901917\tBCE:72.2618\tKLD:17.6401\tC_loss:0.0000\n",
      "Train Epoch: 76 [28160/60000 (47%)]\tLoss: 90.207291\tBCE:72.6148\tKLD:17.5925\tC_loss:0.0000\n",
      "Train Epoch: 76 [30720/60000 (51%)]\tLoss: 89.891403\tBCE:72.1941\tKLD:17.6973\tC_loss:0.0000\n",
      "Train Epoch: 76 [33280/60000 (56%)]\tLoss: 90.869141\tBCE:73.3549\tKLD:17.5142\tC_loss:0.0000\n",
      "Train Epoch: 76 [35840/60000 (60%)]\tLoss: 88.392197\tBCE:70.8109\tKLD:17.5813\tC_loss:0.0000\n",
      "Train Epoch: 76 [38400/60000 (64%)]\tLoss: 87.275986\tBCE:70.2512\tKLD:17.0248\tC_loss:0.0000\n",
      "Train Epoch: 76 [40960/60000 (68%)]\tLoss: 91.488190\tBCE:73.4623\tKLD:18.0259\tC_loss:0.0000\n",
      "Train Epoch: 76 [43520/60000 (73%)]\tLoss: 90.050461\tBCE:72.5925\tKLD:17.4580\tC_loss:0.0000\n",
      "Train Epoch: 76 [46080/60000 (77%)]\tLoss: 91.187454\tBCE:73.7212\tKLD:17.4663\tC_loss:0.0000\n",
      "Train Epoch: 76 [48640/60000 (81%)]\tLoss: 87.918274\tBCE:70.3515\tKLD:17.5668\tC_loss:0.0000\n",
      "Train Epoch: 76 [51200/60000 (85%)]\tLoss: 88.896706\tBCE:71.2535\tKLD:17.6432\tC_loss:0.0000\n",
      "Train Epoch: 76 [53760/60000 (90%)]\tLoss: 89.491074\tBCE:71.8534\tKLD:17.6377\tC_loss:0.0000\n",
      "Train Epoch: 76 [56320/60000 (94%)]\tLoss: 87.642509\tBCE:70.3168\tKLD:17.3257\tC_loss:0.0000\n",
      "Train Epoch: 76 [58880/60000 (98%)]\tLoss: 89.253296\tBCE:71.9023\tKLD:17.3510\tC_loss:0.0000\n",
      "====> Epoch: 76 Average loss: 88.9259\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.2063\n",
      "Train Epoch: 77 [0/60000 (0%)]\tLoss: 87.737556\tBCE:70.0622\tKLD:17.6754\tC_loss:0.0000\n",
      "Train Epoch: 77 [2560/60000 (4%)]\tLoss: 89.790764\tBCE:72.0184\tKLD:17.7723\tC_loss:0.0000\n",
      "Train Epoch: 77 [5120/60000 (9%)]\tLoss: 91.267319\tBCE:73.2712\tKLD:17.9961\tC_loss:0.0000\n",
      "Train Epoch: 77 [7680/60000 (13%)]\tLoss: 87.808502\tBCE:70.1483\tKLD:17.6602\tC_loss:0.0000\n",
      "Train Epoch: 77 [10240/60000 (17%)]\tLoss: 88.201340\tBCE:70.8076\tKLD:17.3937\tC_loss:0.0000\n",
      "Train Epoch: 77 [12800/60000 (21%)]\tLoss: 88.906418\tBCE:71.4463\tKLD:17.4601\tC_loss:0.0000\n",
      "Train Epoch: 77 [15360/60000 (26%)]\tLoss: 88.988449\tBCE:71.5085\tKLD:17.4800\tC_loss:0.0000\n",
      "Train Epoch: 77 [17920/60000 (30%)]\tLoss: 91.005829\tBCE:72.9041\tKLD:18.1017\tC_loss:0.0000\n",
      "Train Epoch: 77 [20480/60000 (34%)]\tLoss: 89.146133\tBCE:71.5674\tKLD:17.5787\tC_loss:0.0000\n",
      "Train Epoch: 77 [23040/60000 (38%)]\tLoss: 87.341957\tBCE:70.0316\tKLD:17.3103\tC_loss:0.0000\n",
      "Train Epoch: 77 [25600/60000 (43%)]\tLoss: 86.614632\tBCE:69.4503\tKLD:17.1643\tC_loss:0.0000\n",
      "Train Epoch: 77 [28160/60000 (47%)]\tLoss: 89.679840\tBCE:72.1328\tKLD:17.5470\tC_loss:0.0000\n",
      "Train Epoch: 77 [30720/60000 (51%)]\tLoss: 90.720047\tBCE:73.1771\tKLD:17.5429\tC_loss:0.0000\n",
      "Train Epoch: 77 [33280/60000 (56%)]\tLoss: 90.251938\tBCE:72.7418\tKLD:17.5102\tC_loss:0.0000\n",
      "Train Epoch: 77 [35840/60000 (60%)]\tLoss: 90.096321\tBCE:72.3640\tKLD:17.7323\tC_loss:0.0000\n",
      "Train Epoch: 77 [38400/60000 (64%)]\tLoss: 92.000061\tBCE:73.9722\tKLD:18.0279\tC_loss:0.0000\n",
      "Train Epoch: 77 [40960/60000 (68%)]\tLoss: 89.131012\tBCE:71.6436\tKLD:17.4874\tC_loss:0.0000\n",
      "Train Epoch: 77 [43520/60000 (73%)]\tLoss: 86.389389\tBCE:69.3346\tKLD:17.0547\tC_loss:0.0000\n",
      "Train Epoch: 77 [46080/60000 (77%)]\tLoss: 88.988960\tBCE:71.1340\tKLD:17.8550\tC_loss:0.0000\n",
      "Train Epoch: 77 [48640/60000 (81%)]\tLoss: 86.504105\tBCE:69.3371\tKLD:17.1670\tC_loss:0.0000\n",
      "Train Epoch: 77 [51200/60000 (85%)]\tLoss: 93.839890\tBCE:75.8076\tKLD:18.0323\tC_loss:0.0000\n",
      "Train Epoch: 77 [53760/60000 (90%)]\tLoss: 90.782280\tBCE:72.8137\tKLD:17.9686\tC_loss:0.0000\n",
      "Train Epoch: 77 [56320/60000 (94%)]\tLoss: 89.803497\tBCE:71.9877\tKLD:17.8158\tC_loss:0.0000\n",
      "Train Epoch: 77 [58880/60000 (98%)]\tLoss: 89.137833\tBCE:71.4402\tKLD:17.6977\tC_loss:0.0000\n",
      "====> Epoch: 77 Average loss: 88.9429\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.1539\n",
      "Train Epoch: 78 [0/60000 (0%)]\tLoss: 88.502625\tBCE:70.8846\tKLD:17.6180\tC_loss:0.0000\n",
      "Train Epoch: 78 [2560/60000 (4%)]\tLoss: 86.172440\tBCE:68.9525\tKLD:17.2200\tC_loss:0.0000\n",
      "Train Epoch: 78 [5120/60000 (9%)]\tLoss: 91.240067\tBCE:73.1851\tKLD:18.0550\tC_loss:0.0000\n",
      "Train Epoch: 78 [7680/60000 (13%)]\tLoss: 88.802750\tBCE:71.5787\tKLD:17.2240\tC_loss:0.0000\n",
      "Train Epoch: 78 [10240/60000 (17%)]\tLoss: 87.402115\tBCE:70.1803\tKLD:17.2218\tC_loss:0.0000\n",
      "Train Epoch: 78 [12800/60000 (21%)]\tLoss: 91.300591\tBCE:73.1806\tKLD:18.1200\tC_loss:0.0000\n",
      "Train Epoch: 78 [15360/60000 (26%)]\tLoss: 90.967468\tBCE:73.3300\tKLD:17.6374\tC_loss:0.0000\n",
      "Train Epoch: 78 [17920/60000 (30%)]\tLoss: 89.059662\tBCE:71.1683\tKLD:17.8913\tC_loss:0.0000\n",
      "Train Epoch: 78 [20480/60000 (34%)]\tLoss: 89.300171\tBCE:71.4272\tKLD:17.8730\tC_loss:0.0000\n",
      "Train Epoch: 78 [23040/60000 (38%)]\tLoss: 87.293625\tBCE:69.8940\tKLD:17.3996\tC_loss:0.0000\n",
      "Train Epoch: 78 [25600/60000 (43%)]\tLoss: 88.269966\tBCE:70.6954\tKLD:17.5746\tC_loss:0.0000\n",
      "Train Epoch: 78 [28160/60000 (47%)]\tLoss: 87.287903\tBCE:69.7998\tKLD:17.4881\tC_loss:0.0000\n",
      "Train Epoch: 78 [30720/60000 (51%)]\tLoss: 87.118256\tBCE:69.6192\tKLD:17.4990\tC_loss:0.0000\n",
      "Train Epoch: 78 [33280/60000 (56%)]\tLoss: 89.108414\tBCE:71.7172\tKLD:17.3912\tC_loss:0.0000\n",
      "Train Epoch: 78 [35840/60000 (60%)]\tLoss: 88.940491\tBCE:71.3313\tKLD:17.6092\tC_loss:0.0000\n",
      "Train Epoch: 78 [38400/60000 (64%)]\tLoss: 88.368179\tBCE:70.6030\tKLD:17.7651\tC_loss:0.0000\n",
      "Train Epoch: 78 [40960/60000 (68%)]\tLoss: 92.840302\tBCE:75.3997\tKLD:17.4406\tC_loss:0.0000\n",
      "Train Epoch: 78 [43520/60000 (73%)]\tLoss: 91.572205\tBCE:73.5348\tKLD:18.0374\tC_loss:0.0000\n",
      "Train Epoch: 78 [46080/60000 (77%)]\tLoss: 90.750587\tBCE:72.9241\tKLD:17.8264\tC_loss:0.0000\n",
      "Train Epoch: 78 [48640/60000 (81%)]\tLoss: 88.007828\tBCE:70.7634\tKLD:17.2445\tC_loss:0.0000\n",
      "Train Epoch: 78 [51200/60000 (85%)]\tLoss: 89.719284\tBCE:71.8443\tKLD:17.8750\tC_loss:0.0000\n",
      "Train Epoch: 78 [53760/60000 (90%)]\tLoss: 88.476662\tBCE:70.8169\tKLD:17.6598\tC_loss:0.0000\n",
      "Train Epoch: 78 [56320/60000 (94%)]\tLoss: 88.474564\tBCE:70.9399\tKLD:17.5346\tC_loss:0.0000\n",
      "Train Epoch: 78 [58880/60000 (98%)]\tLoss: 87.059097\tBCE:69.8781\tKLD:17.1810\tC_loss:0.0000\n",
      "====> Epoch: 78 Average loss: 88.8167\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0837\n",
      "Train Epoch: 79 [0/60000 (0%)]\tLoss: 89.107559\tBCE:71.8602\tKLD:17.2473\tC_loss:0.0000\n",
      "Train Epoch: 79 [2560/60000 (4%)]\tLoss: 86.470428\tBCE:68.7650\tKLD:17.7054\tC_loss:0.0000\n",
      "Train Epoch: 79 [5120/60000 (9%)]\tLoss: 89.344063\tBCE:71.8022\tKLD:17.5419\tC_loss:0.0000\n",
      "Train Epoch: 79 [7680/60000 (13%)]\tLoss: 89.668884\tBCE:71.9521\tKLD:17.7168\tC_loss:0.0000\n",
      "Train Epoch: 79 [10240/60000 (17%)]\tLoss: 89.328056\tBCE:71.5838\tKLD:17.7443\tC_loss:0.0000\n",
      "Train Epoch: 79 [12800/60000 (21%)]\tLoss: 90.993080\tBCE:73.2064\tKLD:17.7867\tC_loss:0.0000\n",
      "Train Epoch: 79 [15360/60000 (26%)]\tLoss: 87.636040\tBCE:70.3922\tKLD:17.2438\tC_loss:0.0000\n",
      "Train Epoch: 79 [17920/60000 (30%)]\tLoss: 88.259705\tBCE:70.3244\tKLD:17.9353\tC_loss:0.0000\n",
      "Train Epoch: 79 [20480/60000 (34%)]\tLoss: 88.686493\tBCE:71.2429\tKLD:17.4436\tC_loss:0.0000\n",
      "Train Epoch: 79 [23040/60000 (38%)]\tLoss: 87.524765\tBCE:69.8363\tKLD:17.6885\tC_loss:0.0000\n",
      "Train Epoch: 79 [25600/60000 (43%)]\tLoss: 85.131973\tBCE:68.1614\tKLD:16.9706\tC_loss:0.0000\n",
      "Train Epoch: 79 [28160/60000 (47%)]\tLoss: 89.182388\tBCE:72.0200\tKLD:17.1624\tC_loss:0.0000\n",
      "Train Epoch: 79 [30720/60000 (51%)]\tLoss: 90.668488\tBCE:73.3340\tKLD:17.3345\tC_loss:0.0000\n",
      "Train Epoch: 79 [33280/60000 (56%)]\tLoss: 91.766212\tBCE:74.0413\tKLD:17.7249\tC_loss:0.0000\n",
      "Train Epoch: 79 [35840/60000 (60%)]\tLoss: 91.786804\tBCE:73.5488\tKLD:18.2380\tC_loss:0.0000\n",
      "Train Epoch: 79 [38400/60000 (64%)]\tLoss: 89.178894\tBCE:71.4678\tKLD:17.7111\tC_loss:0.0000\n",
      "Train Epoch: 79 [40960/60000 (68%)]\tLoss: 87.983368\tBCE:71.0377\tKLD:16.9457\tC_loss:0.0000\n",
      "Train Epoch: 79 [43520/60000 (73%)]\tLoss: 92.837692\tBCE:75.1535\tKLD:17.6842\tC_loss:0.0000\n",
      "Train Epoch: 79 [46080/60000 (77%)]\tLoss: 87.329285\tBCE:69.4567\tKLD:17.8726\tC_loss:0.0000\n",
      "Train Epoch: 79 [48640/60000 (81%)]\tLoss: 87.263412\tBCE:70.2133\tKLD:17.0501\tC_loss:0.0000\n",
      "Train Epoch: 79 [51200/60000 (85%)]\tLoss: 87.929092\tBCE:70.5315\tKLD:17.3976\tC_loss:0.0000\n",
      "Train Epoch: 79 [53760/60000 (90%)]\tLoss: 88.929573\tBCE:71.1987\tKLD:17.7309\tC_loss:0.0000\n",
      "Train Epoch: 79 [56320/60000 (94%)]\tLoss: 89.686203\tBCE:72.0568\tKLD:17.6294\tC_loss:0.0000\n",
      "Train Epoch: 79 [58880/60000 (98%)]\tLoss: 89.093048\tBCE:71.5303\tKLD:17.5628\tC_loss:0.0000\n",
      "====> Epoch: 79 Average loss: 88.8594\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0875\n",
      "Train Epoch: 80 [0/60000 (0%)]\tLoss: 90.580658\tBCE:72.6123\tKLD:17.9683\tC_loss:0.0000\n",
      "Train Epoch: 80 [2560/60000 (4%)]\tLoss: 89.952576\tBCE:71.7294\tKLD:18.2231\tC_loss:0.0000\n",
      "Train Epoch: 80 [5120/60000 (9%)]\tLoss: 88.815430\tBCE:71.4459\tKLD:17.3695\tC_loss:0.0000\n",
      "Train Epoch: 80 [7680/60000 (13%)]\tLoss: 85.764153\tBCE:68.0776\tKLD:17.6865\tC_loss:0.0000\n",
      "Train Epoch: 80 [10240/60000 (17%)]\tLoss: 86.734421\tBCE:69.3291\tKLD:17.4053\tC_loss:0.0000\n",
      "Train Epoch: 80 [12800/60000 (21%)]\tLoss: 87.363983\tBCE:69.9830\tKLD:17.3810\tC_loss:0.0000\n",
      "Train Epoch: 80 [15360/60000 (26%)]\tLoss: 88.435516\tBCE:70.9117\tKLD:17.5238\tC_loss:0.0000\n",
      "Train Epoch: 80 [17920/60000 (30%)]\tLoss: 89.548409\tBCE:71.5614\tKLD:17.9870\tC_loss:0.0000\n",
      "Train Epoch: 80 [20480/60000 (34%)]\tLoss: 89.761185\tBCE:71.9485\tKLD:17.8127\tC_loss:0.0000\n",
      "Train Epoch: 80 [23040/60000 (38%)]\tLoss: 87.865417\tBCE:70.4131\tKLD:17.4523\tC_loss:0.0000\n",
      "Train Epoch: 80 [25600/60000 (43%)]\tLoss: 88.867477\tBCE:71.4153\tKLD:17.4522\tC_loss:0.0000\n",
      "Train Epoch: 80 [28160/60000 (47%)]\tLoss: 88.564758\tBCE:70.6036\tKLD:17.9612\tC_loss:0.0000\n",
      "Train Epoch: 80 [30720/60000 (51%)]\tLoss: 87.864380\tBCE:69.7492\tKLD:18.1152\tC_loss:0.0000\n",
      "Train Epoch: 80 [33280/60000 (56%)]\tLoss: 91.124054\tBCE:73.5105\tKLD:17.6136\tC_loss:0.0000\n",
      "Train Epoch: 80 [35840/60000 (60%)]\tLoss: 88.298553\tBCE:70.5238\tKLD:17.7747\tC_loss:0.0000\n",
      "Train Epoch: 80 [38400/60000 (64%)]\tLoss: 90.685425\tBCE:73.1487\tKLD:17.5367\tC_loss:0.0000\n",
      "Train Epoch: 80 [40960/60000 (68%)]\tLoss: 88.463570\tBCE:70.6968\tKLD:17.7668\tC_loss:0.0000\n",
      "Train Epoch: 80 [43520/60000 (73%)]\tLoss: 89.618240\tBCE:71.9813\tKLD:17.6369\tC_loss:0.0000\n",
      "Train Epoch: 80 [46080/60000 (77%)]\tLoss: 89.148987\tBCE:71.2513\tKLD:17.8976\tC_loss:0.0000\n",
      "Train Epoch: 80 [48640/60000 (81%)]\tLoss: 88.829376\tBCE:71.3443\tKLD:17.4851\tC_loss:0.0000\n",
      "Train Epoch: 80 [51200/60000 (85%)]\tLoss: 89.500702\tBCE:71.6790\tKLD:17.8217\tC_loss:0.0000\n",
      "Train Epoch: 80 [53760/60000 (90%)]\tLoss: 88.693558\tBCE:70.9689\tKLD:17.7246\tC_loss:0.0000\n",
      "Train Epoch: 80 [56320/60000 (94%)]\tLoss: 91.387375\tBCE:73.6393\tKLD:17.7480\tC_loss:0.0000\n",
      "Train Epoch: 80 [58880/60000 (98%)]\tLoss: 89.698814\tBCE:71.7821\tKLD:17.9167\tC_loss:0.0000\n",
      "====> Epoch: 80 Average loss: 88.7842\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 90.9250\n",
      "Random number: 6\n",
      "Train Epoch: 81 [0/60000 (0%)]\tLoss: 88.972076\tBCE:71.1007\tKLD:17.8714\tC_loss:0.0000\n",
      "Train Epoch: 81 [2560/60000 (4%)]\tLoss: 89.474083\tBCE:71.3424\tKLD:18.1317\tC_loss:0.0000\n",
      "Train Epoch: 81 [5120/60000 (9%)]\tLoss: 90.841537\tBCE:73.1285\tKLD:17.7130\tC_loss:0.0000\n",
      "Train Epoch: 81 [7680/60000 (13%)]\tLoss: 87.032074\tBCE:69.9306\tKLD:17.1015\tC_loss:0.0000\n",
      "Train Epoch: 81 [10240/60000 (17%)]\tLoss: 92.292297\tBCE:74.0590\tKLD:18.2333\tC_loss:0.0000\n",
      "Train Epoch: 81 [12800/60000 (21%)]\tLoss: 89.183357\tBCE:71.5292\tKLD:17.6542\tC_loss:0.0000\n",
      "Train Epoch: 81 [15360/60000 (26%)]\tLoss: 88.764854\tBCE:70.5598\tKLD:18.2051\tC_loss:0.0000\n",
      "Train Epoch: 81 [17920/60000 (30%)]\tLoss: 89.143127\tBCE:71.9622\tKLD:17.1809\tC_loss:0.0000\n",
      "Train Epoch: 81 [20480/60000 (34%)]\tLoss: 88.179016\tBCE:70.7034\tKLD:17.4756\tC_loss:0.0000\n",
      "Train Epoch: 81 [23040/60000 (38%)]\tLoss: 89.140793\tBCE:71.4349\tKLD:17.7058\tC_loss:0.0000\n",
      "Train Epoch: 81 [25600/60000 (43%)]\tLoss: 89.742622\tBCE:71.5020\tKLD:18.2406\tC_loss:0.0000\n",
      "Train Epoch: 81 [28160/60000 (47%)]\tLoss: 89.458443\tBCE:71.9334\tKLD:17.5251\tC_loss:0.0000\n",
      "Train Epoch: 81 [30720/60000 (51%)]\tLoss: 90.041275\tBCE:72.6407\tKLD:17.4006\tC_loss:0.0000\n",
      "Train Epoch: 81 [33280/60000 (56%)]\tLoss: 89.203949\tBCE:71.4953\tKLD:17.7087\tC_loss:0.0000\n",
      "Train Epoch: 81 [35840/60000 (60%)]\tLoss: 88.869644\tBCE:71.5573\tKLD:17.3123\tC_loss:0.0000\n",
      "Train Epoch: 81 [38400/60000 (64%)]\tLoss: 88.194275\tBCE:70.9811\tKLD:17.2131\tC_loss:0.0000\n",
      "Train Epoch: 81 [40960/60000 (68%)]\tLoss: 89.506775\tBCE:71.8230\tKLD:17.6838\tC_loss:0.0000\n",
      "Train Epoch: 81 [43520/60000 (73%)]\tLoss: 89.478432\tBCE:71.4497\tKLD:18.0287\tC_loss:0.0000\n",
      "Train Epoch: 81 [46080/60000 (77%)]\tLoss: 91.134514\tBCE:73.5655\tKLD:17.5690\tC_loss:0.0000\n",
      "Train Epoch: 81 [48640/60000 (81%)]\tLoss: 86.868172\tBCE:69.5012\tKLD:17.3670\tC_loss:0.0000\n",
      "Train Epoch: 81 [51200/60000 (85%)]\tLoss: 88.519745\tBCE:71.1045\tKLD:17.4152\tC_loss:0.0000\n",
      "Train Epoch: 81 [53760/60000 (90%)]\tLoss: 87.707748\tBCE:70.2990\tKLD:17.4088\tC_loss:0.0000\n",
      "Train Epoch: 81 [56320/60000 (94%)]\tLoss: 86.050293\tBCE:69.1246\tKLD:16.9257\tC_loss:0.0000\n",
      "Train Epoch: 81 [58880/60000 (98%)]\tLoss: 90.873749\tBCE:73.1248\tKLD:17.7489\tC_loss:0.0000\n",
      "====> Epoch: 81 Average loss: 88.8369\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0400\n",
      "Train Epoch: 82 [0/60000 (0%)]\tLoss: 88.173492\tBCE:70.8703\tKLD:17.3032\tC_loss:0.0000\n",
      "Train Epoch: 82 [2560/60000 (4%)]\tLoss: 87.464775\tBCE:70.0714\tKLD:17.3934\tC_loss:0.0000\n",
      "Train Epoch: 82 [5120/60000 (9%)]\tLoss: 90.132309\tBCE:72.3329\tKLD:17.7994\tC_loss:0.0000\n",
      "Train Epoch: 82 [7680/60000 (13%)]\tLoss: 89.510040\tBCE:71.8553\tKLD:17.6548\tC_loss:0.0000\n",
      "Train Epoch: 82 [10240/60000 (17%)]\tLoss: 86.517426\tBCE:68.6978\tKLD:17.8196\tC_loss:0.0000\n",
      "Train Epoch: 82 [12800/60000 (21%)]\tLoss: 85.245476\tBCE:68.0837\tKLD:17.1617\tC_loss:0.0000\n",
      "Train Epoch: 82 [15360/60000 (26%)]\tLoss: 87.970085\tBCE:70.7696\tKLD:17.2005\tC_loss:0.0000\n",
      "Train Epoch: 82 [17920/60000 (30%)]\tLoss: 86.556602\tBCE:69.2228\tKLD:17.3338\tC_loss:0.0000\n",
      "Train Epoch: 82 [20480/60000 (34%)]\tLoss: 88.715347\tBCE:70.8118\tKLD:17.9035\tC_loss:0.0001\n",
      "Train Epoch: 82 [23040/60000 (38%)]\tLoss: 91.676498\tBCE:73.7064\tKLD:17.9701\tC_loss:0.0000\n",
      "Train Epoch: 82 [25600/60000 (43%)]\tLoss: 87.580627\tBCE:70.3539\tKLD:17.2268\tC_loss:0.0000\n",
      "Train Epoch: 82 [28160/60000 (47%)]\tLoss: 87.843094\tBCE:70.1700\tKLD:17.6731\tC_loss:0.0000\n",
      "Train Epoch: 82 [30720/60000 (51%)]\tLoss: 88.200470\tBCE:70.6170\tKLD:17.5835\tC_loss:0.0000\n",
      "Train Epoch: 82 [33280/60000 (56%)]\tLoss: 89.118866\tBCE:71.5731\tKLD:17.5458\tC_loss:0.0000\n",
      "Train Epoch: 82 [35840/60000 (60%)]\tLoss: 86.883385\tBCE:69.3400\tKLD:17.5434\tC_loss:0.0000\n",
      "Train Epoch: 82 [38400/60000 (64%)]\tLoss: 87.459641\tBCE:70.4493\tKLD:17.0103\tC_loss:0.0000\n",
      "Train Epoch: 82 [40960/60000 (68%)]\tLoss: 88.546539\tBCE:70.9675\tKLD:17.5791\tC_loss:0.0000\n",
      "Train Epoch: 82 [43520/60000 (73%)]\tLoss: 86.235779\tBCE:68.7774\tKLD:17.4583\tC_loss:0.0000\n",
      "Train Epoch: 82 [46080/60000 (77%)]\tLoss: 90.738594\tBCE:72.7117\tKLD:18.0268\tC_loss:0.0000\n",
      "Train Epoch: 82 [48640/60000 (81%)]\tLoss: 90.734268\tBCE:72.7617\tKLD:17.9726\tC_loss:0.0000\n",
      "Train Epoch: 82 [51200/60000 (85%)]\tLoss: 90.226578\tBCE:72.7067\tKLD:17.5199\tC_loss:0.0000\n",
      "Train Epoch: 82 [53760/60000 (90%)]\tLoss: 87.824898\tBCE:70.4942\tKLD:17.3307\tC_loss:0.0000\n",
      "Train Epoch: 82 [56320/60000 (94%)]\tLoss: 86.550499\tBCE:69.5088\tKLD:17.0417\tC_loss:0.0000\n",
      "Train Epoch: 82 [58880/60000 (98%)]\tLoss: 90.415482\tBCE:72.0082\tKLD:18.4073\tC_loss:0.0000\n",
      "====> Epoch: 82 Average loss: 88.6739\tClassifier Accuracy: 99.5593\n",
      "====> Test set loss: 91.2993\n",
      "Train Epoch: 83 [0/60000 (0%)]\tLoss: 89.400757\tBCE:71.7075\tKLD:17.6933\tC_loss:0.0000\n",
      "Train Epoch: 83 [2560/60000 (4%)]\tLoss: 88.366798\tBCE:70.8523\tKLD:17.5144\tC_loss:0.0000\n",
      "Train Epoch: 83 [5120/60000 (9%)]\tLoss: 90.667999\tBCE:72.6853\tKLD:17.9827\tC_loss:0.0000\n",
      "Train Epoch: 83 [7680/60000 (13%)]\tLoss: 90.544922\tBCE:72.6826\tKLD:17.8624\tC_loss:0.0000\n",
      "Train Epoch: 83 [10240/60000 (17%)]\tLoss: 86.252327\tBCE:68.7230\tKLD:17.5294\tC_loss:0.0000\n",
      "Train Epoch: 83 [12800/60000 (21%)]\tLoss: 87.663437\tBCE:70.2528\tKLD:17.4107\tC_loss:0.0000\n",
      "Train Epoch: 83 [15360/60000 (26%)]\tLoss: 88.430038\tBCE:70.9840\tKLD:17.4461\tC_loss:0.0000\n",
      "Train Epoch: 83 [17920/60000 (30%)]\tLoss: 88.723877\tBCE:71.5912\tKLD:17.1326\tC_loss:0.0000\n",
      "Train Epoch: 83 [20480/60000 (34%)]\tLoss: 88.914169\tBCE:70.9846\tKLD:17.9296\tC_loss:0.0000\n",
      "Train Epoch: 83 [23040/60000 (38%)]\tLoss: 90.806625\tBCE:72.9153\tKLD:17.8914\tC_loss:0.0000\n",
      "Train Epoch: 83 [25600/60000 (43%)]\tLoss: 87.596741\tBCE:69.5765\tKLD:18.0202\tC_loss:0.0000\n",
      "Train Epoch: 83 [28160/60000 (47%)]\tLoss: 89.237701\tBCE:71.6353\tKLD:17.6024\tC_loss:0.0000\n",
      "Train Epoch: 83 [30720/60000 (51%)]\tLoss: 90.980217\tBCE:73.2107\tKLD:17.7695\tC_loss:0.0000\n",
      "Train Epoch: 83 [33280/60000 (56%)]\tLoss: 89.420349\tBCE:71.8701\tKLD:17.5502\tC_loss:0.0000\n",
      "Train Epoch: 83 [35840/60000 (60%)]\tLoss: 89.836464\tBCE:72.2842\tKLD:17.5523\tC_loss:0.0000\n",
      "Train Epoch: 83 [38400/60000 (64%)]\tLoss: 86.671112\tBCE:69.6637\tKLD:17.0074\tC_loss:0.0000\n",
      "Train Epoch: 83 [40960/60000 (68%)]\tLoss: 91.041580\tBCE:72.7270\tKLD:18.3146\tC_loss:0.0000\n",
      "Train Epoch: 83 [43520/60000 (73%)]\tLoss: 88.826233\tBCE:71.6791\tKLD:17.1472\tC_loss:0.0000\n",
      "Train Epoch: 83 [46080/60000 (77%)]\tLoss: 89.269966\tBCE:71.5488\tKLD:17.7212\tC_loss:0.0000\n",
      "Train Epoch: 83 [48640/60000 (81%)]\tLoss: 88.416847\tBCE:70.9011\tKLD:17.5158\tC_loss:0.0000\n",
      "Train Epoch: 83 [51200/60000 (85%)]\tLoss: 88.672073\tBCE:71.1920\tKLD:17.4800\tC_loss:0.0000\n",
      "Train Epoch: 83 [53760/60000 (90%)]\tLoss: 89.051689\tBCE:71.7391\tKLD:17.3126\tC_loss:0.0000\n",
      "Train Epoch: 83 [56320/60000 (94%)]\tLoss: 89.985054\tBCE:72.3797\tKLD:17.6053\tC_loss:0.0000\n",
      "Train Epoch: 83 [58880/60000 (98%)]\tLoss: 88.718246\tBCE:71.1764\tKLD:17.5418\tC_loss:0.0000\n",
      "====> Epoch: 83 Average loss: 88.7710\tClassifier Accuracy: 99.9432\n",
      "====> Test set loss: 90.8583\n",
      "Train Epoch: 84 [0/60000 (0%)]\tLoss: 88.349594\tBCE:70.5997\tKLD:17.7499\tC_loss:0.0000\n",
      "Train Epoch: 84 [2560/60000 (4%)]\tLoss: 89.061256\tBCE:71.4258\tKLD:17.6355\tC_loss:0.0000\n",
      "Train Epoch: 84 [5120/60000 (9%)]\tLoss: 88.323364\tBCE:70.2103\tKLD:18.1131\tC_loss:0.0000\n",
      "Train Epoch: 84 [7680/60000 (13%)]\tLoss: 87.414604\tBCE:69.4586\tKLD:17.9560\tC_loss:0.0000\n",
      "Train Epoch: 84 [10240/60000 (17%)]\tLoss: 89.206589\tBCE:71.7956\tKLD:17.4110\tC_loss:0.0000\n",
      "Train Epoch: 84 [12800/60000 (21%)]\tLoss: 89.935074\tBCE:72.2619\tKLD:17.6732\tC_loss:0.0000\n",
      "Train Epoch: 84 [15360/60000 (26%)]\tLoss: 88.510696\tBCE:70.8850\tKLD:17.6257\tC_loss:0.0000\n",
      "Train Epoch: 84 [17920/60000 (30%)]\tLoss: 91.172653\tBCE:73.0125\tKLD:18.1602\tC_loss:0.0000\n",
      "Train Epoch: 84 [20480/60000 (34%)]\tLoss: 88.371918\tBCE:70.7798\tKLD:17.5921\tC_loss:0.0000\n",
      "Train Epoch: 84 [23040/60000 (38%)]\tLoss: 88.714157\tBCE:71.1411\tKLD:17.5730\tC_loss:0.0000\n",
      "Train Epoch: 84 [25600/60000 (43%)]\tLoss: 86.960739\tBCE:69.2167\tKLD:17.7441\tC_loss:0.0000\n",
      "Train Epoch: 84 [28160/60000 (47%)]\tLoss: 90.597260\tBCE:72.8305\tKLD:17.7667\tC_loss:0.0000\n",
      "Train Epoch: 84 [30720/60000 (51%)]\tLoss: 89.524673\tBCE:71.6468\tKLD:17.8778\tC_loss:0.0000\n",
      "Train Epoch: 84 [33280/60000 (56%)]\tLoss: 89.434898\tBCE:71.8320\tKLD:17.6029\tC_loss:0.0000\n",
      "Train Epoch: 84 [35840/60000 (60%)]\tLoss: 87.132545\tBCE:69.6416\tKLD:17.4910\tC_loss:0.0000\n",
      "Train Epoch: 84 [38400/60000 (64%)]\tLoss: 89.279037\tBCE:71.5725\tKLD:17.7065\tC_loss:0.0000\n",
      "Train Epoch: 84 [40960/60000 (68%)]\tLoss: 87.967262\tBCE:70.5214\tKLD:17.4459\tC_loss:0.0000\n",
      "Train Epoch: 84 [43520/60000 (73%)]\tLoss: 90.794357\tBCE:72.8484\tKLD:17.9460\tC_loss:0.0000\n",
      "Train Epoch: 84 [46080/60000 (77%)]\tLoss: 89.666695\tBCE:72.0439\tKLD:17.6228\tC_loss:0.0000\n",
      "Train Epoch: 84 [48640/60000 (81%)]\tLoss: 87.376854\tBCE:69.7891\tKLD:17.5877\tC_loss:0.0000\n",
      "Train Epoch: 84 [51200/60000 (85%)]\tLoss: 89.753342\tBCE:71.9168\tKLD:17.8366\tC_loss:0.0000\n",
      "Train Epoch: 84 [53760/60000 (90%)]\tLoss: 93.411674\tBCE:75.2968\tKLD:18.1148\tC_loss:0.0000\n",
      "Train Epoch: 84 [56320/60000 (94%)]\tLoss: 90.978897\tBCE:72.9676\tKLD:18.0113\tC_loss:0.0000\n",
      "Train Epoch: 84 [58880/60000 (98%)]\tLoss: 89.447319\tBCE:71.5425\tKLD:17.9048\tC_loss:0.0000\n",
      "====> Epoch: 84 Average loss: 88.5950\tClassifier Accuracy: 99.9983\n",
      "====> Test set loss: 91.0663\n",
      "Train Epoch: 85 [0/60000 (0%)]\tLoss: 88.736084\tBCE:71.3065\tKLD:17.4296\tC_loss:0.0000\n",
      "Train Epoch: 85 [2560/60000 (4%)]\tLoss: 86.587799\tBCE:68.6981\tKLD:17.8897\tC_loss:0.0000\n",
      "Train Epoch: 85 [5120/60000 (9%)]\tLoss: 89.904724\tBCE:72.4879\tKLD:17.4168\tC_loss:0.0000\n",
      "Train Epoch: 85 [7680/60000 (13%)]\tLoss: 89.641747\tBCE:72.1072\tKLD:17.5346\tC_loss:0.0000\n",
      "Train Epoch: 85 [10240/60000 (17%)]\tLoss: 89.609673\tBCE:71.8017\tKLD:17.8079\tC_loss:0.0000\n",
      "Train Epoch: 85 [12800/60000 (21%)]\tLoss: 89.926277\tBCE:72.1046\tKLD:17.8217\tC_loss:0.0000\n",
      "Train Epoch: 85 [15360/60000 (26%)]\tLoss: 90.250603\tBCE:72.5806\tKLD:17.6700\tC_loss:0.0000\n",
      "Train Epoch: 85 [17920/60000 (30%)]\tLoss: 92.261894\tBCE:73.8078\tKLD:18.4541\tC_loss:0.0000\n",
      "Train Epoch: 85 [20480/60000 (34%)]\tLoss: 88.131187\tBCE:70.5176\tKLD:17.6136\tC_loss:0.0000\n",
      "Train Epoch: 85 [23040/60000 (38%)]\tLoss: 88.525970\tBCE:70.9105\tKLD:17.6155\tC_loss:0.0000\n",
      "Train Epoch: 85 [25600/60000 (43%)]\tLoss: 88.375298\tBCE:70.6543\tKLD:17.7210\tC_loss:0.0000\n",
      "Train Epoch: 85 [28160/60000 (47%)]\tLoss: 87.805733\tBCE:70.1566\tKLD:17.6492\tC_loss:0.0000\n",
      "Train Epoch: 85 [30720/60000 (51%)]\tLoss: 89.183815\tBCE:71.4096\tKLD:17.7742\tC_loss:0.0000\n",
      "Train Epoch: 85 [33280/60000 (56%)]\tLoss: 87.524147\tBCE:69.8510\tKLD:17.6731\tC_loss:0.0000\n",
      "Train Epoch: 85 [35840/60000 (60%)]\tLoss: 89.138962\tBCE:71.0882\tKLD:18.0507\tC_loss:0.0000\n",
      "Train Epoch: 85 [38400/60000 (64%)]\tLoss: 88.303864\tBCE:70.7135\tKLD:17.5904\tC_loss:0.0000\n",
      "Train Epoch: 85 [40960/60000 (68%)]\tLoss: 90.224380\tBCE:72.3888\tKLD:17.8356\tC_loss:0.0000\n",
      "Train Epoch: 85 [43520/60000 (73%)]\tLoss: 89.008362\tBCE:71.0290\tKLD:17.9794\tC_loss:0.0000\n",
      "Train Epoch: 85 [46080/60000 (77%)]\tLoss: 90.138618\tBCE:72.2771\tKLD:17.8615\tC_loss:0.0000\n",
      "Train Epoch: 85 [48640/60000 (81%)]\tLoss: 86.311913\tBCE:69.2169\tKLD:17.0950\tC_loss:0.0000\n",
      "Train Epoch: 85 [51200/60000 (85%)]\tLoss: 88.194572\tBCE:70.8598\tKLD:17.3348\tC_loss:0.0000\n",
      "Train Epoch: 85 [53760/60000 (90%)]\tLoss: 88.762100\tBCE:71.1006\tKLD:17.6615\tC_loss:0.0000\n",
      "Train Epoch: 85 [56320/60000 (94%)]\tLoss: 89.712944\tBCE:72.0256\tKLD:17.6874\tC_loss:0.0000\n",
      "Train Epoch: 85 [58880/60000 (98%)]\tLoss: 87.507431\tBCE:69.6585\tKLD:17.8490\tC_loss:0.0000\n",
      "====> Epoch: 85 Average loss: 88.6100\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.1529\n",
      "Random number: 3\n",
      "Train Epoch: 86 [0/60000 (0%)]\tLoss: 87.626381\tBCE:69.6342\tKLD:17.9922\tC_loss:0.0000\n",
      "Train Epoch: 86 [2560/60000 (4%)]\tLoss: 87.525208\tBCE:69.8584\tKLD:17.6668\tC_loss:0.0000\n",
      "Train Epoch: 86 [5120/60000 (9%)]\tLoss: 87.029091\tBCE:69.4160\tKLD:17.6131\tC_loss:0.0000\n",
      "Train Epoch: 86 [7680/60000 (13%)]\tLoss: 92.703033\tBCE:74.4173\tKLD:18.2857\tC_loss:0.0000\n",
      "Train Epoch: 86 [10240/60000 (17%)]\tLoss: 87.938309\tBCE:70.0885\tKLD:17.8498\tC_loss:0.0000\n",
      "Train Epoch: 86 [12800/60000 (21%)]\tLoss: 89.376793\tBCE:72.0837\tKLD:17.2931\tC_loss:0.0000\n",
      "Train Epoch: 86 [15360/60000 (26%)]\tLoss: 90.747528\tBCE:73.2072\tKLD:17.5404\tC_loss:0.0000\n",
      "Train Epoch: 86 [17920/60000 (30%)]\tLoss: 90.014442\tBCE:72.2549\tKLD:17.7595\tC_loss:0.0000\n",
      "Train Epoch: 86 [20480/60000 (34%)]\tLoss: 85.832474\tBCE:68.5810\tKLD:17.2514\tC_loss:0.0000\n",
      "Train Epoch: 86 [23040/60000 (38%)]\tLoss: 90.411621\tBCE:72.6885\tKLD:17.7231\tC_loss:0.0000\n",
      "Train Epoch: 86 [25600/60000 (43%)]\tLoss: 90.874924\tBCE:72.6686\tKLD:18.2064\tC_loss:0.0000\n",
      "Train Epoch: 86 [28160/60000 (47%)]\tLoss: 90.675110\tBCE:72.7830\tKLD:17.8921\tC_loss:0.0000\n",
      "Train Epoch: 86 [30720/60000 (51%)]\tLoss: 87.901993\tBCE:70.4534\tKLD:17.4486\tC_loss:0.0000\n",
      "Train Epoch: 86 [33280/60000 (56%)]\tLoss: 87.692917\tBCE:70.8177\tKLD:16.8752\tC_loss:0.0000\n",
      "Train Epoch: 86 [35840/60000 (60%)]\tLoss: 90.189217\tBCE:71.8056\tKLD:18.3836\tC_loss:0.0000\n",
      "Train Epoch: 86 [38400/60000 (64%)]\tLoss: 88.737366\tBCE:71.3167\tKLD:17.4207\tC_loss:0.0000\n",
      "Train Epoch: 86 [40960/60000 (68%)]\tLoss: 89.110779\tBCE:71.7833\tKLD:17.3275\tC_loss:0.0000\n",
      "Train Epoch: 86 [43520/60000 (73%)]\tLoss: 87.215416\tBCE:69.3879\tKLD:17.8276\tC_loss:0.0000\n",
      "Train Epoch: 86 [46080/60000 (77%)]\tLoss: 89.224495\tBCE:71.4874\tKLD:17.7371\tC_loss:0.0000\n",
      "Train Epoch: 86 [48640/60000 (81%)]\tLoss: 90.947815\tBCE:73.0540\tKLD:17.8938\tC_loss:0.0000\n",
      "Train Epoch: 86 [51200/60000 (85%)]\tLoss: 88.089813\tBCE:70.5907\tKLD:17.4991\tC_loss:0.0000\n",
      "Train Epoch: 86 [53760/60000 (90%)]\tLoss: 90.621956\tBCE:72.7600\tKLD:17.8620\tC_loss:0.0000\n",
      "Train Epoch: 86 [56320/60000 (94%)]\tLoss: 90.026489\tBCE:72.1723\tKLD:17.8542\tC_loss:0.0000\n",
      "Train Epoch: 86 [58880/60000 (98%)]\tLoss: 86.950333\tBCE:69.6349\tKLD:17.3155\tC_loss:0.0000\n",
      "====> Epoch: 86 Average loss: 88.6346\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.9353\n",
      "Train Epoch: 87 [0/60000 (0%)]\tLoss: 87.485535\tBCE:69.9416\tKLD:17.5440\tC_loss:0.0000\n",
      "Train Epoch: 87 [2560/60000 (4%)]\tLoss: 88.500984\tBCE:70.2713\tKLD:18.2297\tC_loss:0.0000\n",
      "Train Epoch: 87 [5120/60000 (9%)]\tLoss: 85.471146\tBCE:68.3773\tKLD:17.0939\tC_loss:0.0000\n",
      "Train Epoch: 87 [7680/60000 (13%)]\tLoss: 90.160225\tBCE:72.1165\tKLD:18.0437\tC_loss:0.0000\n",
      "Train Epoch: 87 [10240/60000 (17%)]\tLoss: 88.419724\tBCE:70.8953\tKLD:17.5245\tC_loss:0.0000\n",
      "Train Epoch: 87 [12800/60000 (21%)]\tLoss: 90.546028\tBCE:72.5629\tKLD:17.9831\tC_loss:0.0000\n",
      "Train Epoch: 87 [15360/60000 (26%)]\tLoss: 87.335663\tBCE:69.8122\tKLD:17.5234\tC_loss:0.0000\n",
      "Train Epoch: 87 [17920/60000 (30%)]\tLoss: 88.741928\tBCE:71.4385\tKLD:17.3034\tC_loss:0.0000\n",
      "Train Epoch: 87 [20480/60000 (34%)]\tLoss: 88.470360\tBCE:70.8710\tKLD:17.5993\tC_loss:0.0000\n",
      "Train Epoch: 87 [23040/60000 (38%)]\tLoss: 92.149429\tBCE:73.9523\tKLD:18.1971\tC_loss:0.0000\n",
      "Train Epoch: 87 [25600/60000 (43%)]\tLoss: 89.058449\tBCE:71.2471\tKLD:17.8114\tC_loss:0.0000\n",
      "Train Epoch: 87 [28160/60000 (47%)]\tLoss: 88.977066\tBCE:70.9688\tKLD:18.0082\tC_loss:0.0000\n",
      "Train Epoch: 87 [30720/60000 (51%)]\tLoss: 86.497162\tBCE:69.1103\tKLD:17.3868\tC_loss:0.0000\n",
      "Train Epoch: 87 [33280/60000 (56%)]\tLoss: 89.590927\tBCE:72.1575\tKLD:17.4334\tC_loss:0.0000\n",
      "Train Epoch: 87 [35840/60000 (60%)]\tLoss: 86.554329\tBCE:69.0514\tKLD:17.5029\tC_loss:0.0000\n",
      "Train Epoch: 87 [38400/60000 (64%)]\tLoss: 87.955338\tBCE:70.2145\tKLD:17.7409\tC_loss:0.0000\n",
      "Train Epoch: 87 [40960/60000 (68%)]\tLoss: 88.102722\tBCE:70.3786\tKLD:17.7242\tC_loss:0.0000\n",
      "Train Epoch: 87 [43520/60000 (73%)]\tLoss: 86.922386\tBCE:69.6324\tKLD:17.2900\tC_loss:0.0000\n",
      "Train Epoch: 87 [46080/60000 (77%)]\tLoss: 88.500122\tBCE:70.9687\tKLD:17.5314\tC_loss:0.0000\n",
      "Train Epoch: 87 [48640/60000 (81%)]\tLoss: 87.205826\tBCE:69.6879\tKLD:17.5179\tC_loss:0.0000\n",
      "Train Epoch: 87 [51200/60000 (85%)]\tLoss: 87.765900\tBCE:71.0748\tKLD:16.6911\tC_loss:0.0000\n",
      "Train Epoch: 87 [53760/60000 (90%)]\tLoss: 91.140228\tBCE:72.5972\tKLD:18.5430\tC_loss:0.0000\n",
      "Train Epoch: 87 [56320/60000 (94%)]\tLoss: 88.276367\tBCE:70.8716\tKLD:17.4048\tC_loss:0.0000\n",
      "Train Epoch: 87 [58880/60000 (98%)]\tLoss: 90.703323\tBCE:72.7809\tKLD:17.9224\tC_loss:0.0000\n",
      "====> Epoch: 87 Average loss: 88.4872\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0290\n",
      "Train Epoch: 88 [0/60000 (0%)]\tLoss: 90.464172\tBCE:72.6705\tKLD:17.7937\tC_loss:0.0000\n",
      "Train Epoch: 88 [2560/60000 (4%)]\tLoss: 87.715378\tBCE:69.3280\tKLD:18.3874\tC_loss:0.0000\n",
      "Train Epoch: 88 [5120/60000 (9%)]\tLoss: 87.687912\tBCE:69.9987\tKLD:17.6892\tC_loss:0.0000\n",
      "Train Epoch: 88 [7680/60000 (13%)]\tLoss: 89.631332\tBCE:72.0823\tKLD:17.5490\tC_loss:0.0000\n",
      "Train Epoch: 88 [10240/60000 (17%)]\tLoss: 88.806686\tBCE:71.1510\tKLD:17.6557\tC_loss:0.0000\n",
      "Train Epoch: 88 [12800/60000 (21%)]\tLoss: 87.557808\tBCE:69.6699\tKLD:17.8879\tC_loss:0.0000\n",
      "Train Epoch: 88 [15360/60000 (26%)]\tLoss: 90.828011\tBCE:72.7201\tKLD:18.1080\tC_loss:0.0000\n",
      "Train Epoch: 88 [17920/60000 (30%)]\tLoss: 88.666199\tBCE:70.9083\tKLD:17.7579\tC_loss:0.0000\n",
      "Train Epoch: 88 [20480/60000 (34%)]\tLoss: 89.952606\tBCE:72.0868\tKLD:17.8658\tC_loss:0.0000\n",
      "Train Epoch: 88 [23040/60000 (38%)]\tLoss: 87.845261\tBCE:70.1751\tKLD:17.6702\tC_loss:0.0000\n",
      "Train Epoch: 88 [25600/60000 (43%)]\tLoss: 88.552582\tBCE:71.1235\tKLD:17.4291\tC_loss:0.0000\n",
      "Train Epoch: 88 [28160/60000 (47%)]\tLoss: 88.432968\tBCE:70.5709\tKLD:17.8621\tC_loss:0.0000\n",
      "Train Epoch: 88 [30720/60000 (51%)]\tLoss: 86.675217\tBCE:69.7199\tKLD:16.9554\tC_loss:0.0000\n",
      "Train Epoch: 88 [33280/60000 (56%)]\tLoss: 87.771927\tBCE:70.2837\tKLD:17.4883\tC_loss:0.0000\n",
      "Train Epoch: 88 [35840/60000 (60%)]\tLoss: 90.853447\tBCE:73.2802\tKLD:17.5732\tC_loss:0.0000\n",
      "Train Epoch: 88 [38400/60000 (64%)]\tLoss: 88.501907\tBCE:70.7136\tKLD:17.7883\tC_loss:0.0000\n",
      "Train Epoch: 88 [40960/60000 (68%)]\tLoss: 88.989624\tBCE:71.3431\tKLD:17.6465\tC_loss:0.0000\n",
      "Train Epoch: 88 [43520/60000 (73%)]\tLoss: 90.701675\tBCE:72.5001\tKLD:18.2016\tC_loss:0.0000\n",
      "Train Epoch: 88 [46080/60000 (77%)]\tLoss: 85.841286\tBCE:68.9943\tKLD:16.8469\tC_loss:0.0000\n",
      "Train Epoch: 88 [48640/60000 (81%)]\tLoss: 89.107758\tBCE:71.5537\tKLD:17.5541\tC_loss:0.0000\n",
      "Train Epoch: 88 [51200/60000 (85%)]\tLoss: 88.682533\tBCE:70.5509\tKLD:18.1316\tC_loss:0.0000\n",
      "Train Epoch: 88 [53760/60000 (90%)]\tLoss: 90.632973\tBCE:72.5578\tKLD:18.0752\tC_loss:0.0000\n",
      "Train Epoch: 88 [56320/60000 (94%)]\tLoss: 87.831543\tBCE:70.1152\tKLD:17.7163\tC_loss:0.0000\n",
      "Train Epoch: 88 [58880/60000 (98%)]\tLoss: 87.878754\tBCE:70.5738\tKLD:17.3050\tC_loss:0.0000\n",
      "====> Epoch: 88 Average loss: 88.4365\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.1621\n",
      "Train Epoch: 89 [0/60000 (0%)]\tLoss: 92.242996\tBCE:74.4801\tKLD:17.7629\tC_loss:0.0000\n",
      "Train Epoch: 89 [2560/60000 (4%)]\tLoss: 89.071152\tBCE:71.4154\tKLD:17.6558\tC_loss:0.0000\n",
      "Train Epoch: 89 [5120/60000 (9%)]\tLoss: 87.078903\tBCE:69.7623\tKLD:17.3167\tC_loss:0.0000\n",
      "Train Epoch: 89 [7680/60000 (13%)]\tLoss: 87.583435\tBCE:70.4200\tKLD:17.1634\tC_loss:0.0000\n",
      "Train Epoch: 89 [10240/60000 (17%)]\tLoss: 88.023132\tBCE:70.4936\tKLD:17.5296\tC_loss:0.0000\n",
      "Train Epoch: 89 [12800/60000 (21%)]\tLoss: 88.331322\tBCE:71.0900\tKLD:17.2414\tC_loss:0.0000\n",
      "Train Epoch: 89 [15360/60000 (26%)]\tLoss: 89.106766\tBCE:71.3529\tKLD:17.7539\tC_loss:0.0000\n",
      "Train Epoch: 89 [17920/60000 (30%)]\tLoss: 87.551300\tBCE:69.5094\tKLD:18.0419\tC_loss:0.0000\n",
      "Train Epoch: 89 [20480/60000 (34%)]\tLoss: 88.664368\tBCE:70.7031\tKLD:17.9612\tC_loss:0.0000\n",
      "Train Epoch: 89 [23040/60000 (38%)]\tLoss: 86.721344\tBCE:69.0728\tKLD:17.6485\tC_loss:0.0000\n",
      "Train Epoch: 89 [25600/60000 (43%)]\tLoss: 88.814865\tBCE:71.2483\tKLD:17.5666\tC_loss:0.0000\n",
      "Train Epoch: 89 [28160/60000 (47%)]\tLoss: 88.502869\tBCE:70.8606\tKLD:17.6422\tC_loss:0.0000\n",
      "Train Epoch: 89 [30720/60000 (51%)]\tLoss: 89.353592\tBCE:71.5235\tKLD:17.8301\tC_loss:0.0000\n",
      "Train Epoch: 89 [33280/60000 (56%)]\tLoss: 89.463448\tBCE:71.2995\tKLD:18.1639\tC_loss:0.0000\n",
      "Train Epoch: 89 [35840/60000 (60%)]\tLoss: 87.327148\tBCE:69.6955\tKLD:17.6316\tC_loss:0.0000\n",
      "Train Epoch: 89 [38400/60000 (64%)]\tLoss: 86.391281\tBCE:69.4172\tKLD:16.9740\tC_loss:0.0000\n",
      "Train Epoch: 89 [40960/60000 (68%)]\tLoss: 90.064499\tBCE:72.6074\tKLD:17.4571\tC_loss:0.0000\n",
      "Train Epoch: 89 [43520/60000 (73%)]\tLoss: 87.267113\tBCE:69.5061\tKLD:17.7611\tC_loss:0.0000\n",
      "Train Epoch: 89 [46080/60000 (77%)]\tLoss: 90.737579\tBCE:72.7129\tKLD:18.0247\tC_loss:0.0000\n",
      "Train Epoch: 89 [48640/60000 (81%)]\tLoss: 88.966660\tBCE:70.9026\tKLD:18.0641\tC_loss:0.0000\n",
      "Train Epoch: 89 [51200/60000 (85%)]\tLoss: 88.535912\tBCE:70.4328\tKLD:18.1031\tC_loss:0.0000\n",
      "Train Epoch: 89 [53760/60000 (90%)]\tLoss: 87.278145\tBCE:70.2040\tKLD:17.0742\tC_loss:0.0000\n",
      "Train Epoch: 89 [56320/60000 (94%)]\tLoss: 89.161377\tBCE:71.4957\tKLD:17.6657\tC_loss:0.0000\n",
      "Train Epoch: 89 [58880/60000 (98%)]\tLoss: 89.064316\tBCE:70.9710\tKLD:18.0933\tC_loss:0.0000\n",
      "====> Epoch: 89 Average loss: 88.4457\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.9572\n",
      "Train Epoch: 90 [0/60000 (0%)]\tLoss: 86.733574\tBCE:69.6908\tKLD:17.0427\tC_loss:0.0000\n",
      "Train Epoch: 90 [2560/60000 (4%)]\tLoss: 87.480423\tBCE:69.8139\tKLD:17.6666\tC_loss:0.0000\n",
      "Train Epoch: 90 [5120/60000 (9%)]\tLoss: 88.312439\tBCE:71.0315\tKLD:17.2809\tC_loss:0.0000\n",
      "Train Epoch: 90 [7680/60000 (13%)]\tLoss: 88.647675\tBCE:70.8452\tKLD:17.8024\tC_loss:0.0000\n",
      "Train Epoch: 90 [10240/60000 (17%)]\tLoss: 88.600540\tBCE:71.0789\tKLD:17.5216\tC_loss:0.0000\n",
      "Train Epoch: 90 [12800/60000 (21%)]\tLoss: 88.465744\tBCE:70.9256\tKLD:17.5402\tC_loss:0.0000\n",
      "Train Epoch: 90 [15360/60000 (26%)]\tLoss: 89.620781\tBCE:71.7872\tKLD:17.8336\tC_loss:0.0000\n",
      "Train Epoch: 90 [17920/60000 (30%)]\tLoss: 86.503281\tBCE:69.0725\tKLD:17.4308\tC_loss:0.0000\n",
      "Train Epoch: 90 [20480/60000 (34%)]\tLoss: 90.889648\tBCE:72.7738\tKLD:18.1159\tC_loss:0.0000\n",
      "Train Epoch: 90 [23040/60000 (38%)]\tLoss: 85.641548\tBCE:68.0784\tKLD:17.5632\tC_loss:0.0000\n",
      "Train Epoch: 90 [25600/60000 (43%)]\tLoss: 90.239037\tBCE:72.5404\tKLD:17.6987\tC_loss:0.0000\n",
      "Train Epoch: 90 [28160/60000 (47%)]\tLoss: 91.504997\tBCE:73.4606\tKLD:18.0444\tC_loss:0.0000\n",
      "Train Epoch: 90 [30720/60000 (51%)]\tLoss: 85.826546\tBCE:68.6004\tKLD:17.2261\tC_loss:0.0000\n",
      "Train Epoch: 90 [33280/60000 (56%)]\tLoss: 88.298935\tBCE:70.8144\tKLD:17.4845\tC_loss:0.0000\n",
      "Train Epoch: 90 [35840/60000 (60%)]\tLoss: 87.340157\tBCE:69.4034\tKLD:17.9367\tC_loss:0.0000\n",
      "Train Epoch: 90 [38400/60000 (64%)]\tLoss: 91.707062\tBCE:73.8617\tKLD:17.8454\tC_loss:0.0000\n",
      "Train Epoch: 90 [40960/60000 (68%)]\tLoss: 88.189156\tBCE:70.4634\tKLD:17.7258\tC_loss:0.0000\n",
      "Train Epoch: 90 [43520/60000 (73%)]\tLoss: 87.572632\tBCE:70.0004\tKLD:17.5723\tC_loss:0.0000\n",
      "Train Epoch: 90 [46080/60000 (77%)]\tLoss: 87.569405\tBCE:70.2274\tKLD:17.3420\tC_loss:0.0000\n",
      "Train Epoch: 90 [48640/60000 (81%)]\tLoss: 87.510063\tBCE:70.1289\tKLD:17.3812\tC_loss:0.0000\n",
      "Train Epoch: 90 [51200/60000 (85%)]\tLoss: 87.896683\tBCE:70.2115\tKLD:17.6852\tC_loss:0.0000\n",
      "Train Epoch: 90 [53760/60000 (90%)]\tLoss: 89.692780\tBCE:71.9259\tKLD:17.7668\tC_loss:0.0000\n",
      "Train Epoch: 90 [56320/60000 (94%)]\tLoss: 88.385338\tBCE:70.7706\tKLD:17.6147\tC_loss:0.0000\n",
      "Train Epoch: 90 [58880/60000 (98%)]\tLoss: 90.073036\tBCE:72.1026\tKLD:17.9704\tC_loss:0.0000\n",
      "====> Epoch: 90 Average loss: 88.5124\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.5238\n",
      "Random number: 7\n",
      "Train Epoch: 91 [0/60000 (0%)]\tLoss: 89.162453\tBCE:71.3507\tKLD:17.8117\tC_loss:0.0000\n",
      "Train Epoch: 91 [2560/60000 (4%)]\tLoss: 88.531151\tBCE:70.6963\tKLD:17.8349\tC_loss:0.0000\n",
      "Train Epoch: 91 [5120/60000 (9%)]\tLoss: 90.268448\tBCE:72.5851\tKLD:17.6833\tC_loss:0.0000\n",
      "Train Epoch: 91 [7680/60000 (13%)]\tLoss: 86.891342\tBCE:69.2248\tKLD:17.6666\tC_loss:0.0000\n",
      "Train Epoch: 91 [10240/60000 (17%)]\tLoss: 87.032013\tBCE:69.5653\tKLD:17.4667\tC_loss:0.0000\n",
      "Train Epoch: 91 [12800/60000 (21%)]\tLoss: 85.754333\tBCE:68.7789\tKLD:16.9754\tC_loss:0.0000\n",
      "Train Epoch: 91 [15360/60000 (26%)]\tLoss: 86.866745\tBCE:69.7703\tKLD:17.0965\tC_loss:0.0000\n",
      "Train Epoch: 91 [17920/60000 (30%)]\tLoss: 89.875504\tBCE:71.9534\tKLD:17.9221\tC_loss:0.0000\n",
      "Train Epoch: 91 [20480/60000 (34%)]\tLoss: 89.856880\tBCE:71.4533\tKLD:18.4036\tC_loss:0.0000\n",
      "Train Epoch: 91 [23040/60000 (38%)]\tLoss: 84.753067\tBCE:67.7809\tKLD:16.9721\tC_loss:0.0000\n",
      "Train Epoch: 91 [25600/60000 (43%)]\tLoss: 89.766846\tBCE:72.0838\tKLD:17.6830\tC_loss:0.0000\n",
      "Train Epoch: 91 [28160/60000 (47%)]\tLoss: 89.042290\tBCE:71.7566\tKLD:17.2857\tC_loss:0.0000\n",
      "Train Epoch: 91 [30720/60000 (51%)]\tLoss: 90.650459\tBCE:72.9303\tKLD:17.7201\tC_loss:0.0000\n",
      "Train Epoch: 91 [33280/60000 (56%)]\tLoss: 86.555283\tBCE:69.2669\tKLD:17.2884\tC_loss:0.0000\n",
      "Train Epoch: 91 [35840/60000 (60%)]\tLoss: 87.523575\tBCE:69.8008\tKLD:17.7228\tC_loss:0.0000\n",
      "Train Epoch: 91 [38400/60000 (64%)]\tLoss: 90.313019\tBCE:72.5582\tKLD:17.7548\tC_loss:0.0000\n",
      "Train Epoch: 91 [40960/60000 (68%)]\tLoss: 87.166458\tBCE:69.3115\tKLD:17.8550\tC_loss:0.0000\n",
      "Train Epoch: 91 [43520/60000 (73%)]\tLoss: 88.490257\tBCE:70.6339\tKLD:17.8564\tC_loss:0.0000\n",
      "Train Epoch: 91 [46080/60000 (77%)]\tLoss: 88.908752\tBCE:71.1287\tKLD:17.7800\tC_loss:0.0000\n",
      "Train Epoch: 91 [48640/60000 (81%)]\tLoss: 86.666473\tBCE:69.6980\tKLD:16.9684\tC_loss:0.0000\n",
      "Train Epoch: 91 [51200/60000 (85%)]\tLoss: 88.863434\tBCE:71.2085\tKLD:17.6549\tC_loss:0.0000\n",
      "Train Epoch: 91 [53760/60000 (90%)]\tLoss: 90.013115\tBCE:71.9090\tKLD:18.1041\tC_loss:0.0000\n",
      "Train Epoch: 91 [56320/60000 (94%)]\tLoss: 89.285583\tBCE:71.3724\tKLD:17.9132\tC_loss:0.0000\n",
      "Train Epoch: 91 [58880/60000 (98%)]\tLoss: 87.399124\tBCE:70.0636\tKLD:17.3355\tC_loss:0.0000\n",
      "====> Epoch: 91 Average loss: 88.4839\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.1416\n",
      "Train Epoch: 92 [0/60000 (0%)]\tLoss: 88.839394\tBCE:71.5832\tKLD:17.2561\tC_loss:0.0000\n",
      "Train Epoch: 92 [2560/60000 (4%)]\tLoss: 89.527100\tBCE:71.3436\tKLD:18.1835\tC_loss:0.0000\n",
      "Train Epoch: 92 [5120/60000 (9%)]\tLoss: 88.824081\tBCE:71.5377\tKLD:17.2863\tC_loss:0.0000\n",
      "Train Epoch: 92 [7680/60000 (13%)]\tLoss: 88.538513\tBCE:70.7889\tKLD:17.7496\tC_loss:0.0000\n",
      "Train Epoch: 92 [10240/60000 (17%)]\tLoss: 90.231407\tBCE:71.8675\tKLD:18.3639\tC_loss:0.0000\n",
      "Train Epoch: 92 [12800/60000 (21%)]\tLoss: 86.484543\tBCE:68.9807\tKLD:17.5038\tC_loss:0.0000\n",
      "Train Epoch: 92 [15360/60000 (26%)]\tLoss: 89.663300\tBCE:71.7863\tKLD:17.8770\tC_loss:0.0000\n",
      "Train Epoch: 92 [17920/60000 (30%)]\tLoss: 89.090912\tBCE:71.6559\tKLD:17.4350\tC_loss:0.0000\n",
      "Train Epoch: 92 [20480/60000 (34%)]\tLoss: 90.668396\tBCE:72.5558\tKLD:18.1126\tC_loss:0.0000\n",
      "Train Epoch: 92 [23040/60000 (38%)]\tLoss: 86.756302\tBCE:69.3638\tKLD:17.3925\tC_loss:0.0000\n",
      "Train Epoch: 92 [25600/60000 (43%)]\tLoss: 88.991486\tBCE:71.6976\tKLD:17.2939\tC_loss:0.0000\n",
      "Train Epoch: 92 [28160/60000 (47%)]\tLoss: 88.747749\tBCE:71.0220\tKLD:17.7257\tC_loss:0.0000\n",
      "Train Epoch: 92 [30720/60000 (51%)]\tLoss: 88.305176\tBCE:70.5627\tKLD:17.7425\tC_loss:0.0000\n",
      "Train Epoch: 92 [33280/60000 (56%)]\tLoss: 87.946701\tBCE:70.5755\tKLD:17.3712\tC_loss:0.0000\n",
      "Train Epoch: 92 [35840/60000 (60%)]\tLoss: 88.911873\tBCE:71.2393\tKLD:17.6725\tC_loss:0.0000\n",
      "Train Epoch: 92 [38400/60000 (64%)]\tLoss: 88.374413\tBCE:70.6342\tKLD:17.7402\tC_loss:0.0000\n",
      "Train Epoch: 92 [40960/60000 (68%)]\tLoss: 88.558830\tBCE:71.1939\tKLD:17.3649\tC_loss:0.0000\n",
      "Train Epoch: 92 [43520/60000 (73%)]\tLoss: 88.848312\tBCE:71.0800\tKLD:17.7683\tC_loss:0.0000\n",
      "Train Epoch: 92 [46080/60000 (77%)]\tLoss: 86.542084\tBCE:69.4140\tKLD:17.1280\tC_loss:0.0000\n",
      "Train Epoch: 92 [48640/60000 (81%)]\tLoss: 87.543320\tBCE:70.0986\tKLD:17.4448\tC_loss:0.0000\n",
      "Train Epoch: 92 [51200/60000 (85%)]\tLoss: 92.296783\tBCE:74.0735\tKLD:18.2233\tC_loss:0.0000\n",
      "Train Epoch: 92 [53760/60000 (90%)]\tLoss: 88.716080\tBCE:71.1374\tKLD:17.5787\tC_loss:0.0000\n",
      "Train Epoch: 92 [56320/60000 (94%)]\tLoss: 88.318542\tBCE:70.4019\tKLD:17.9166\tC_loss:0.0000\n",
      "Train Epoch: 92 [58880/60000 (98%)]\tLoss: 86.293228\tBCE:69.2811\tKLD:17.0121\tC_loss:0.0000\n",
      "====> Epoch: 92 Average loss: 88.4688\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.8690\n",
      "Train Epoch: 93 [0/60000 (0%)]\tLoss: 90.176849\tBCE:72.1673\tKLD:18.0096\tC_loss:0.0000\n",
      "Train Epoch: 93 [2560/60000 (4%)]\tLoss: 88.555511\tBCE:70.5812\tKLD:17.9743\tC_loss:0.0000\n",
      "Train Epoch: 93 [5120/60000 (9%)]\tLoss: 89.764107\tBCE:71.8922\tKLD:17.8719\tC_loss:0.0000\n",
      "Train Epoch: 93 [7680/60000 (13%)]\tLoss: 87.727539\tBCE:70.5043\tKLD:17.2232\tC_loss:0.0000\n",
      "Train Epoch: 93 [10240/60000 (17%)]\tLoss: 88.858559\tBCE:71.3169\tKLD:17.5416\tC_loss:0.0000\n",
      "Train Epoch: 93 [12800/60000 (21%)]\tLoss: 88.490166\tBCE:70.7638\tKLD:17.7263\tC_loss:0.0000\n",
      "Train Epoch: 93 [15360/60000 (26%)]\tLoss: 89.891815\tBCE:71.8075\tKLD:18.0843\tC_loss:0.0000\n",
      "Train Epoch: 93 [17920/60000 (30%)]\tLoss: 91.763199\tBCE:73.6834\tKLD:18.0798\tC_loss:0.0000\n",
      "Train Epoch: 93 [20480/60000 (34%)]\tLoss: 88.402908\tBCE:71.0064\tKLD:17.3965\tC_loss:0.0000\n",
      "Train Epoch: 93 [23040/60000 (38%)]\tLoss: 85.027451\tBCE:67.7925\tKLD:17.2349\tC_loss:0.0000\n",
      "Train Epoch: 93 [25600/60000 (43%)]\tLoss: 87.739784\tBCE:70.3459\tKLD:17.3939\tC_loss:0.0000\n",
      "Train Epoch: 93 [28160/60000 (47%)]\tLoss: 88.592163\tBCE:70.8054\tKLD:17.7868\tC_loss:0.0000\n",
      "Train Epoch: 93 [30720/60000 (51%)]\tLoss: 86.536682\tBCE:69.0659\tKLD:17.4708\tC_loss:0.0000\n",
      "Train Epoch: 93 [33280/60000 (56%)]\tLoss: 85.959839\tBCE:68.8253\tKLD:17.1346\tC_loss:0.0000\n",
      "Train Epoch: 93 [35840/60000 (60%)]\tLoss: 87.394028\tBCE:69.6501\tKLD:17.7439\tC_loss:0.0000\n",
      "Train Epoch: 93 [38400/60000 (64%)]\tLoss: 87.388977\tBCE:69.7433\tKLD:17.6457\tC_loss:0.0000\n",
      "Train Epoch: 93 [40960/60000 (68%)]\tLoss: 87.047867\tBCE:69.7851\tKLD:17.2627\tC_loss:0.0000\n",
      "Train Epoch: 93 [43520/60000 (73%)]\tLoss: 90.298302\tBCE:72.1538\tKLD:18.1445\tC_loss:0.0000\n",
      "Train Epoch: 93 [46080/60000 (77%)]\tLoss: 88.022614\tBCE:70.5916\tKLD:17.4310\tC_loss:0.0000\n",
      "Train Epoch: 93 [48640/60000 (81%)]\tLoss: 87.118347\tBCE:69.6329\tKLD:17.4854\tC_loss:0.0000\n",
      "Train Epoch: 93 [51200/60000 (85%)]\tLoss: 87.688271\tBCE:70.3509\tKLD:17.3374\tC_loss:0.0000\n",
      "Train Epoch: 93 [53760/60000 (90%)]\tLoss: 85.070175\tBCE:67.9694\tKLD:17.1008\tC_loss:0.0000\n",
      "Train Epoch: 93 [56320/60000 (94%)]\tLoss: 91.863121\tBCE:73.4274\tKLD:18.4358\tC_loss:0.0000\n",
      "Train Epoch: 93 [58880/60000 (98%)]\tLoss: 89.261131\tBCE:71.6276\tKLD:17.6335\tC_loss:0.0000\n",
      "====> Epoch: 93 Average loss: 88.3263\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.2955\n",
      "Train Epoch: 94 [0/60000 (0%)]\tLoss: 88.197800\tBCE:70.2084\tKLD:17.9894\tC_loss:0.0000\n",
      "Train Epoch: 94 [2560/60000 (4%)]\tLoss: 88.935593\tBCE:71.1168\tKLD:17.8188\tC_loss:0.0000\n",
      "Train Epoch: 94 [5120/60000 (9%)]\tLoss: 88.975235\tBCE:71.4022\tKLD:17.5730\tC_loss:0.0000\n",
      "Train Epoch: 94 [7680/60000 (13%)]\tLoss: 87.393494\tBCE:69.0781\tKLD:18.3154\tC_loss:0.0000\n",
      "Train Epoch: 94 [10240/60000 (17%)]\tLoss: 86.534752\tBCE:68.8841\tKLD:17.6507\tC_loss:0.0000\n",
      "Train Epoch: 94 [12800/60000 (21%)]\tLoss: 86.602341\tBCE:69.2997\tKLD:17.3027\tC_loss:0.0000\n",
      "Train Epoch: 94 [15360/60000 (26%)]\tLoss: 90.133629\tBCE:72.1766\tKLD:17.9570\tC_loss:0.0000\n",
      "Train Epoch: 94 [17920/60000 (30%)]\tLoss: 87.205330\tBCE:70.0202\tKLD:17.1851\tC_loss:0.0000\n",
      "Train Epoch: 94 [20480/60000 (34%)]\tLoss: 87.171539\tBCE:69.6159\tKLD:17.5556\tC_loss:0.0000\n",
      "Train Epoch: 94 [23040/60000 (38%)]\tLoss: 88.558258\tBCE:70.1920\tKLD:18.3663\tC_loss:0.0000\n",
      "Train Epoch: 94 [25600/60000 (43%)]\tLoss: 90.061600\tBCE:72.5756\tKLD:17.4860\tC_loss:0.0000\n",
      "Train Epoch: 94 [28160/60000 (47%)]\tLoss: 87.766479\tBCE:69.9430\tKLD:17.8235\tC_loss:0.0000\n",
      "Train Epoch: 94 [30720/60000 (51%)]\tLoss: 86.809364\tBCE:69.4381\tKLD:17.3712\tC_loss:0.0000\n",
      "Train Epoch: 94 [33280/60000 (56%)]\tLoss: 88.277069\tBCE:70.9052\tKLD:17.3718\tC_loss:0.0000\n",
      "Train Epoch: 94 [35840/60000 (60%)]\tLoss: 90.318901\tBCE:72.0733\tKLD:18.2456\tC_loss:0.0000\n",
      "Train Epoch: 94 [38400/60000 (64%)]\tLoss: 88.363022\tBCE:71.0697\tKLD:17.2933\tC_loss:0.0000\n",
      "Train Epoch: 94 [40960/60000 (68%)]\tLoss: 85.637283\tBCE:68.1349\tKLD:17.5024\tC_loss:0.0000\n",
      "Train Epoch: 94 [43520/60000 (73%)]\tLoss: 87.888054\tBCE:70.4549\tKLD:17.4331\tC_loss:0.0000\n",
      "Train Epoch: 94 [46080/60000 (77%)]\tLoss: 91.012596\tBCE:72.4996\tKLD:18.5129\tC_loss:0.0000\n",
      "Train Epoch: 94 [48640/60000 (81%)]\tLoss: 87.858673\tBCE:70.3272\tKLD:17.5315\tC_loss:0.0000\n",
      "Train Epoch: 94 [51200/60000 (85%)]\tLoss: 90.186592\tBCE:72.2674\tKLD:17.9192\tC_loss:0.0000\n",
      "Train Epoch: 94 [53760/60000 (90%)]\tLoss: 89.988823\tBCE:72.3267\tKLD:17.6621\tC_loss:0.0000\n",
      "Train Epoch: 94 [56320/60000 (94%)]\tLoss: 88.990814\tBCE:71.3718\tKLD:17.6190\tC_loss:0.0000\n",
      "Train Epoch: 94 [58880/60000 (98%)]\tLoss: 88.415504\tBCE:71.0686\tKLD:17.3469\tC_loss:0.0000\n",
      "====> Epoch: 94 Average loss: 88.4048\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0259\n",
      "Train Epoch: 95 [0/60000 (0%)]\tLoss: 89.158195\tBCE:71.4804\tKLD:17.6778\tC_loss:0.0000\n",
      "Train Epoch: 95 [2560/60000 (4%)]\tLoss: 89.868256\tBCE:71.9200\tKLD:17.9483\tC_loss:0.0000\n",
      "Train Epoch: 95 [5120/60000 (9%)]\tLoss: 90.563942\tBCE:72.3236\tKLD:18.2403\tC_loss:0.0000\n",
      "Train Epoch: 95 [7680/60000 (13%)]\tLoss: 87.719490\tBCE:70.2294\tKLD:17.4901\tC_loss:0.0000\n",
      "Train Epoch: 95 [10240/60000 (17%)]\tLoss: 88.748993\tBCE:71.6119\tKLD:17.1371\tC_loss:0.0000\n",
      "Train Epoch: 95 [12800/60000 (21%)]\tLoss: 87.857437\tBCE:70.3201\tKLD:17.5374\tC_loss:0.0000\n",
      "Train Epoch: 95 [15360/60000 (26%)]\tLoss: 87.999115\tBCE:70.0281\tKLD:17.9710\tC_loss:0.0000\n",
      "Train Epoch: 95 [17920/60000 (30%)]\tLoss: 88.440445\tBCE:70.4891\tKLD:17.9514\tC_loss:0.0000\n",
      "Train Epoch: 95 [20480/60000 (34%)]\tLoss: 88.336723\tBCE:70.4670\tKLD:17.8698\tC_loss:0.0000\n",
      "Train Epoch: 95 [23040/60000 (38%)]\tLoss: 87.408112\tBCE:69.4407\tKLD:17.9674\tC_loss:0.0000\n",
      "Train Epoch: 95 [25600/60000 (43%)]\tLoss: 86.155556\tBCE:68.9313\tKLD:17.2243\tC_loss:0.0000\n",
      "Train Epoch: 95 [28160/60000 (47%)]\tLoss: 86.412796\tBCE:69.4312\tKLD:16.9816\tC_loss:0.0000\n",
      "Train Epoch: 95 [30720/60000 (51%)]\tLoss: 88.565468\tBCE:70.6398\tKLD:17.9257\tC_loss:0.0000\n",
      "Train Epoch: 95 [33280/60000 (56%)]\tLoss: 86.979973\tBCE:69.4449\tKLD:17.5351\tC_loss:0.0000\n",
      "Train Epoch: 95 [35840/60000 (60%)]\tLoss: 91.005829\tBCE:72.8903\tKLD:18.1156\tC_loss:0.0000\n",
      "Train Epoch: 95 [38400/60000 (64%)]\tLoss: 87.831841\tBCE:69.9935\tKLD:17.8383\tC_loss:0.0000\n",
      "Train Epoch: 95 [40960/60000 (68%)]\tLoss: 89.286285\tBCE:71.1693\tKLD:18.1169\tC_loss:0.0000\n",
      "Train Epoch: 95 [43520/60000 (73%)]\tLoss: 90.011353\tBCE:72.0224\tKLD:17.9890\tC_loss:0.0000\n",
      "Train Epoch: 95 [46080/60000 (77%)]\tLoss: 86.065346\tBCE:68.6778\tKLD:17.3875\tC_loss:0.0000\n",
      "Train Epoch: 95 [48640/60000 (81%)]\tLoss: 89.223572\tBCE:71.4199\tKLD:17.8037\tC_loss:0.0000\n",
      "Train Epoch: 95 [51200/60000 (85%)]\tLoss: 88.764732\tBCE:70.7812\tKLD:17.9835\tC_loss:0.0000\n",
      "Train Epoch: 95 [53760/60000 (90%)]\tLoss: 89.087456\tBCE:71.2947\tKLD:17.7928\tC_loss:0.0000\n",
      "Train Epoch: 95 [56320/60000 (94%)]\tLoss: 87.117828\tBCE:69.9343\tKLD:17.1835\tC_loss:0.0000\n",
      "Train Epoch: 95 [58880/60000 (98%)]\tLoss: 89.915367\tBCE:71.7866\tKLD:18.1288\tC_loss:0.0000\n",
      "====> Epoch: 95 Average loss: 88.3224\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.0941\n",
      "Random number: 3\n",
      "Train Epoch: 96 [0/60000 (0%)]\tLoss: 87.963997\tBCE:70.2000\tKLD:17.7640\tC_loss:0.0000\n",
      "Train Epoch: 96 [2560/60000 (4%)]\tLoss: 91.450081\tBCE:73.1439\tKLD:18.3062\tC_loss:0.0000\n",
      "Train Epoch: 96 [5120/60000 (9%)]\tLoss: 87.824982\tBCE:70.6838\tKLD:17.1412\tC_loss:0.0000\n",
      "Train Epoch: 96 [7680/60000 (13%)]\tLoss: 90.526855\tBCE:72.8670\tKLD:17.6599\tC_loss:0.0000\n",
      "Train Epoch: 96 [10240/60000 (17%)]\tLoss: 86.674942\tBCE:69.5306\tKLD:17.1444\tC_loss:0.0000\n",
      "Train Epoch: 96 [12800/60000 (21%)]\tLoss: 85.379303\tBCE:67.7519\tKLD:17.6274\tC_loss:0.0000\n",
      "Train Epoch: 96 [15360/60000 (26%)]\tLoss: 86.576698\tBCE:68.7100\tKLD:17.8667\tC_loss:0.0000\n",
      "Train Epoch: 96 [17920/60000 (30%)]\tLoss: 89.247055\tBCE:71.4597\tKLD:17.7874\tC_loss:0.0000\n",
      "Train Epoch: 96 [20480/60000 (34%)]\tLoss: 88.115417\tBCE:70.5048\tKLD:17.6106\tC_loss:0.0000\n",
      "Train Epoch: 96 [23040/60000 (38%)]\tLoss: 89.802368\tBCE:72.2937\tKLD:17.5086\tC_loss:0.0000\n",
      "Train Epoch: 96 [25600/60000 (43%)]\tLoss: 89.399315\tBCE:71.6521\tKLD:17.7472\tC_loss:0.0000\n",
      "Train Epoch: 96 [28160/60000 (47%)]\tLoss: 91.554901\tBCE:73.7705\tKLD:17.7844\tC_loss:0.0000\n",
      "Train Epoch: 96 [30720/60000 (51%)]\tLoss: 86.576027\tBCE:69.2188\tKLD:17.3572\tC_loss:0.0000\n",
      "Train Epoch: 96 [33280/60000 (56%)]\tLoss: 89.459961\tBCE:71.7442\tKLD:17.7157\tC_loss:0.0000\n",
      "Train Epoch: 96 [35840/60000 (60%)]\tLoss: 88.955948\tBCE:71.1819\tKLD:17.7741\tC_loss:0.0000\n",
      "Train Epoch: 96 [38400/60000 (64%)]\tLoss: 87.797409\tBCE:70.4245\tKLD:17.3729\tC_loss:0.0000\n",
      "Train Epoch: 96 [40960/60000 (68%)]\tLoss: 88.643661\tBCE:70.7904\tKLD:17.8533\tC_loss:0.0000\n",
      "Train Epoch: 96 [43520/60000 (73%)]\tLoss: 87.955826\tBCE:70.6192\tKLD:17.3366\tC_loss:0.0000\n",
      "Train Epoch: 96 [46080/60000 (77%)]\tLoss: 88.815125\tBCE:71.0400\tKLD:17.7751\tC_loss:0.0000\n",
      "Train Epoch: 96 [48640/60000 (81%)]\tLoss: 88.865036\tBCE:71.2471\tKLD:17.6180\tC_loss:0.0000\n",
      "Train Epoch: 96 [51200/60000 (85%)]\tLoss: 89.277420\tBCE:71.6580\tKLD:17.6195\tC_loss:0.0000\n",
      "Train Epoch: 96 [53760/60000 (90%)]\tLoss: 87.834106\tBCE:69.8446\tKLD:17.9895\tC_loss:0.0000\n",
      "Train Epoch: 96 [56320/60000 (94%)]\tLoss: 87.314476\tBCE:70.1966\tKLD:17.1179\tC_loss:0.0000\n",
      "Train Epoch: 96 [58880/60000 (98%)]\tLoss: 88.811234\tBCE:71.0091\tKLD:17.8021\tC_loss:0.0000\n",
      "====> Epoch: 96 Average loss: 88.3232\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.9876\n",
      "Train Epoch: 97 [0/60000 (0%)]\tLoss: 90.263466\tBCE:72.2731\tKLD:17.9903\tC_loss:0.0000\n",
      "Train Epoch: 97 [2560/60000 (4%)]\tLoss: 87.374084\tBCE:70.0347\tKLD:17.3394\tC_loss:0.0000\n",
      "Train Epoch: 97 [5120/60000 (9%)]\tLoss: 86.974266\tBCE:69.5499\tKLD:17.4243\tC_loss:0.0000\n",
      "Train Epoch: 97 [7680/60000 (13%)]\tLoss: 90.942001\tBCE:73.0091\tKLD:17.9329\tC_loss:0.0000\n",
      "Train Epoch: 97 [10240/60000 (17%)]\tLoss: 88.058746\tBCE:70.5725\tKLD:17.4863\tC_loss:0.0000\n",
      "Train Epoch: 97 [12800/60000 (21%)]\tLoss: 87.464180\tBCE:70.0316\tKLD:17.4326\tC_loss:0.0000\n",
      "Train Epoch: 97 [15360/60000 (26%)]\tLoss: 86.865379\tBCE:69.2695\tKLD:17.5959\tC_loss:0.0000\n",
      "Train Epoch: 97 [17920/60000 (30%)]\tLoss: 88.182396\tBCE:70.5662\tKLD:17.6162\tC_loss:0.0000\n",
      "Train Epoch: 97 [20480/60000 (34%)]\tLoss: 86.717247\tBCE:69.3951\tKLD:17.3222\tC_loss:0.0000\n",
      "Train Epoch: 97 [23040/60000 (38%)]\tLoss: 90.413651\tBCE:72.6660\tKLD:17.7476\tC_loss:0.0000\n",
      "Train Epoch: 97 [25600/60000 (43%)]\tLoss: 88.924255\tBCE:70.6277\tKLD:18.2965\tC_loss:0.0000\n",
      "Train Epoch: 97 [28160/60000 (47%)]\tLoss: 88.510223\tBCE:70.4233\tKLD:18.0869\tC_loss:0.0000\n",
      "Train Epoch: 97 [30720/60000 (51%)]\tLoss: 89.571068\tBCE:71.9227\tKLD:17.6484\tC_loss:0.0000\n",
      "Train Epoch: 97 [33280/60000 (56%)]\tLoss: 86.918266\tBCE:69.3083\tKLD:17.6100\tC_loss:0.0000\n",
      "Train Epoch: 97 [35840/60000 (60%)]\tLoss: 90.457008\tBCE:72.1028\tKLD:18.3542\tC_loss:0.0000\n",
      "Train Epoch: 97 [38400/60000 (64%)]\tLoss: 88.062592\tBCE:70.1017\tKLD:17.9609\tC_loss:0.0000\n",
      "Train Epoch: 97 [40960/60000 (68%)]\tLoss: 89.123917\tBCE:71.8406\tKLD:17.2833\tC_loss:0.0000\n",
      "Train Epoch: 97 [43520/60000 (73%)]\tLoss: 86.702835\tBCE:69.0192\tKLD:17.6836\tC_loss:0.0000\n",
      "Train Epoch: 97 [46080/60000 (77%)]\tLoss: 89.232475\tBCE:71.2779\tKLD:17.9546\tC_loss:0.0000\n",
      "Train Epoch: 97 [48640/60000 (81%)]\tLoss: 86.412155\tBCE:69.3098\tKLD:17.1023\tC_loss:0.0000\n",
      "Train Epoch: 97 [51200/60000 (85%)]\tLoss: 89.291115\tBCE:71.6999\tKLD:17.5912\tC_loss:0.0000\n",
      "Train Epoch: 97 [53760/60000 (90%)]\tLoss: 86.977531\tBCE:69.6597\tKLD:17.3178\tC_loss:0.0000\n",
      "Train Epoch: 97 [56320/60000 (94%)]\tLoss: 88.379791\tBCE:70.6077\tKLD:17.7721\tC_loss:0.0000\n",
      "Train Epoch: 97 [58880/60000 (98%)]\tLoss: 89.076920\tBCE:71.0452\tKLD:18.0317\tC_loss:0.0000\n",
      "====> Epoch: 97 Average loss: 88.2152\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0080\n",
      "Train Epoch: 98 [0/60000 (0%)]\tLoss: 88.902550\tBCE:71.4694\tKLD:17.4331\tC_loss:0.0000\n",
      "Train Epoch: 98 [2560/60000 (4%)]\tLoss: 86.793884\tBCE:69.0880\tKLD:17.7058\tC_loss:0.0000\n",
      "Train Epoch: 98 [5120/60000 (9%)]\tLoss: 86.044937\tBCE:68.8754\tKLD:17.1696\tC_loss:0.0000\n",
      "Train Epoch: 98 [7680/60000 (13%)]\tLoss: 86.887711\tBCE:69.3416\tKLD:17.5461\tC_loss:0.0000\n",
      "Train Epoch: 98 [10240/60000 (17%)]\tLoss: 87.846939\tBCE:70.2418\tKLD:17.6051\tC_loss:0.0000\n",
      "Train Epoch: 98 [12800/60000 (21%)]\tLoss: 89.808426\tBCE:72.1807\tKLD:17.6277\tC_loss:0.0000\n",
      "Train Epoch: 98 [15360/60000 (26%)]\tLoss: 88.691681\tBCE:70.7952\tKLD:17.8965\tC_loss:0.0000\n",
      "Train Epoch: 98 [17920/60000 (30%)]\tLoss: 89.998634\tBCE:72.0542\tKLD:17.9445\tC_loss:0.0000\n",
      "Train Epoch: 98 [20480/60000 (34%)]\tLoss: 89.194061\tBCE:70.9052\tKLD:18.2889\tC_loss:0.0000\n",
      "Train Epoch: 98 [23040/60000 (38%)]\tLoss: 85.275276\tBCE:68.2957\tKLD:16.9796\tC_loss:0.0000\n",
      "Train Epoch: 98 [25600/60000 (43%)]\tLoss: 88.010437\tBCE:70.2954\tKLD:17.7151\tC_loss:0.0000\n",
      "Train Epoch: 98 [28160/60000 (47%)]\tLoss: 88.815361\tBCE:71.5251\tKLD:17.2903\tC_loss:0.0000\n",
      "Train Epoch: 98 [30720/60000 (51%)]\tLoss: 93.453857\tBCE:75.0851\tKLD:18.3688\tC_loss:0.0000\n",
      "Train Epoch: 98 [33280/60000 (56%)]\tLoss: 89.091522\tBCE:71.0451\tKLD:18.0464\tC_loss:0.0000\n",
      "Train Epoch: 98 [35840/60000 (60%)]\tLoss: 87.472893\tBCE:70.3829\tKLD:17.0900\tC_loss:0.0000\n",
      "Train Epoch: 98 [38400/60000 (64%)]\tLoss: 88.788490\tBCE:70.8553\tKLD:17.9332\tC_loss:0.0000\n",
      "Train Epoch: 98 [40960/60000 (68%)]\tLoss: 86.516914\tBCE:68.8563\tKLD:17.6606\tC_loss:0.0000\n",
      "Train Epoch: 98 [43520/60000 (73%)]\tLoss: 90.412079\tBCE:72.0628\tKLD:18.3493\tC_loss:0.0000\n",
      "Train Epoch: 98 [46080/60000 (77%)]\tLoss: 89.672890\tBCE:71.8893\tKLD:17.7836\tC_loss:0.0000\n",
      "Train Epoch: 98 [48640/60000 (81%)]\tLoss: 88.960495\tBCE:70.8394\tKLD:18.1211\tC_loss:0.0000\n",
      "Train Epoch: 98 [51200/60000 (85%)]\tLoss: 88.341583\tBCE:70.5932\tKLD:17.7483\tC_loss:0.0000\n",
      "Train Epoch: 98 [53760/60000 (90%)]\tLoss: 91.129265\tBCE:73.1805\tKLD:17.9488\tC_loss:0.0000\n",
      "Train Epoch: 98 [56320/60000 (94%)]\tLoss: 86.256653\tBCE:69.1467\tKLD:17.1099\tC_loss:0.0000\n",
      "Train Epoch: 98 [58880/60000 (98%)]\tLoss: 92.118416\tBCE:73.5993\tKLD:18.5192\tC_loss:0.0000\n",
      "====> Epoch: 98 Average loss: 88.1551\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.9929\n",
      "Train Epoch: 99 [0/60000 (0%)]\tLoss: 86.105309\tBCE:68.9205\tKLD:17.1848\tC_loss:0.0000\n",
      "Train Epoch: 99 [2560/60000 (4%)]\tLoss: 87.855804\tBCE:69.8510\tKLD:18.0048\tC_loss:0.0000\n",
      "Train Epoch: 99 [5120/60000 (9%)]\tLoss: 88.544571\tBCE:71.0667\tKLD:17.4779\tC_loss:0.0000\n",
      "Train Epoch: 99 [7680/60000 (13%)]\tLoss: 85.773651\tBCE:68.4431\tKLD:17.3305\tC_loss:0.0000\n",
      "Train Epoch: 99 [10240/60000 (17%)]\tLoss: 87.925858\tBCE:70.0638\tKLD:17.8620\tC_loss:0.0000\n",
      "Train Epoch: 99 [12800/60000 (21%)]\tLoss: 87.582916\tBCE:70.1761\tKLD:17.4068\tC_loss:0.0000\n",
      "Train Epoch: 99 [15360/60000 (26%)]\tLoss: 89.080460\tBCE:70.9591\tKLD:18.1213\tC_loss:0.0000\n",
      "Train Epoch: 99 [17920/60000 (30%)]\tLoss: 89.660858\tBCE:72.0641\tKLD:17.5967\tC_loss:0.0000\n",
      "Train Epoch: 99 [20480/60000 (34%)]\tLoss: 88.539116\tBCE:70.4621\tKLD:18.0770\tC_loss:0.0000\n",
      "Train Epoch: 99 [23040/60000 (38%)]\tLoss: 88.027954\tBCE:70.8788\tKLD:17.1492\tC_loss:0.0000\n",
      "Train Epoch: 99 [25600/60000 (43%)]\tLoss: 87.627991\tBCE:70.1330\tKLD:17.4950\tC_loss:0.0000\n",
      "Train Epoch: 99 [28160/60000 (47%)]\tLoss: 90.919174\tBCE:72.7912\tKLD:18.1280\tC_loss:0.0000\n",
      "Train Epoch: 99 [30720/60000 (51%)]\tLoss: 89.139236\tBCE:71.3215\tKLD:17.8177\tC_loss:0.0000\n",
      "Train Epoch: 99 [33280/60000 (56%)]\tLoss: 90.982132\tBCE:72.5827\tKLD:18.3995\tC_loss:0.0000\n",
      "Train Epoch: 99 [35840/60000 (60%)]\tLoss: 87.374405\tBCE:69.7450\tKLD:17.6294\tC_loss:0.0000\n",
      "Train Epoch: 99 [38400/60000 (64%)]\tLoss: 89.379196\tBCE:71.7501\tKLD:17.6291\tC_loss:0.0000\n",
      "Train Epoch: 99 [40960/60000 (68%)]\tLoss: 88.862747\tBCE:70.8055\tKLD:18.0572\tC_loss:0.0000\n",
      "Train Epoch: 99 [43520/60000 (73%)]\tLoss: 90.504333\tBCE:72.9754\tKLD:17.5290\tC_loss:0.0000\n",
      "Train Epoch: 99 [46080/60000 (77%)]\tLoss: 88.223083\tBCE:70.1899\tKLD:18.0331\tC_loss:0.0000\n",
      "Train Epoch: 99 [48640/60000 (81%)]\tLoss: 90.852890\tBCE:72.9001\tKLD:17.9528\tC_loss:0.0000\n",
      "Train Epoch: 99 [51200/60000 (85%)]\tLoss: 88.772621\tBCE:70.5513\tKLD:18.2214\tC_loss:0.0000\n",
      "Train Epoch: 99 [53760/60000 (90%)]\tLoss: 89.847122\tBCE:71.8063\tKLD:18.0408\tC_loss:0.0000\n",
      "Train Epoch: 99 [56320/60000 (94%)]\tLoss: 85.500931\tBCE:68.3281\tKLD:17.1728\tC_loss:0.0000\n",
      "Train Epoch: 99 [58880/60000 (98%)]\tLoss: 88.704117\tBCE:71.0751\tKLD:17.6290\tC_loss:0.0000\n",
      "====> Epoch: 99 Average loss: 88.2547\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.4743\n",
      "Train Epoch: 100 [0/60000 (0%)]\tLoss: 88.396912\tBCE:70.3705\tKLD:18.0264\tC_loss:0.0000\n",
      "Train Epoch: 100 [2560/60000 (4%)]\tLoss: 85.909149\tBCE:68.5602\tKLD:17.3490\tC_loss:0.0000\n",
      "Train Epoch: 100 [5120/60000 (9%)]\tLoss: 88.199623\tBCE:70.9341\tKLD:17.2656\tC_loss:0.0000\n",
      "Train Epoch: 100 [7680/60000 (13%)]\tLoss: 86.350220\tBCE:68.6784\tKLD:17.6718\tC_loss:0.0000\n",
      "Train Epoch: 100 [10240/60000 (17%)]\tLoss: 88.338463\tBCE:70.4740\tKLD:17.8645\tC_loss:0.0000\n",
      "Train Epoch: 100 [12800/60000 (21%)]\tLoss: 88.268684\tBCE:70.6040\tKLD:17.6647\tC_loss:0.0000\n",
      "Train Epoch: 100 [15360/60000 (26%)]\tLoss: 87.378822\tBCE:69.6828\tKLD:17.6961\tC_loss:0.0000\n",
      "Train Epoch: 100 [17920/60000 (30%)]\tLoss: 89.054474\tBCE:71.2902\tKLD:17.7643\tC_loss:0.0000\n",
      "Train Epoch: 100 [20480/60000 (34%)]\tLoss: 89.329193\tBCE:71.5386\tKLD:17.7906\tC_loss:0.0000\n",
      "Train Epoch: 100 [23040/60000 (38%)]\tLoss: 88.631676\tBCE:70.5391\tKLD:18.0926\tC_loss:0.0000\n",
      "Train Epoch: 100 [25600/60000 (43%)]\tLoss: 87.957718\tBCE:70.0409\tKLD:17.9169\tC_loss:0.0000\n",
      "Train Epoch: 100 [28160/60000 (47%)]\tLoss: 87.795990\tBCE:70.2416\tKLD:17.5544\tC_loss:0.0000\n",
      "Train Epoch: 100 [30720/60000 (51%)]\tLoss: 92.992256\tBCE:74.7602\tKLD:18.2321\tC_loss:0.0000\n",
      "Train Epoch: 100 [33280/60000 (56%)]\tLoss: 86.423172\tBCE:69.1796\tKLD:17.2436\tC_loss:0.0000\n",
      "Train Epoch: 100 [35840/60000 (60%)]\tLoss: 88.015556\tBCE:70.5659\tKLD:17.4496\tC_loss:0.0000\n",
      "Train Epoch: 100 [38400/60000 (64%)]\tLoss: 85.566559\tBCE:68.4266\tKLD:17.1399\tC_loss:0.0000\n",
      "Train Epoch: 100 [40960/60000 (68%)]\tLoss: 90.920319\tBCE:72.6406\tKLD:18.2797\tC_loss:0.0000\n",
      "Train Epoch: 100 [43520/60000 (73%)]\tLoss: 87.085251\tBCE:69.4237\tKLD:17.6616\tC_loss:0.0000\n",
      "Train Epoch: 100 [46080/60000 (77%)]\tLoss: 90.189377\tBCE:72.2300\tKLD:17.9593\tC_loss:0.0000\n",
      "Train Epoch: 100 [48640/60000 (81%)]\tLoss: 89.029572\tBCE:71.4554\tKLD:17.5742\tC_loss:0.0000\n",
      "Train Epoch: 100 [51200/60000 (85%)]\tLoss: 88.833145\tBCE:71.1474\tKLD:17.6858\tC_loss:0.0000\n",
      "Train Epoch: 100 [53760/60000 (90%)]\tLoss: 88.232880\tBCE:70.2557\tKLD:17.9772\tC_loss:0.0000\n",
      "Train Epoch: 100 [56320/60000 (94%)]\tLoss: 86.956421\tBCE:69.2367\tKLD:17.7197\tC_loss:0.0000\n",
      "Train Epoch: 100 [58880/60000 (98%)]\tLoss: 89.252495\tBCE:71.4979\tKLD:17.7546\tC_loss:0.0000\n",
      "====> Epoch: 100 Average loss: 88.2270\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.1054\n",
      "Random number: 4\n",
      "Train Epoch: 101 [0/60000 (0%)]\tLoss: 88.408203\tBCE:70.3235\tKLD:18.0847\tC_loss:0.0000\n",
      "Train Epoch: 101 [2560/60000 (4%)]\tLoss: 87.477509\tBCE:70.2715\tKLD:17.2060\tC_loss:0.0000\n",
      "Train Epoch: 101 [5120/60000 (9%)]\tLoss: 86.766647\tBCE:68.9528\tKLD:17.8139\tC_loss:0.0000\n",
      "Train Epoch: 101 [7680/60000 (13%)]\tLoss: 90.758469\tBCE:72.9931\tKLD:17.7654\tC_loss:0.0000\n",
      "Train Epoch: 101 [10240/60000 (17%)]\tLoss: 86.078598\tBCE:67.8975\tKLD:18.1811\tC_loss:0.0000\n",
      "Train Epoch: 101 [12800/60000 (21%)]\tLoss: 87.887466\tBCE:69.7592\tKLD:18.1283\tC_loss:0.0000\n",
      "Train Epoch: 101 [15360/60000 (26%)]\tLoss: 89.146706\tBCE:71.2061\tKLD:17.9406\tC_loss:0.0000\n",
      "Train Epoch: 101 [17920/60000 (30%)]\tLoss: 89.311203\tBCE:71.5988\tKLD:17.7124\tC_loss:0.0000\n",
      "Train Epoch: 101 [20480/60000 (34%)]\tLoss: 90.487312\tBCE:71.9787\tKLD:18.5086\tC_loss:0.0000\n",
      "Train Epoch: 101 [23040/60000 (38%)]\tLoss: 89.520874\tBCE:72.3878\tKLD:17.1331\tC_loss:0.0000\n",
      "Train Epoch: 101 [25600/60000 (43%)]\tLoss: 90.299675\tBCE:72.3861\tKLD:17.9136\tC_loss:0.0000\n",
      "Train Epoch: 101 [28160/60000 (47%)]\tLoss: 88.832687\tBCE:71.1517\tKLD:17.6810\tC_loss:0.0000\n",
      "Train Epoch: 101 [30720/60000 (51%)]\tLoss: 88.220932\tBCE:70.2887\tKLD:17.9322\tC_loss:0.0000\n",
      "Train Epoch: 101 [33280/60000 (56%)]\tLoss: 87.201218\tBCE:69.4407\tKLD:17.7605\tC_loss:0.0000\n",
      "Train Epoch: 101 [35840/60000 (60%)]\tLoss: 87.010666\tBCE:69.4050\tKLD:17.6057\tC_loss:0.0000\n",
      "Train Epoch: 101 [38400/60000 (64%)]\tLoss: 91.174889\tBCE:72.9609\tKLD:18.2140\tC_loss:0.0000\n",
      "Train Epoch: 101 [40960/60000 (68%)]\tLoss: 87.829575\tBCE:69.8781\tKLD:17.9515\tC_loss:0.0000\n",
      "Train Epoch: 101 [43520/60000 (73%)]\tLoss: 88.819199\tBCE:71.4312\tKLD:17.3880\tC_loss:0.0000\n",
      "Train Epoch: 101 [46080/60000 (77%)]\tLoss: 88.529770\tBCE:70.6031\tKLD:17.9267\tC_loss:0.0000\n",
      "Train Epoch: 101 [48640/60000 (81%)]\tLoss: 87.516251\tBCE:69.5093\tKLD:18.0069\tC_loss:0.0000\n",
      "Train Epoch: 101 [51200/60000 (85%)]\tLoss: 87.065903\tBCE:69.3577\tKLD:17.7082\tC_loss:0.0000\n",
      "Train Epoch: 101 [53760/60000 (90%)]\tLoss: 88.983810\tBCE:71.3784\tKLD:17.6054\tC_loss:0.0000\n",
      "Train Epoch: 101 [56320/60000 (94%)]\tLoss: 89.753067\tBCE:72.1107\tKLD:17.6424\tC_loss:0.0000\n",
      "Train Epoch: 101 [58880/60000 (98%)]\tLoss: 87.571976\tBCE:69.7844\tKLD:17.7876\tC_loss:0.0000\n",
      "====> Epoch: 101 Average loss: 88.1500\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0898\n",
      "Train Epoch: 102 [0/60000 (0%)]\tLoss: 88.432167\tBCE:70.8509\tKLD:17.5813\tC_loss:0.0000\n",
      "Train Epoch: 102 [2560/60000 (4%)]\tLoss: 88.331924\tBCE:70.2554\tKLD:18.0765\tC_loss:0.0000\n",
      "Train Epoch: 102 [5120/60000 (9%)]\tLoss: 86.758316\tBCE:69.1693\tKLD:17.5890\tC_loss:0.0000\n",
      "Train Epoch: 102 [7680/60000 (13%)]\tLoss: 91.998978\tBCE:73.5521\tKLD:18.4469\tC_loss:0.0000\n",
      "Train Epoch: 102 [10240/60000 (17%)]\tLoss: 88.974022\tBCE:70.9534\tKLD:18.0207\tC_loss:0.0000\n",
      "Train Epoch: 102 [12800/60000 (21%)]\tLoss: 87.092918\tBCE:69.4740\tKLD:17.6189\tC_loss:0.0000\n",
      "Train Epoch: 102 [15360/60000 (26%)]\tLoss: 88.923264\tBCE:71.2294\tKLD:17.6939\tC_loss:0.0000\n",
      "Train Epoch: 102 [17920/60000 (30%)]\tLoss: 89.365585\tBCE:71.0673\tKLD:18.2983\tC_loss:0.0000\n",
      "Train Epoch: 102 [20480/60000 (34%)]\tLoss: 89.373123\tBCE:71.3751\tKLD:17.9980\tC_loss:0.0000\n",
      "Train Epoch: 102 [23040/60000 (38%)]\tLoss: 88.941673\tBCE:70.9860\tKLD:17.9556\tC_loss:0.0000\n",
      "Train Epoch: 102 [25600/60000 (43%)]\tLoss: 87.620262\tBCE:70.0024\tKLD:17.6179\tC_loss:0.0000\n",
      "Train Epoch: 102 [28160/60000 (47%)]\tLoss: 86.354111\tBCE:68.7539\tKLD:17.6002\tC_loss:0.0000\n",
      "Train Epoch: 102 [30720/60000 (51%)]\tLoss: 90.900597\tBCE:73.0623\tKLD:17.8383\tC_loss:0.0000\n",
      "Train Epoch: 102 [33280/60000 (56%)]\tLoss: 87.856094\tBCE:70.1582\tKLD:17.6979\tC_loss:0.0000\n",
      "Train Epoch: 102 [35840/60000 (60%)]\tLoss: 87.734665\tBCE:70.4750\tKLD:17.2596\tC_loss:0.0000\n",
      "Train Epoch: 102 [38400/60000 (64%)]\tLoss: 88.161072\tBCE:70.4003\tKLD:17.7608\tC_loss:0.0000\n",
      "Train Epoch: 102 [40960/60000 (68%)]\tLoss: 88.184280\tBCE:70.3516\tKLD:17.8327\tC_loss:0.0000\n",
      "Train Epoch: 102 [43520/60000 (73%)]\tLoss: 85.913452\tBCE:68.5362\tKLD:17.3772\tC_loss:0.0000\n",
      "Train Epoch: 102 [46080/60000 (77%)]\tLoss: 91.195824\tBCE:73.1037\tKLD:18.0921\tC_loss:0.0000\n",
      "Train Epoch: 102 [48640/60000 (81%)]\tLoss: 87.797211\tBCE:70.7365\tKLD:17.0607\tC_loss:0.0000\n",
      "Train Epoch: 102 [51200/60000 (85%)]\tLoss: 86.809921\tBCE:69.1160\tKLD:17.6939\tC_loss:0.0000\n",
      "Train Epoch: 102 [53760/60000 (90%)]\tLoss: 91.591446\tBCE:73.0237\tKLD:18.5678\tC_loss:0.0000\n",
      "Train Epoch: 102 [56320/60000 (94%)]\tLoss: 88.742386\tBCE:71.0239\tKLD:17.7185\tC_loss:0.0000\n",
      "Train Epoch: 102 [58880/60000 (98%)]\tLoss: 89.694473\tBCE:71.8039\tKLD:17.8906\tC_loss:0.0000\n",
      "====> Epoch: 102 Average loss: 88.1444\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.1820\n",
      "Train Epoch: 103 [0/60000 (0%)]\tLoss: 90.372070\tBCE:72.3275\tKLD:18.0446\tC_loss:0.0000\n",
      "Train Epoch: 103 [2560/60000 (4%)]\tLoss: 86.636894\tBCE:69.0412\tKLD:17.5957\tC_loss:0.0000\n",
      "Train Epoch: 103 [5120/60000 (9%)]\tLoss: 87.590378\tBCE:70.6347\tKLD:16.9557\tC_loss:0.0000\n",
      "Train Epoch: 103 [7680/60000 (13%)]\tLoss: 88.467888\tBCE:70.4026\tKLD:18.0653\tC_loss:0.0000\n",
      "Train Epoch: 103 [10240/60000 (17%)]\tLoss: 86.804527\tBCE:68.6494\tKLD:18.1551\tC_loss:0.0000\n",
      "Train Epoch: 103 [12800/60000 (21%)]\tLoss: 88.628777\tBCE:70.7230\tKLD:17.9057\tC_loss:0.0000\n",
      "Train Epoch: 103 [15360/60000 (26%)]\tLoss: 84.228180\tBCE:66.8780\tKLD:17.3502\tC_loss:0.0000\n",
      "Train Epoch: 103 [17920/60000 (30%)]\tLoss: 86.687004\tBCE:69.3692\tKLD:17.3178\tC_loss:0.0000\n",
      "Train Epoch: 103 [20480/60000 (34%)]\tLoss: 86.226387\tBCE:68.4417\tKLD:17.7847\tC_loss:0.0000\n",
      "Train Epoch: 103 [23040/60000 (38%)]\tLoss: 88.744949\tBCE:71.0551\tKLD:17.6898\tC_loss:0.0000\n",
      "Train Epoch: 103 [25600/60000 (43%)]\tLoss: 89.450050\tBCE:71.2342\tKLD:18.2158\tC_loss:0.0000\n",
      "Train Epoch: 103 [28160/60000 (47%)]\tLoss: 88.016235\tBCE:69.6799\tKLD:18.3364\tC_loss:0.0000\n",
      "Train Epoch: 103 [30720/60000 (51%)]\tLoss: 88.947601\tBCE:71.1085\tKLD:17.8391\tC_loss:0.0000\n",
      "Train Epoch: 103 [33280/60000 (56%)]\tLoss: 84.885559\tBCE:67.6376\tKLD:17.2479\tC_loss:0.0000\n",
      "Train Epoch: 103 [35840/60000 (60%)]\tLoss: 89.414894\tBCE:71.4021\tKLD:18.0128\tC_loss:0.0000\n",
      "Train Epoch: 103 [38400/60000 (64%)]\tLoss: 86.863678\tBCE:70.0123\tKLD:16.8514\tC_loss:0.0000\n",
      "Train Epoch: 103 [40960/60000 (68%)]\tLoss: 87.924721\tBCE:70.0704\tKLD:17.8543\tC_loss:0.0000\n",
      "Train Epoch: 103 [43520/60000 (73%)]\tLoss: 89.206528\tBCE:71.3899\tKLD:17.8166\tC_loss:0.0000\n",
      "Train Epoch: 103 [46080/60000 (77%)]\tLoss: 89.170494\tBCE:71.2651\tKLD:17.9054\tC_loss:0.0000\n",
      "Train Epoch: 103 [48640/60000 (81%)]\tLoss: 89.259789\tBCE:71.5267\tKLD:17.7331\tC_loss:0.0000\n",
      "Train Epoch: 103 [51200/60000 (85%)]\tLoss: 88.245834\tBCE:70.7117\tKLD:17.5341\tC_loss:0.0000\n",
      "Train Epoch: 103 [53760/60000 (90%)]\tLoss: 88.567978\tBCE:70.5596\tKLD:18.0084\tC_loss:0.0000\n",
      "Train Epoch: 103 [56320/60000 (94%)]\tLoss: 90.341827\tBCE:72.2722\tKLD:18.0696\tC_loss:0.0000\n",
      "Train Epoch: 103 [58880/60000 (98%)]\tLoss: 88.182030\tBCE:70.3484\tKLD:17.8337\tC_loss:0.0000\n",
      "====> Epoch: 103 Average loss: 88.0200\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.9749\n",
      "Train Epoch: 104 [0/60000 (0%)]\tLoss: 86.816422\tBCE:69.2761\tKLD:17.5404\tC_loss:0.0000\n",
      "Train Epoch: 104 [2560/60000 (4%)]\tLoss: 84.997963\tBCE:67.7006\tKLD:17.2974\tC_loss:0.0000\n",
      "Train Epoch: 104 [5120/60000 (9%)]\tLoss: 87.933167\tBCE:70.3448\tKLD:17.5883\tC_loss:0.0000\n",
      "Train Epoch: 104 [7680/60000 (13%)]\tLoss: 89.488876\tBCE:71.4114\tKLD:18.0775\tC_loss:0.0000\n",
      "Train Epoch: 104 [10240/60000 (17%)]\tLoss: 87.525520\tBCE:69.9213\tKLD:17.6043\tC_loss:0.0000\n",
      "Train Epoch: 104 [12800/60000 (21%)]\tLoss: 90.997055\tBCE:72.5350\tKLD:18.4621\tC_loss:0.0000\n",
      "Train Epoch: 104 [15360/60000 (26%)]\tLoss: 87.259384\tBCE:69.6106\tKLD:17.6488\tC_loss:0.0000\n",
      "Train Epoch: 104 [17920/60000 (30%)]\tLoss: 90.498741\tBCE:72.3130\tKLD:18.1857\tC_loss:0.0000\n",
      "Train Epoch: 104 [20480/60000 (34%)]\tLoss: 90.611664\tBCE:73.1256\tKLD:17.4861\tC_loss:0.0000\n",
      "Train Epoch: 104 [23040/60000 (38%)]\tLoss: 87.344666\tBCE:69.6501\tKLD:17.6946\tC_loss:0.0000\n",
      "Train Epoch: 104 [25600/60000 (43%)]\tLoss: 87.450485\tBCE:69.1872\tKLD:18.2633\tC_loss:0.0000\n",
      "Train Epoch: 104 [28160/60000 (47%)]\tLoss: 85.786407\tBCE:69.0106\tKLD:16.7759\tC_loss:0.0000\n",
      "Train Epoch: 104 [30720/60000 (51%)]\tLoss: 90.035202\tBCE:72.1308\tKLD:17.9044\tC_loss:0.0000\n",
      "Train Epoch: 104 [33280/60000 (56%)]\tLoss: 89.881149\tBCE:71.9966\tKLD:17.8846\tC_loss:0.0000\n",
      "Train Epoch: 104 [35840/60000 (60%)]\tLoss: 87.426765\tBCE:70.0732\tKLD:17.3536\tC_loss:0.0000\n",
      "Train Epoch: 104 [38400/60000 (64%)]\tLoss: 90.519958\tBCE:72.1880\tKLD:18.3320\tC_loss:0.0000\n",
      "Train Epoch: 104 [40960/60000 (68%)]\tLoss: 87.508331\tBCE:70.3398\tKLD:17.1685\tC_loss:0.0000\n",
      "Train Epoch: 104 [43520/60000 (73%)]\tLoss: 89.432419\tBCE:71.9981\tKLD:17.4343\tC_loss:0.0000\n",
      "Train Epoch: 104 [46080/60000 (77%)]\tLoss: 86.287460\tBCE:68.9138\tKLD:17.3737\tC_loss:0.0000\n",
      "Train Epoch: 104 [48640/60000 (81%)]\tLoss: 89.080124\tBCE:71.0739\tKLD:18.0062\tC_loss:0.0000\n",
      "Train Epoch: 104 [51200/60000 (85%)]\tLoss: 88.936890\tBCE:71.2331\tKLD:17.7038\tC_loss:0.0000\n",
      "Train Epoch: 104 [53760/60000 (90%)]\tLoss: 86.586716\tBCE:69.2150\tKLD:17.3717\tC_loss:0.0000\n",
      "Train Epoch: 104 [56320/60000 (94%)]\tLoss: 87.344803\tBCE:69.6891\tKLD:17.6557\tC_loss:0.0000\n",
      "Train Epoch: 104 [58880/60000 (98%)]\tLoss: 85.147263\tBCE:68.0387\tKLD:17.1085\tC_loss:0.0000\n",
      "====> Epoch: 104 Average loss: 87.9854\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0319\n",
      "Train Epoch: 105 [0/60000 (0%)]\tLoss: 87.321136\tBCE:70.1435\tKLD:17.1776\tC_loss:0.0000\n",
      "Train Epoch: 105 [2560/60000 (4%)]\tLoss: 86.012169\tBCE:68.5864\tKLD:17.4258\tC_loss:0.0000\n",
      "Train Epoch: 105 [5120/60000 (9%)]\tLoss: 86.486458\tBCE:68.8579\tKLD:17.6286\tC_loss:0.0000\n",
      "Train Epoch: 105 [7680/60000 (13%)]\tLoss: 85.676552\tBCE:68.4831\tKLD:17.1934\tC_loss:0.0000\n",
      "Train Epoch: 105 [10240/60000 (17%)]\tLoss: 84.656906\tBCE:67.2561\tKLD:17.4008\tC_loss:0.0000\n",
      "Train Epoch: 105 [12800/60000 (21%)]\tLoss: 88.648438\tBCE:70.8004\tKLD:17.8480\tC_loss:0.0000\n",
      "Train Epoch: 105 [15360/60000 (26%)]\tLoss: 87.622421\tBCE:69.6668\tKLD:17.9556\tC_loss:0.0000\n",
      "Train Epoch: 105 [17920/60000 (30%)]\tLoss: 92.089142\tBCE:74.0916\tKLD:17.9975\tC_loss:0.0000\n",
      "Train Epoch: 105 [20480/60000 (34%)]\tLoss: 88.806633\tBCE:70.7296\tKLD:18.0770\tC_loss:0.0000\n",
      "Train Epoch: 105 [23040/60000 (38%)]\tLoss: 87.071487\tBCE:69.5946\tKLD:17.4769\tC_loss:0.0000\n",
      "Train Epoch: 105 [25600/60000 (43%)]\tLoss: 88.635963\tBCE:70.7640\tKLD:17.8720\tC_loss:0.0000\n",
      "Train Epoch: 105 [28160/60000 (47%)]\tLoss: 87.853554\tBCE:70.2091\tKLD:17.6445\tC_loss:0.0000\n",
      "Train Epoch: 105 [30720/60000 (51%)]\tLoss: 87.349785\tBCE:69.7349\tKLD:17.6149\tC_loss:0.0000\n",
      "Train Epoch: 105 [33280/60000 (56%)]\tLoss: 87.727776\tBCE:70.5085\tKLD:17.2193\tC_loss:0.0000\n",
      "Train Epoch: 105 [35840/60000 (60%)]\tLoss: 88.269341\tBCE:70.1412\tKLD:18.1282\tC_loss:0.0000\n",
      "Train Epoch: 105 [38400/60000 (64%)]\tLoss: 87.639702\tBCE:70.2014\tKLD:17.4383\tC_loss:0.0000\n",
      "Train Epoch: 105 [40960/60000 (68%)]\tLoss: 87.574493\tBCE:69.8913\tKLD:17.6832\tC_loss:0.0000\n",
      "Train Epoch: 105 [43520/60000 (73%)]\tLoss: 87.322510\tBCE:69.8885\tKLD:17.4340\tC_loss:0.0000\n",
      "Train Epoch: 105 [46080/60000 (77%)]\tLoss: 86.102631\tBCE:68.6677\tKLD:17.4349\tC_loss:0.0000\n",
      "Train Epoch: 105 [48640/60000 (81%)]\tLoss: 88.320129\tBCE:70.4322\tKLD:17.8880\tC_loss:0.0000\n",
      "Train Epoch: 105 [51200/60000 (85%)]\tLoss: 89.368309\tBCE:71.9094\tKLD:17.4589\tC_loss:0.0000\n",
      "Train Epoch: 105 [53760/60000 (90%)]\tLoss: 88.338501\tBCE:70.5853\tKLD:17.7532\tC_loss:0.0000\n",
      "Train Epoch: 105 [56320/60000 (94%)]\tLoss: 89.946655\tBCE:72.4042\tKLD:17.5425\tC_loss:0.0000\n",
      "Train Epoch: 105 [58880/60000 (98%)]\tLoss: 88.573936\tBCE:71.2894\tKLD:17.2845\tC_loss:0.0000\n",
      "====> Epoch: 105 Average loss: 88.1245\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 90.9463\n",
      "Random number: 4\n",
      "Train Epoch: 106 [0/60000 (0%)]\tLoss: 88.494705\tBCE:71.1178\tKLD:17.3769\tC_loss:0.0000\n",
      "Train Epoch: 106 [2560/60000 (4%)]\tLoss: 91.123123\tBCE:72.9023\tKLD:18.2208\tC_loss:0.0000\n",
      "Train Epoch: 106 [5120/60000 (9%)]\tLoss: 87.286049\tBCE:69.9511\tKLD:17.3350\tC_loss:0.0000\n",
      "Train Epoch: 106 [7680/60000 (13%)]\tLoss: 88.433281\tBCE:70.0581\tKLD:18.3752\tC_loss:0.0000\n",
      "Train Epoch: 106 [10240/60000 (17%)]\tLoss: 87.481964\tBCE:70.0945\tKLD:17.3875\tC_loss:0.0000\n",
      "Train Epoch: 106 [12800/60000 (21%)]\tLoss: 88.356361\tBCE:71.0999\tKLD:17.2564\tC_loss:0.0000\n",
      "Train Epoch: 106 [15360/60000 (26%)]\tLoss: 86.407051\tBCE:68.5040\tKLD:17.9030\tC_loss:0.0000\n",
      "Train Epoch: 106 [17920/60000 (30%)]\tLoss: 87.531372\tBCE:69.9495\tKLD:17.5819\tC_loss:0.0000\n",
      "Train Epoch: 106 [20480/60000 (34%)]\tLoss: 89.675949\tBCE:72.1075\tKLD:17.5684\tC_loss:0.0000\n",
      "Train Epoch: 106 [23040/60000 (38%)]\tLoss: 87.425446\tBCE:69.5681\tKLD:17.8574\tC_loss:0.0000\n",
      "Train Epoch: 106 [25600/60000 (43%)]\tLoss: 86.985374\tBCE:69.5130\tKLD:17.4723\tC_loss:0.0000\n",
      "Train Epoch: 106 [28160/60000 (47%)]\tLoss: 88.536942\tBCE:70.7574\tKLD:17.7795\tC_loss:0.0000\n",
      "Train Epoch: 106 [30720/60000 (51%)]\tLoss: 89.128212\tBCE:71.3929\tKLD:17.7353\tC_loss:0.0000\n",
      "Train Epoch: 106 [33280/60000 (56%)]\tLoss: 87.487885\tBCE:70.2420\tKLD:17.2459\tC_loss:0.0000\n",
      "Train Epoch: 106 [35840/60000 (60%)]\tLoss: 87.851196\tBCE:70.2507\tKLD:17.6005\tC_loss:0.0000\n",
      "Train Epoch: 106 [38400/60000 (64%)]\tLoss: 85.101158\tBCE:68.0426\tKLD:17.0585\tC_loss:0.0000\n",
      "Train Epoch: 106 [40960/60000 (68%)]\tLoss: 90.618988\tBCE:72.7812\tKLD:17.8378\tC_loss:0.0000\n",
      "Train Epoch: 106 [43520/60000 (73%)]\tLoss: 91.370949\tBCE:73.1775\tKLD:18.1934\tC_loss:0.0000\n",
      "Train Epoch: 106 [46080/60000 (77%)]\tLoss: 87.692879\tBCE:70.3599\tKLD:17.3330\tC_loss:0.0000\n",
      "Train Epoch: 106 [48640/60000 (81%)]\tLoss: 88.695038\tBCE:70.6349\tKLD:18.0601\tC_loss:0.0000\n",
      "Train Epoch: 106 [51200/60000 (85%)]\tLoss: 86.990517\tBCE:69.2700\tKLD:17.7205\tC_loss:0.0000\n",
      "Train Epoch: 106 [53760/60000 (90%)]\tLoss: 88.715042\tBCE:71.2039\tKLD:17.5112\tC_loss:0.0000\n",
      "Train Epoch: 106 [56320/60000 (94%)]\tLoss: 85.252792\tBCE:68.1751\tKLD:17.0777\tC_loss:0.0000\n",
      "Train Epoch: 106 [58880/60000 (98%)]\tLoss: 89.209274\tBCE:71.6732\tKLD:17.5361\tC_loss:0.0000\n",
      "====> Epoch: 106 Average loss: 87.9961\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.1160\n",
      "Train Epoch: 107 [0/60000 (0%)]\tLoss: 85.745461\tBCE:68.2784\tKLD:17.4670\tC_loss:0.0000\n",
      "Train Epoch: 107 [2560/60000 (4%)]\tLoss: 87.961906\tBCE:70.7238\tKLD:17.2381\tC_loss:0.0000\n",
      "Train Epoch: 107 [5120/60000 (9%)]\tLoss: 86.588425\tBCE:69.3462\tKLD:17.2423\tC_loss:0.0000\n",
      "Train Epoch: 107 [7680/60000 (13%)]\tLoss: 87.450562\tBCE:69.8339\tKLD:17.6167\tC_loss:0.0000\n",
      "Train Epoch: 107 [10240/60000 (17%)]\tLoss: 88.459259\tBCE:70.4666\tKLD:17.9926\tC_loss:0.0000\n",
      "Train Epoch: 107 [12800/60000 (21%)]\tLoss: 87.251663\tBCE:69.5996\tKLD:17.6520\tC_loss:0.0000\n",
      "Train Epoch: 107 [15360/60000 (26%)]\tLoss: 89.173630\tBCE:70.9136\tKLD:18.2601\tC_loss:0.0000\n",
      "Train Epoch: 107 [17920/60000 (30%)]\tLoss: 88.230103\tBCE:70.2763\tKLD:17.9538\tC_loss:0.0000\n",
      "Train Epoch: 107 [20480/60000 (34%)]\tLoss: 88.275299\tBCE:70.9598\tKLD:17.3155\tC_loss:0.0000\n",
      "Train Epoch: 107 [23040/60000 (38%)]\tLoss: 88.339798\tBCE:70.2167\tKLD:18.1231\tC_loss:0.0000\n",
      "Train Epoch: 107 [25600/60000 (43%)]\tLoss: 88.259407\tBCE:70.6128\tKLD:17.6466\tC_loss:0.0000\n",
      "Train Epoch: 107 [28160/60000 (47%)]\tLoss: 88.513824\tBCE:70.5038\tKLD:18.0101\tC_loss:0.0000\n",
      "Train Epoch: 107 [30720/60000 (51%)]\tLoss: 89.652359\tBCE:71.8713\tKLD:17.7811\tC_loss:0.0000\n",
      "Train Epoch: 107 [33280/60000 (56%)]\tLoss: 88.932129\tBCE:71.1511\tKLD:17.7810\tC_loss:0.0000\n",
      "Train Epoch: 107 [35840/60000 (60%)]\tLoss: 90.310242\tBCE:72.1676\tKLD:18.1427\tC_loss:0.0000\n",
      "Train Epoch: 107 [38400/60000 (64%)]\tLoss: 88.376205\tBCE:70.7604\tKLD:17.6158\tC_loss:0.0000\n",
      "Train Epoch: 107 [40960/60000 (68%)]\tLoss: 85.888535\tBCE:68.4904\tKLD:17.3982\tC_loss:0.0000\n",
      "Train Epoch: 107 [43520/60000 (73%)]\tLoss: 84.870522\tBCE:67.6433\tKLD:17.2273\tC_loss:0.0000\n",
      "Train Epoch: 107 [46080/60000 (77%)]\tLoss: 87.788567\tBCE:70.1322\tKLD:17.6564\tC_loss:0.0000\n",
      "Train Epoch: 107 [48640/60000 (81%)]\tLoss: 84.796829\tBCE:67.6434\tKLD:17.1534\tC_loss:0.0000\n",
      "Train Epoch: 107 [51200/60000 (85%)]\tLoss: 86.306976\tBCE:68.6637\tKLD:17.6433\tC_loss:0.0000\n",
      "Train Epoch: 107 [53760/60000 (90%)]\tLoss: 89.912201\tBCE:71.9641\tKLD:17.9481\tC_loss:0.0000\n",
      "Train Epoch: 107 [56320/60000 (94%)]\tLoss: 89.631706\tBCE:71.5648\tKLD:18.0669\tC_loss:0.0000\n",
      "Train Epoch: 107 [58880/60000 (98%)]\tLoss: 88.194458\tBCE:70.4077\tKLD:17.7868\tC_loss:0.0000\n",
      "====> Epoch: 107 Average loss: 87.9585\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.8804\n",
      "Train Epoch: 108 [0/60000 (0%)]\tLoss: 88.225098\tBCE:70.9128\tKLD:17.3123\tC_loss:0.0000\n",
      "Train Epoch: 108 [2560/60000 (4%)]\tLoss: 87.161774\tBCE:69.2515\tKLD:17.9103\tC_loss:0.0000\n",
      "Train Epoch: 108 [5120/60000 (9%)]\tLoss: 88.963570\tBCE:71.4852\tKLD:17.4783\tC_loss:0.0000\n",
      "Train Epoch: 108 [7680/60000 (13%)]\tLoss: 86.502151\tBCE:68.7412\tKLD:17.7610\tC_loss:0.0000\n",
      "Train Epoch: 108 [10240/60000 (17%)]\tLoss: 87.707451\tBCE:70.0338\tKLD:17.6737\tC_loss:0.0000\n",
      "Train Epoch: 108 [12800/60000 (21%)]\tLoss: 87.795395\tBCE:70.1181\tKLD:17.6773\tC_loss:0.0000\n",
      "Train Epoch: 108 [15360/60000 (26%)]\tLoss: 88.439941\tBCE:70.5658\tKLD:17.8742\tC_loss:0.0000\n",
      "Train Epoch: 108 [17920/60000 (30%)]\tLoss: 89.195435\tBCE:71.0878\tKLD:18.1076\tC_loss:0.0000\n",
      "Train Epoch: 108 [20480/60000 (34%)]\tLoss: 88.037720\tBCE:70.6315\tKLD:17.4062\tC_loss:0.0000\n",
      "Train Epoch: 108 [23040/60000 (38%)]\tLoss: 86.622162\tBCE:68.7880\tKLD:17.8342\tC_loss:0.0000\n",
      "Train Epoch: 108 [25600/60000 (43%)]\tLoss: 89.009109\tBCE:71.2691\tKLD:17.7400\tC_loss:0.0000\n",
      "Train Epoch: 108 [28160/60000 (47%)]\tLoss: 88.892059\tBCE:71.0279\tKLD:17.8641\tC_loss:0.0000\n",
      "Train Epoch: 108 [30720/60000 (51%)]\tLoss: 87.783524\tBCE:70.2032\tKLD:17.5803\tC_loss:0.0000\n",
      "Train Epoch: 108 [33280/60000 (56%)]\tLoss: 88.433975\tBCE:70.7430\tKLD:17.6910\tC_loss:0.0000\n",
      "Train Epoch: 108 [35840/60000 (60%)]\tLoss: 88.743378\tBCE:70.7936\tKLD:17.9498\tC_loss:0.0000\n",
      "Train Epoch: 108 [38400/60000 (64%)]\tLoss: 88.355553\tBCE:70.8590\tKLD:17.4966\tC_loss:0.0000\n",
      "Train Epoch: 108 [40960/60000 (68%)]\tLoss: 87.684113\tBCE:70.0790\tKLD:17.6051\tC_loss:0.0000\n",
      "Train Epoch: 108 [43520/60000 (73%)]\tLoss: 85.962776\tBCE:68.4052\tKLD:17.5576\tC_loss:0.0000\n",
      "Train Epoch: 108 [46080/60000 (77%)]\tLoss: 89.787590\tBCE:71.9438\tKLD:17.8438\tC_loss:0.0000\n",
      "Train Epoch: 108 [48640/60000 (81%)]\tLoss: 89.952744\tBCE:71.6237\tKLD:18.3290\tC_loss:0.0000\n",
      "Train Epoch: 108 [51200/60000 (85%)]\tLoss: 87.332108\tBCE:69.9229\tKLD:17.4092\tC_loss:0.0000\n",
      "Train Epoch: 108 [53760/60000 (90%)]\tLoss: 87.734467\tBCE:70.1337\tKLD:17.6008\tC_loss:0.0000\n",
      "Train Epoch: 108 [56320/60000 (94%)]\tLoss: 88.154694\tBCE:70.4173\tKLD:17.7374\tC_loss:0.0000\n",
      "Train Epoch: 108 [58880/60000 (98%)]\tLoss: 86.529694\tBCE:69.3489\tKLD:17.1808\tC_loss:0.0000\n",
      "====> Epoch: 108 Average loss: 88.0265\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.1937\n",
      "Train Epoch: 109 [0/60000 (0%)]\tLoss: 90.102234\tBCE:71.8079\tKLD:18.2943\tC_loss:0.0000\n",
      "Train Epoch: 109 [2560/60000 (4%)]\tLoss: 91.319405\tBCE:72.7721\tKLD:18.5473\tC_loss:0.0000\n",
      "Train Epoch: 109 [5120/60000 (9%)]\tLoss: 88.868713\tBCE:71.3566\tKLD:17.5122\tC_loss:0.0000\n",
      "Train Epoch: 109 [7680/60000 (13%)]\tLoss: 85.147034\tBCE:68.0446\tKLD:17.1024\tC_loss:0.0000\n",
      "Train Epoch: 109 [10240/60000 (17%)]\tLoss: 89.780739\tBCE:71.6198\tKLD:18.1609\tC_loss:0.0000\n",
      "Train Epoch: 109 [12800/60000 (21%)]\tLoss: 86.248993\tBCE:68.2496\tKLD:17.9994\tC_loss:0.0000\n",
      "Train Epoch: 109 [15360/60000 (26%)]\tLoss: 89.346313\tBCE:71.8323\tKLD:17.5140\tC_loss:0.0000\n",
      "Train Epoch: 109 [17920/60000 (30%)]\tLoss: 90.619308\tBCE:72.3200\tKLD:18.2993\tC_loss:0.0000\n",
      "Train Epoch: 109 [20480/60000 (34%)]\tLoss: 87.621849\tBCE:69.8768\tKLD:17.7450\tC_loss:0.0000\n",
      "Train Epoch: 109 [23040/60000 (38%)]\tLoss: 88.281143\tBCE:70.2930\tKLD:17.9881\tC_loss:0.0000\n",
      "Train Epoch: 109 [25600/60000 (43%)]\tLoss: 86.297401\tBCE:69.3832\tKLD:16.9142\tC_loss:0.0000\n",
      "Train Epoch: 109 [28160/60000 (47%)]\tLoss: 86.924606\tBCE:68.8563\tKLD:18.0683\tC_loss:0.0000\n",
      "Train Epoch: 109 [30720/60000 (51%)]\tLoss: 89.218208\tBCE:71.0439\tKLD:18.1743\tC_loss:0.0000\n",
      "Train Epoch: 109 [33280/60000 (56%)]\tLoss: 86.851349\tBCE:69.3571\tKLD:17.4943\tC_loss:0.0000\n",
      "Train Epoch: 109 [35840/60000 (60%)]\tLoss: 85.874374\tBCE:68.7414\tKLD:17.1330\tC_loss:0.0000\n",
      "Train Epoch: 109 [38400/60000 (64%)]\tLoss: 86.218407\tBCE:68.5911\tKLD:17.6273\tC_loss:0.0000\n",
      "Train Epoch: 109 [40960/60000 (68%)]\tLoss: 86.120590\tBCE:68.6201\tKLD:17.5005\tC_loss:0.0000\n",
      "Train Epoch: 109 [43520/60000 (73%)]\tLoss: 86.179016\tBCE:68.5172\tKLD:17.6619\tC_loss:0.0000\n",
      "Train Epoch: 109 [46080/60000 (77%)]\tLoss: 86.942001\tBCE:69.6927\tKLD:17.2493\tC_loss:0.0000\n",
      "Train Epoch: 109 [48640/60000 (81%)]\tLoss: 87.390465\tBCE:69.7622\tKLD:17.6282\tC_loss:0.0000\n",
      "Train Epoch: 109 [51200/60000 (85%)]\tLoss: 88.273415\tBCE:70.7348\tKLD:17.5386\tC_loss:0.0000\n",
      "Train Epoch: 109 [53760/60000 (90%)]\tLoss: 87.067604\tBCE:69.5266\tKLD:17.5411\tC_loss:0.0000\n",
      "Train Epoch: 109 [56320/60000 (94%)]\tLoss: 89.007324\tBCE:71.4123\tKLD:17.5950\tC_loss:0.0000\n",
      "Train Epoch: 109 [58880/60000 (98%)]\tLoss: 88.020355\tBCE:70.3736\tKLD:17.6468\tC_loss:0.0000\n",
      "====> Epoch: 109 Average loss: 87.9405\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.1302\n",
      "Train Epoch: 110 [0/60000 (0%)]\tLoss: 87.100281\tBCE:69.2898\tKLD:17.8105\tC_loss:0.0000\n",
      "Train Epoch: 110 [2560/60000 (4%)]\tLoss: 86.743904\tBCE:68.8493\tKLD:17.8946\tC_loss:0.0000\n",
      "Train Epoch: 110 [5120/60000 (9%)]\tLoss: 88.174484\tBCE:70.8169\tKLD:17.3576\tC_loss:0.0000\n",
      "Train Epoch: 110 [7680/60000 (13%)]\tLoss: 87.100090\tBCE:68.4122\tKLD:18.6878\tC_loss:0.0000\n",
      "Train Epoch: 110 [10240/60000 (17%)]\tLoss: 90.951859\tBCE:72.6653\tKLD:18.2866\tC_loss:0.0000\n",
      "Train Epoch: 110 [12800/60000 (21%)]\tLoss: 89.706673\tBCE:71.6756\tKLD:18.0310\tC_loss:0.0000\n",
      "Train Epoch: 110 [15360/60000 (26%)]\tLoss: 89.540070\tBCE:71.4748\tKLD:18.0653\tC_loss:0.0000\n",
      "Train Epoch: 110 [17920/60000 (30%)]\tLoss: 87.086151\tBCE:69.4474\tKLD:17.6387\tC_loss:0.0000\n",
      "Train Epoch: 110 [20480/60000 (34%)]\tLoss: 85.950012\tBCE:68.4595\tKLD:17.4905\tC_loss:0.0000\n",
      "Train Epoch: 110 [23040/60000 (38%)]\tLoss: 87.721756\tBCE:69.8681\tKLD:17.8536\tC_loss:0.0000\n",
      "Train Epoch: 110 [25600/60000 (43%)]\tLoss: 87.734734\tBCE:70.0938\tKLD:17.6409\tC_loss:0.0000\n",
      "Train Epoch: 110 [28160/60000 (47%)]\tLoss: 90.741592\tBCE:73.0632\tKLD:17.6784\tC_loss:0.0000\n",
      "Train Epoch: 110 [30720/60000 (51%)]\tLoss: 88.616661\tBCE:70.3553\tKLD:18.2614\tC_loss:0.0000\n",
      "Train Epoch: 110 [33280/60000 (56%)]\tLoss: 88.504379\tBCE:70.7569\tKLD:17.7475\tC_loss:0.0000\n",
      "Train Epoch: 110 [35840/60000 (60%)]\tLoss: 87.865463\tBCE:70.5677\tKLD:17.2977\tC_loss:0.0000\n",
      "Train Epoch: 110 [38400/60000 (64%)]\tLoss: 89.538925\tBCE:71.1332\tKLD:18.4057\tC_loss:0.0000\n",
      "Train Epoch: 110 [40960/60000 (68%)]\tLoss: 89.168915\tBCE:71.3031\tKLD:17.8658\tC_loss:0.0000\n",
      "Train Epoch: 110 [43520/60000 (73%)]\tLoss: 90.514175\tBCE:72.2918\tKLD:18.2223\tC_loss:0.0000\n",
      "Train Epoch: 110 [46080/60000 (77%)]\tLoss: 91.214012\tBCE:73.2795\tKLD:17.9345\tC_loss:0.0000\n",
      "Train Epoch: 110 [48640/60000 (81%)]\tLoss: 89.049103\tBCE:71.3283\tKLD:17.7208\tC_loss:0.0000\n",
      "Train Epoch: 110 [51200/60000 (85%)]\tLoss: 89.434692\tBCE:71.0650\tKLD:18.3697\tC_loss:0.0000\n",
      "Train Epoch: 110 [53760/60000 (90%)]\tLoss: 90.929230\tBCE:73.4084\tKLD:17.5208\tC_loss:0.0000\n",
      "Train Epoch: 110 [56320/60000 (94%)]\tLoss: 88.139587\tBCE:70.2963\tKLD:17.8433\tC_loss:0.0000\n",
      "Train Epoch: 110 [58880/60000 (98%)]\tLoss: 88.214958\tBCE:70.5818\tKLD:17.6331\tC_loss:0.0000\n",
      "====> Epoch: 110 Average loss: 87.9502\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.0312\n",
      "Random number: 9\n",
      "Train Epoch: 111 [0/60000 (0%)]\tLoss: 85.657410\tBCE:68.1069\tKLD:17.5505\tC_loss:0.0000\n",
      "Train Epoch: 111 [2560/60000 (4%)]\tLoss: 87.837608\tBCE:69.6683\tKLD:18.1693\tC_loss:0.0000\n",
      "Train Epoch: 111 [5120/60000 (9%)]\tLoss: 88.558655\tBCE:70.9472\tKLD:17.6115\tC_loss:0.0000\n",
      "Train Epoch: 111 [7680/60000 (13%)]\tLoss: 86.796692\tBCE:69.0325\tKLD:17.7642\tC_loss:0.0000\n",
      "Train Epoch: 111 [10240/60000 (17%)]\tLoss: 86.279022\tBCE:68.8475\tKLD:17.4315\tC_loss:0.0000\n",
      "Train Epoch: 111 [12800/60000 (21%)]\tLoss: 87.476654\tBCE:70.0524\tKLD:17.4242\tC_loss:0.0000\n",
      "Train Epoch: 111 [15360/60000 (26%)]\tLoss: 88.658043\tBCE:70.1991\tKLD:18.4590\tC_loss:0.0000\n",
      "Train Epoch: 111 [17920/60000 (30%)]\tLoss: 88.299843\tBCE:70.2387\tKLD:18.0611\tC_loss:0.0000\n",
      "Train Epoch: 111 [20480/60000 (34%)]\tLoss: 90.741066\tBCE:73.0170\tKLD:17.7240\tC_loss:0.0000\n",
      "Train Epoch: 111 [23040/60000 (38%)]\tLoss: 90.250961\tBCE:72.0029\tKLD:18.2481\tC_loss:0.0000\n",
      "Train Epoch: 111 [25600/60000 (43%)]\tLoss: 88.470024\tBCE:70.7717\tKLD:17.6983\tC_loss:0.0000\n",
      "Train Epoch: 111 [28160/60000 (47%)]\tLoss: 89.656586\tBCE:71.6653\tKLD:17.9912\tC_loss:0.0000\n",
      "Train Epoch: 111 [30720/60000 (51%)]\tLoss: 87.954460\tBCE:70.2242\tKLD:17.7302\tC_loss:0.0000\n",
      "Train Epoch: 111 [33280/60000 (56%)]\tLoss: 89.643509\tBCE:71.7187\tKLD:17.9249\tC_loss:0.0000\n",
      "Train Epoch: 111 [35840/60000 (60%)]\tLoss: 85.225143\tBCE:67.5500\tKLD:17.6751\tC_loss:0.0000\n",
      "Train Epoch: 111 [38400/60000 (64%)]\tLoss: 87.309402\tBCE:69.5304\tKLD:17.7790\tC_loss:0.0000\n",
      "Train Epoch: 111 [40960/60000 (68%)]\tLoss: 88.700081\tBCE:70.4069\tKLD:18.2932\tC_loss:0.0000\n",
      "Train Epoch: 111 [43520/60000 (73%)]\tLoss: 88.248535\tBCE:70.3694\tKLD:17.8791\tC_loss:0.0000\n",
      "Train Epoch: 111 [46080/60000 (77%)]\tLoss: 86.195183\tBCE:68.7817\tKLD:17.4135\tC_loss:0.0000\n",
      "Train Epoch: 111 [48640/60000 (81%)]\tLoss: 89.432007\tBCE:71.1714\tKLD:18.2606\tC_loss:0.0000\n",
      "Train Epoch: 111 [51200/60000 (85%)]\tLoss: 88.043007\tBCE:70.0037\tKLD:18.0393\tC_loss:0.0000\n",
      "Train Epoch: 111 [53760/60000 (90%)]\tLoss: 89.185822\tBCE:71.3553\tKLD:17.8305\tC_loss:0.0000\n",
      "Train Epoch: 111 [56320/60000 (94%)]\tLoss: 88.300720\tBCE:70.4134\tKLD:17.8873\tC_loss:0.0000\n",
      "Train Epoch: 111 [58880/60000 (98%)]\tLoss: 86.892326\tBCE:69.5087\tKLD:17.3836\tC_loss:0.0000\n",
      "====> Epoch: 111 Average loss: 87.8710\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0612\n",
      "Train Epoch: 112 [0/60000 (0%)]\tLoss: 86.928558\tBCE:69.3716\tKLD:17.5569\tC_loss:0.0000\n",
      "Train Epoch: 112 [2560/60000 (4%)]\tLoss: 86.991257\tBCE:69.7243\tKLD:17.2669\tC_loss:0.0000\n",
      "Train Epoch: 112 [5120/60000 (9%)]\tLoss: 90.834969\tBCE:73.1194\tKLD:17.7156\tC_loss:0.0000\n",
      "Train Epoch: 112 [7680/60000 (13%)]\tLoss: 89.913803\tBCE:71.4029\tKLD:18.5109\tC_loss:0.0000\n",
      "Train Epoch: 112 [10240/60000 (17%)]\tLoss: 89.135284\tBCE:70.9149\tKLD:18.2204\tC_loss:0.0000\n",
      "Train Epoch: 112 [12800/60000 (21%)]\tLoss: 86.854309\tBCE:68.9133\tKLD:17.9410\tC_loss:0.0000\n",
      "Train Epoch: 112 [15360/60000 (26%)]\tLoss: 87.047302\tBCE:69.1569\tKLD:17.8904\tC_loss:0.0000\n",
      "Train Epoch: 112 [17920/60000 (30%)]\tLoss: 88.608299\tBCE:70.9841\tKLD:17.6242\tC_loss:0.0000\n",
      "Train Epoch: 112 [20480/60000 (34%)]\tLoss: 88.847000\tBCE:70.7817\tKLD:18.0653\tC_loss:0.0000\n",
      "Train Epoch: 112 [23040/60000 (38%)]\tLoss: 86.132095\tBCE:68.9874\tKLD:17.1447\tC_loss:0.0000\n",
      "Train Epoch: 112 [25600/60000 (43%)]\tLoss: 85.098251\tBCE:67.8030\tKLD:17.2953\tC_loss:0.0000\n",
      "Train Epoch: 112 [28160/60000 (47%)]\tLoss: 88.473106\tBCE:70.8936\tKLD:17.5795\tC_loss:0.0000\n",
      "Train Epoch: 112 [30720/60000 (51%)]\tLoss: 87.161179\tBCE:69.1715\tKLD:17.9897\tC_loss:0.0000\n",
      "Train Epoch: 112 [33280/60000 (56%)]\tLoss: 89.562271\tBCE:71.7334\tKLD:17.8289\tC_loss:0.0000\n",
      "Train Epoch: 112 [35840/60000 (60%)]\tLoss: 91.547363\tBCE:73.1455\tKLD:18.4019\tC_loss:0.0000\n",
      "Train Epoch: 112 [38400/60000 (64%)]\tLoss: 89.325638\tBCE:70.9215\tKLD:18.4041\tC_loss:0.0000\n",
      "Train Epoch: 112 [40960/60000 (68%)]\tLoss: 89.609543\tBCE:71.9488\tKLD:17.6608\tC_loss:0.0000\n",
      "Train Epoch: 112 [43520/60000 (73%)]\tLoss: 90.195747\tBCE:72.2428\tKLD:17.9529\tC_loss:0.0000\n",
      "Train Epoch: 112 [46080/60000 (77%)]\tLoss: 86.502693\tBCE:69.3781\tKLD:17.1246\tC_loss:0.0000\n",
      "Train Epoch: 112 [48640/60000 (81%)]\tLoss: 89.766182\tBCE:71.4355\tKLD:18.3307\tC_loss:0.0000\n",
      "Train Epoch: 112 [51200/60000 (85%)]\tLoss: 87.413536\tBCE:69.9401\tKLD:17.4734\tC_loss:0.0000\n",
      "Train Epoch: 112 [53760/60000 (90%)]\tLoss: 86.645241\tBCE:68.8344\tKLD:17.8108\tC_loss:0.0000\n",
      "Train Epoch: 112 [56320/60000 (94%)]\tLoss: 85.990875\tBCE:68.9211\tKLD:17.0698\tC_loss:0.0000\n",
      "Train Epoch: 112 [58880/60000 (98%)]\tLoss: 88.831879\tBCE:71.5523\tKLD:17.2795\tC_loss:0.0000\n",
      "====> Epoch: 112 Average loss: 87.8926\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.2887\n",
      "Train Epoch: 113 [0/60000 (0%)]\tLoss: 87.102165\tBCE:69.4782\tKLD:17.6240\tC_loss:0.0000\n",
      "Train Epoch: 113 [2560/60000 (4%)]\tLoss: 87.121017\tBCE:69.6452\tKLD:17.4758\tC_loss:0.0000\n",
      "Train Epoch: 113 [5120/60000 (9%)]\tLoss: 90.375053\tBCE:72.2022\tKLD:18.1728\tC_loss:0.0000\n",
      "Train Epoch: 113 [7680/60000 (13%)]\tLoss: 89.030884\tBCE:71.3698\tKLD:17.6611\tC_loss:0.0000\n",
      "Train Epoch: 113 [10240/60000 (17%)]\tLoss: 87.014023\tBCE:69.8334\tKLD:17.1806\tC_loss:0.0000\n",
      "Train Epoch: 113 [12800/60000 (21%)]\tLoss: 88.476624\tBCE:71.0126\tKLD:17.4641\tC_loss:0.0000\n",
      "Train Epoch: 113 [15360/60000 (26%)]\tLoss: 88.710426\tBCE:70.6662\tKLD:18.0442\tC_loss:0.0000\n",
      "Train Epoch: 113 [17920/60000 (30%)]\tLoss: 88.284866\tBCE:70.3143\tKLD:17.9706\tC_loss:0.0000\n",
      "Train Epoch: 113 [20480/60000 (34%)]\tLoss: 88.098938\tBCE:70.4671\tKLD:17.6319\tC_loss:0.0000\n",
      "Train Epoch: 113 [23040/60000 (38%)]\tLoss: 86.374405\tBCE:68.9769\tKLD:17.3975\tC_loss:0.0000\n",
      "Train Epoch: 113 [25600/60000 (43%)]\tLoss: 85.690140\tBCE:68.6515\tKLD:17.0387\tC_loss:0.0000\n",
      "Train Epoch: 113 [28160/60000 (47%)]\tLoss: 88.900848\tBCE:70.9534\tKLD:17.9474\tC_loss:0.0000\n",
      "Train Epoch: 113 [30720/60000 (51%)]\tLoss: 85.540894\tBCE:68.5923\tKLD:16.9486\tC_loss:0.0000\n",
      "Train Epoch: 113 [33280/60000 (56%)]\tLoss: 86.403862\tBCE:69.1700\tKLD:17.2339\tC_loss:0.0000\n",
      "Train Epoch: 113 [35840/60000 (60%)]\tLoss: 87.183609\tBCE:69.5753\tKLD:17.6083\tC_loss:0.0000\n",
      "Train Epoch: 113 [38400/60000 (64%)]\tLoss: 86.445137\tBCE:68.6292\tKLD:17.8160\tC_loss:0.0000\n",
      "Train Epoch: 113 [40960/60000 (68%)]\tLoss: 88.865654\tBCE:70.5754\tKLD:18.2903\tC_loss:0.0000\n",
      "Train Epoch: 113 [43520/60000 (73%)]\tLoss: 87.951988\tBCE:70.2938\tKLD:17.6582\tC_loss:0.0000\n",
      "Train Epoch: 113 [46080/60000 (77%)]\tLoss: 88.876259\tBCE:70.9090\tKLD:17.9673\tC_loss:0.0000\n",
      "Train Epoch: 113 [48640/60000 (81%)]\tLoss: 89.855019\tBCE:71.9684\tKLD:17.8867\tC_loss:0.0000\n",
      "Train Epoch: 113 [51200/60000 (85%)]\tLoss: 88.200050\tBCE:70.4622\tKLD:17.7379\tC_loss:0.0000\n",
      "Train Epoch: 113 [53760/60000 (90%)]\tLoss: 86.274765\tBCE:68.7516\tKLD:17.5232\tC_loss:0.0000\n",
      "Train Epoch: 113 [56320/60000 (94%)]\tLoss: 88.761070\tBCE:70.9510\tKLD:17.8101\tC_loss:0.0000\n",
      "Train Epoch: 113 [58880/60000 (98%)]\tLoss: 88.081245\tBCE:70.1917\tKLD:17.8895\tC_loss:0.0000\n",
      "====> Epoch: 113 Average loss: 87.7633\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.9963\n",
      "Train Epoch: 114 [0/60000 (0%)]\tLoss: 87.097534\tBCE:69.9993\tKLD:17.0982\tC_loss:0.0000\n",
      "Train Epoch: 114 [2560/60000 (4%)]\tLoss: 86.154846\tBCE:68.4403\tKLD:17.7145\tC_loss:0.0000\n",
      "Train Epoch: 114 [5120/60000 (9%)]\tLoss: 88.027168\tBCE:70.5189\tKLD:17.5083\tC_loss:0.0000\n",
      "Train Epoch: 114 [7680/60000 (13%)]\tLoss: 90.162773\tBCE:72.1202\tKLD:18.0426\tC_loss:0.0000\n",
      "Train Epoch: 114 [10240/60000 (17%)]\tLoss: 88.589691\tBCE:70.8791\tKLD:17.7106\tC_loss:0.0000\n",
      "Train Epoch: 114 [12800/60000 (21%)]\tLoss: 89.876709\tBCE:71.6241\tKLD:18.2526\tC_loss:0.0000\n",
      "Train Epoch: 114 [15360/60000 (26%)]\tLoss: 84.755432\tBCE:67.4389\tKLD:17.3165\tC_loss:0.0000\n",
      "Train Epoch: 114 [17920/60000 (30%)]\tLoss: 88.959869\tBCE:70.6251\tKLD:18.3347\tC_loss:0.0000\n",
      "Train Epoch: 114 [20480/60000 (34%)]\tLoss: 90.006248\tBCE:71.7141\tKLD:18.2921\tC_loss:0.0000\n",
      "Train Epoch: 114 [23040/60000 (38%)]\tLoss: 90.310944\tBCE:72.5089\tKLD:17.8020\tC_loss:0.0000\n",
      "Train Epoch: 114 [25600/60000 (43%)]\tLoss: 89.661385\tBCE:71.8884\tKLD:17.7730\tC_loss:0.0000\n",
      "Train Epoch: 114 [28160/60000 (47%)]\tLoss: 88.129456\tBCE:70.3387\tKLD:17.7907\tC_loss:0.0000\n",
      "Train Epoch: 114 [30720/60000 (51%)]\tLoss: 88.229530\tBCE:70.7385\tKLD:17.4911\tC_loss:0.0000\n",
      "Train Epoch: 114 [33280/60000 (56%)]\tLoss: 85.679932\tBCE:68.3823\tKLD:17.2976\tC_loss:0.0000\n",
      "Train Epoch: 114 [35840/60000 (60%)]\tLoss: 87.877129\tBCE:70.0304\tKLD:17.8468\tC_loss:0.0000\n",
      "Train Epoch: 114 [38400/60000 (64%)]\tLoss: 88.133705\tBCE:70.9338\tKLD:17.1999\tC_loss:0.0000\n",
      "Train Epoch: 114 [40960/60000 (68%)]\tLoss: 88.979980\tBCE:71.1131\tKLD:17.8668\tC_loss:0.0000\n",
      "Train Epoch: 114 [43520/60000 (73%)]\tLoss: 89.817696\tBCE:71.6283\tKLD:18.1894\tC_loss:0.0000\n",
      "Train Epoch: 114 [46080/60000 (77%)]\tLoss: 88.694061\tBCE:71.1504\tKLD:17.5437\tC_loss:0.0000\n",
      "Train Epoch: 114 [48640/60000 (81%)]\tLoss: 88.487961\tBCE:70.6081\tKLD:17.8799\tC_loss:0.0000\n",
      "Train Epoch: 114 [51200/60000 (85%)]\tLoss: 88.918777\tBCE:70.7675\tKLD:18.1513\tC_loss:0.0000\n",
      "Train Epoch: 114 [53760/60000 (90%)]\tLoss: 86.822128\tBCE:69.5912\tKLD:17.2310\tC_loss:0.0000\n",
      "Train Epoch: 114 [56320/60000 (94%)]\tLoss: 86.517281\tBCE:68.9979\tKLD:17.5193\tC_loss:0.0000\n",
      "Train Epoch: 114 [58880/60000 (98%)]\tLoss: 89.315521\tBCE:70.8640\tKLD:18.4515\tC_loss:0.0000\n",
      "====> Epoch: 114 Average loss: 87.8090\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.8583\n",
      "Train Epoch: 115 [0/60000 (0%)]\tLoss: 88.210312\tBCE:70.6167\tKLD:17.5936\tC_loss:0.0000\n",
      "Train Epoch: 115 [2560/60000 (4%)]\tLoss: 88.084305\tBCE:70.3452\tKLD:17.7391\tC_loss:0.0000\n",
      "Train Epoch: 115 [5120/60000 (9%)]\tLoss: 89.173149\tBCE:71.2685\tKLD:17.9046\tC_loss:0.0000\n",
      "Train Epoch: 115 [7680/60000 (13%)]\tLoss: 88.391068\tBCE:70.6900\tKLD:17.7010\tC_loss:0.0000\n",
      "Train Epoch: 115 [10240/60000 (17%)]\tLoss: 88.310020\tBCE:70.3140\tKLD:17.9960\tC_loss:0.0000\n",
      "Train Epoch: 115 [12800/60000 (21%)]\tLoss: 88.509155\tBCE:70.2064\tKLD:18.3027\tC_loss:0.0000\n",
      "Train Epoch: 115 [15360/60000 (26%)]\tLoss: 87.879135\tBCE:70.1724\tKLD:17.7068\tC_loss:0.0000\n",
      "Train Epoch: 115 [17920/60000 (30%)]\tLoss: 84.921288\tBCE:67.6321\tKLD:17.2892\tC_loss:0.0000\n",
      "Train Epoch: 115 [20480/60000 (34%)]\tLoss: 87.502907\tBCE:69.2157\tKLD:18.2872\tC_loss:0.0000\n",
      "Train Epoch: 115 [23040/60000 (38%)]\tLoss: 88.593147\tBCE:70.9019\tKLD:17.6913\tC_loss:0.0000\n",
      "Train Epoch: 115 [25600/60000 (43%)]\tLoss: 91.448593\tBCE:73.7634\tKLD:17.6852\tC_loss:0.0000\n",
      "Train Epoch: 115 [28160/60000 (47%)]\tLoss: 86.359222\tBCE:68.7572\tKLD:17.6021\tC_loss:0.0000\n",
      "Train Epoch: 115 [30720/60000 (51%)]\tLoss: 88.206573\tBCE:70.3968\tKLD:17.8098\tC_loss:0.0000\n",
      "Train Epoch: 115 [33280/60000 (56%)]\tLoss: 89.368439\tBCE:71.9214\tKLD:17.4471\tC_loss:0.0000\n",
      "Train Epoch: 115 [35840/60000 (60%)]\tLoss: 88.719147\tBCE:70.8241\tKLD:17.8950\tC_loss:0.0000\n",
      "Train Epoch: 115 [38400/60000 (64%)]\tLoss: 88.315834\tBCE:70.6067\tKLD:17.7092\tC_loss:0.0000\n",
      "Train Epoch: 115 [40960/60000 (68%)]\tLoss: 85.529526\tBCE:68.0788\tKLD:17.4507\tC_loss:0.0000\n",
      "Train Epoch: 115 [43520/60000 (73%)]\tLoss: 91.197220\tBCE:73.4666\tKLD:17.7306\tC_loss:0.0000\n",
      "Train Epoch: 115 [46080/60000 (77%)]\tLoss: 87.681717\tBCE:69.7076\tKLD:17.9742\tC_loss:0.0000\n",
      "Train Epoch: 115 [48640/60000 (81%)]\tLoss: 87.125168\tBCE:69.3760\tKLD:17.7492\tC_loss:0.0000\n",
      "Train Epoch: 115 [51200/60000 (85%)]\tLoss: 88.964302\tBCE:71.1043\tKLD:17.8600\tC_loss:0.0000\n",
      "Train Epoch: 115 [53760/60000 (90%)]\tLoss: 85.871445\tBCE:68.5450\tKLD:17.3264\tC_loss:0.0000\n",
      "Train Epoch: 115 [56320/60000 (94%)]\tLoss: 86.858391\tBCE:69.4256\tKLD:17.4328\tC_loss:0.0000\n",
      "Train Epoch: 115 [58880/60000 (98%)]\tLoss: 90.167603\tBCE:72.1439\tKLD:18.0237\tC_loss:0.0000\n",
      "====> Epoch: 115 Average loss: 87.8637\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 90.9005\n",
      "Random number: 9\n",
      "Train Epoch: 116 [0/60000 (0%)]\tLoss: 88.791473\tBCE:71.1421\tKLD:17.6494\tC_loss:0.0000\n",
      "Train Epoch: 116 [2560/60000 (4%)]\tLoss: 87.564957\tBCE:69.7689\tKLD:17.7960\tC_loss:0.0000\n",
      "Train Epoch: 116 [5120/60000 (9%)]\tLoss: 88.260559\tBCE:70.2920\tKLD:17.9685\tC_loss:0.0000\n",
      "Train Epoch: 116 [7680/60000 (13%)]\tLoss: 86.752228\tBCE:69.5281\tKLD:17.2241\tC_loss:0.0000\n",
      "Train Epoch: 116 [10240/60000 (17%)]\tLoss: 86.668289\tBCE:69.5636\tKLD:17.1047\tC_loss:0.0000\n",
      "Train Epoch: 116 [12800/60000 (21%)]\tLoss: 90.276047\tBCE:72.0636\tKLD:18.2124\tC_loss:0.0000\n",
      "Train Epoch: 116 [15360/60000 (26%)]\tLoss: 87.859955\tBCE:70.5460\tKLD:17.3139\tC_loss:0.0000\n",
      "Train Epoch: 116 [17920/60000 (30%)]\tLoss: 87.502205\tBCE:69.5202\tKLD:17.9820\tC_loss:0.0000\n",
      "Train Epoch: 116 [20480/60000 (34%)]\tLoss: 85.031860\tBCE:67.7484\tKLD:17.2835\tC_loss:0.0000\n",
      "Train Epoch: 116 [23040/60000 (38%)]\tLoss: 88.016037\tBCE:70.5916\tKLD:17.4244\tC_loss:0.0000\n",
      "Train Epoch: 116 [25600/60000 (43%)]\tLoss: 87.753822\tBCE:69.7182\tKLD:18.0356\tC_loss:0.0000\n",
      "Train Epoch: 116 [28160/60000 (47%)]\tLoss: 87.688690\tBCE:69.8444\tKLD:17.8443\tC_loss:0.0000\n",
      "Train Epoch: 116 [30720/60000 (51%)]\tLoss: 86.427727\tBCE:69.0200\tKLD:17.4077\tC_loss:0.0000\n",
      "Train Epoch: 116 [33280/60000 (56%)]\tLoss: 89.116341\tBCE:71.2308\tKLD:17.8855\tC_loss:0.0000\n",
      "Train Epoch: 116 [35840/60000 (60%)]\tLoss: 88.049690\tBCE:70.0500\tKLD:17.9997\tC_loss:0.0000\n",
      "Train Epoch: 116 [38400/60000 (64%)]\tLoss: 89.141373\tBCE:71.2014\tKLD:17.9399\tC_loss:0.0000\n",
      "Train Epoch: 116 [40960/60000 (68%)]\tLoss: 88.229477\tBCE:70.4781\tKLD:17.7514\tC_loss:0.0000\n",
      "Train Epoch: 116 [43520/60000 (73%)]\tLoss: 88.569122\tBCE:70.5647\tKLD:18.0044\tC_loss:0.0000\n",
      "Train Epoch: 116 [46080/60000 (77%)]\tLoss: 89.398621\tBCE:71.1498\tKLD:18.2488\tC_loss:0.0000\n",
      "Train Epoch: 116 [48640/60000 (81%)]\tLoss: 85.342590\tBCE:68.0190\tKLD:17.3236\tC_loss:0.0000\n",
      "Train Epoch: 116 [51200/60000 (85%)]\tLoss: 86.941971\tBCE:69.6319\tKLD:17.3101\tC_loss:0.0000\n",
      "Train Epoch: 116 [53760/60000 (90%)]\tLoss: 88.362244\tBCE:70.6357\tKLD:17.7265\tC_loss:0.0000\n",
      "Train Epoch: 116 [56320/60000 (94%)]\tLoss: 89.647209\tBCE:72.1483\tKLD:17.4990\tC_loss:0.0000\n",
      "Train Epoch: 116 [58880/60000 (98%)]\tLoss: 88.763016\tBCE:70.7151\tKLD:18.0479\tC_loss:0.0000\n",
      "====> Epoch: 116 Average loss: 87.7204\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0356\n",
      "Train Epoch: 117 [0/60000 (0%)]\tLoss: 87.290863\tBCE:69.4588\tKLD:17.8321\tC_loss:0.0000\n",
      "Train Epoch: 117 [2560/60000 (4%)]\tLoss: 88.085678\tBCE:70.1454\tKLD:17.9403\tC_loss:0.0000\n",
      "Train Epoch: 117 [5120/60000 (9%)]\tLoss: 87.979034\tBCE:70.6232\tKLD:17.3558\tC_loss:0.0000\n",
      "Train Epoch: 117 [7680/60000 (13%)]\tLoss: 88.034805\tBCE:69.5612\tKLD:18.4736\tC_loss:0.0000\n",
      "Train Epoch: 117 [10240/60000 (17%)]\tLoss: 88.853531\tBCE:70.8486\tKLD:18.0049\tC_loss:0.0000\n",
      "Train Epoch: 117 [12800/60000 (21%)]\tLoss: 85.615585\tBCE:68.1475\tKLD:17.4681\tC_loss:0.0000\n",
      "Train Epoch: 117 [15360/60000 (26%)]\tLoss: 88.456291\tBCE:70.6963\tKLD:17.7600\tC_loss:0.0000\n",
      "Train Epoch: 117 [17920/60000 (30%)]\tLoss: 89.312569\tBCE:71.6254\tKLD:17.6872\tC_loss:0.0000\n",
      "Train Epoch: 117 [20480/60000 (34%)]\tLoss: 90.474762\tBCE:72.3733\tKLD:18.1015\tC_loss:0.0000\n",
      "Train Epoch: 117 [23040/60000 (38%)]\tLoss: 86.868225\tBCE:69.4559\tKLD:17.4123\tC_loss:0.0000\n",
      "Train Epoch: 117 [25600/60000 (43%)]\tLoss: 89.580032\tBCE:71.3965\tKLD:18.1836\tC_loss:0.0000\n",
      "Train Epoch: 117 [28160/60000 (47%)]\tLoss: 86.730453\tBCE:69.4458\tKLD:17.2847\tC_loss:0.0000\n",
      "Train Epoch: 117 [30720/60000 (51%)]\tLoss: 86.545815\tBCE:68.8225\tKLD:17.7233\tC_loss:0.0000\n",
      "Train Epoch: 117 [33280/60000 (56%)]\tLoss: 86.450966\tBCE:68.8502\tKLD:17.6007\tC_loss:0.0000\n",
      "Train Epoch: 117 [35840/60000 (60%)]\tLoss: 86.491119\tBCE:69.1164\tKLD:17.3747\tC_loss:0.0000\n",
      "Train Epoch: 117 [38400/60000 (64%)]\tLoss: 87.545509\tBCE:69.7279\tKLD:17.8176\tC_loss:0.0000\n",
      "Train Epoch: 117 [40960/60000 (68%)]\tLoss: 89.811844\tBCE:71.9965\tKLD:17.8153\tC_loss:0.0000\n",
      "Train Epoch: 117 [43520/60000 (73%)]\tLoss: 89.319237\tBCE:71.2822\tKLD:18.0371\tC_loss:0.0000\n",
      "Train Epoch: 117 [46080/60000 (77%)]\tLoss: 88.603844\tBCE:70.3792\tKLD:18.2246\tC_loss:0.0000\n",
      "Train Epoch: 117 [48640/60000 (81%)]\tLoss: 88.372894\tBCE:70.4165\tKLD:17.9564\tC_loss:0.0000\n",
      "Train Epoch: 117 [51200/60000 (85%)]\tLoss: 85.932770\tBCE:68.3323\tKLD:17.6004\tC_loss:0.0000\n",
      "Train Epoch: 117 [53760/60000 (90%)]\tLoss: 86.976151\tBCE:69.7065\tKLD:17.2696\tC_loss:0.0000\n",
      "Train Epoch: 117 [56320/60000 (94%)]\tLoss: 88.273758\tBCE:70.3123\tKLD:17.9615\tC_loss:0.0000\n",
      "Train Epoch: 117 [58880/60000 (98%)]\tLoss: 86.885902\tBCE:69.3638\tKLD:17.5222\tC_loss:0.0000\n",
      "====> Epoch: 117 Average loss: 87.7153\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.1661\n",
      "Train Epoch: 118 [0/60000 (0%)]\tLoss: 88.557983\tBCE:71.1442\tKLD:17.4137\tC_loss:0.0000\n",
      "Train Epoch: 118 [2560/60000 (4%)]\tLoss: 88.531303\tBCE:71.1337\tKLD:17.3976\tC_loss:0.0000\n",
      "Train Epoch: 118 [5120/60000 (9%)]\tLoss: 87.660942\tBCE:69.9570\tKLD:17.7039\tC_loss:0.0000\n",
      "Train Epoch: 118 [7680/60000 (13%)]\tLoss: 87.582886\tBCE:69.8008\tKLD:17.7821\tC_loss:0.0000\n",
      "Train Epoch: 118 [10240/60000 (17%)]\tLoss: 88.095810\tBCE:70.2779\tKLD:17.8179\tC_loss:0.0000\n",
      "Train Epoch: 118 [12800/60000 (21%)]\tLoss: 87.958832\tBCE:70.3041\tKLD:17.6547\tC_loss:0.0000\n",
      "Train Epoch: 118 [15360/60000 (26%)]\tLoss: 85.322136\tBCE:67.7275\tKLD:17.5946\tC_loss:0.0000\n",
      "Train Epoch: 118 [17920/60000 (30%)]\tLoss: 87.805969\tBCE:69.8824\tKLD:17.9236\tC_loss:0.0000\n",
      "Train Epoch: 118 [20480/60000 (34%)]\tLoss: 87.814560\tBCE:70.0365\tKLD:17.7781\tC_loss:0.0000\n",
      "Train Epoch: 118 [23040/60000 (38%)]\tLoss: 87.408401\tBCE:69.3996\tKLD:18.0088\tC_loss:0.0000\n",
      "Train Epoch: 118 [25600/60000 (43%)]\tLoss: 86.212868\tBCE:68.9426\tKLD:17.2703\tC_loss:0.0000\n",
      "Train Epoch: 118 [28160/60000 (47%)]\tLoss: 88.352829\tBCE:70.4623\tKLD:17.8905\tC_loss:0.0000\n",
      "Train Epoch: 118 [30720/60000 (51%)]\tLoss: 91.246498\tBCE:72.5722\tKLD:18.6743\tC_loss:0.0000\n",
      "Train Epoch: 118 [33280/60000 (56%)]\tLoss: 88.097412\tBCE:70.0629\tKLD:18.0345\tC_loss:0.0000\n",
      "Train Epoch: 118 [35840/60000 (60%)]\tLoss: 90.469101\tBCE:72.0143\tKLD:18.4548\tC_loss:0.0000\n",
      "Train Epoch: 118 [38400/60000 (64%)]\tLoss: 90.062912\tBCE:71.6271\tKLD:18.4358\tC_loss:0.0000\n",
      "Train Epoch: 118 [40960/60000 (68%)]\tLoss: 88.140182\tBCE:70.0977\tKLD:18.0425\tC_loss:0.0000\n",
      "Train Epoch: 118 [43520/60000 (73%)]\tLoss: 85.722923\tBCE:68.3477\tKLD:17.3753\tC_loss:0.0000\n",
      "Train Epoch: 118 [46080/60000 (77%)]\tLoss: 89.956795\tBCE:71.9762\tKLD:17.9806\tC_loss:0.0000\n",
      "Train Epoch: 118 [48640/60000 (81%)]\tLoss: 86.156677\tBCE:69.0526\tKLD:17.1041\tC_loss:0.0000\n",
      "Train Epoch: 118 [51200/60000 (85%)]\tLoss: 88.162300\tBCE:70.4223\tKLD:17.7400\tC_loss:0.0000\n",
      "Train Epoch: 118 [53760/60000 (90%)]\tLoss: 87.265961\tBCE:69.8435\tKLD:17.4225\tC_loss:0.0000\n",
      "Train Epoch: 118 [56320/60000 (94%)]\tLoss: 87.830681\tBCE:70.4595\tKLD:17.3711\tC_loss:0.0000\n",
      "Train Epoch: 118 [58880/60000 (98%)]\tLoss: 88.739288\tBCE:70.9491\tKLD:17.7902\tC_loss:0.0000\n",
      "====> Epoch: 118 Average loss: 87.7652\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.2549\n",
      "Train Epoch: 119 [0/60000 (0%)]\tLoss: 87.280762\tBCE:69.3852\tKLD:17.8955\tC_loss:0.0000\n",
      "Train Epoch: 119 [2560/60000 (4%)]\tLoss: 84.962700\tBCE:67.5274\tKLD:17.4353\tC_loss:0.0000\n",
      "Train Epoch: 119 [5120/60000 (9%)]\tLoss: 87.134682\tBCE:69.4772\tKLD:17.6575\tC_loss:0.0000\n",
      "Train Epoch: 119 [7680/60000 (13%)]\tLoss: 90.147972\tBCE:72.2086\tKLD:17.9394\tC_loss:0.0000\n",
      "Train Epoch: 119 [10240/60000 (17%)]\tLoss: 89.125519\tBCE:70.7762\tKLD:18.3493\tC_loss:0.0000\n",
      "Train Epoch: 119 [12800/60000 (21%)]\tLoss: 89.609802\tBCE:71.4504\tKLD:18.1594\tC_loss:0.0000\n",
      "Train Epoch: 119 [15360/60000 (26%)]\tLoss: 85.549713\tBCE:68.1012\tKLD:17.4485\tC_loss:0.0000\n",
      "Train Epoch: 119 [17920/60000 (30%)]\tLoss: 89.877686\tBCE:71.5817\tKLD:18.2959\tC_loss:0.0000\n",
      "Train Epoch: 119 [20480/60000 (34%)]\tLoss: 88.602455\tBCE:70.6311\tKLD:17.9714\tC_loss:0.0000\n",
      "Train Epoch: 119 [23040/60000 (38%)]\tLoss: 88.757050\tBCE:70.3897\tKLD:18.3673\tC_loss:0.0000\n",
      "Train Epoch: 119 [25600/60000 (43%)]\tLoss: 88.837357\tBCE:71.0782\tKLD:17.7592\tC_loss:0.0000\n",
      "Train Epoch: 119 [28160/60000 (47%)]\tLoss: 86.771729\tBCE:68.9483\tKLD:17.8234\tC_loss:0.0000\n",
      "Train Epoch: 119 [30720/60000 (51%)]\tLoss: 91.113083\tBCE:72.7725\tKLD:18.3406\tC_loss:0.0000\n",
      "Train Epoch: 119 [33280/60000 (56%)]\tLoss: 86.549225\tBCE:69.2549\tKLD:17.2943\tC_loss:0.0000\n",
      "Train Epoch: 119 [35840/60000 (60%)]\tLoss: 85.574799\tBCE:67.9333\tKLD:17.6415\tC_loss:0.0000\n",
      "Train Epoch: 119 [38400/60000 (64%)]\tLoss: 89.718483\tBCE:71.5211\tKLD:18.1974\tC_loss:0.0000\n",
      "Train Epoch: 119 [40960/60000 (68%)]\tLoss: 86.845688\tBCE:69.1142\tKLD:17.7315\tC_loss:0.0000\n",
      "Train Epoch: 119 [43520/60000 (73%)]\tLoss: 89.285568\tBCE:71.3547\tKLD:17.9309\tC_loss:0.0000\n",
      "Train Epoch: 119 [46080/60000 (77%)]\tLoss: 88.032959\tBCE:70.5065\tKLD:17.5265\tC_loss:0.0000\n",
      "Train Epoch: 119 [48640/60000 (81%)]\tLoss: 89.267120\tBCE:71.3089\tKLD:17.9582\tC_loss:0.0000\n",
      "Train Epoch: 119 [51200/60000 (85%)]\tLoss: 88.956253\tBCE:71.3065\tKLD:17.6497\tC_loss:0.0000\n",
      "Train Epoch: 119 [53760/60000 (90%)]\tLoss: 85.780571\tBCE:68.2325\tKLD:17.5480\tC_loss:0.0000\n",
      "Train Epoch: 119 [56320/60000 (94%)]\tLoss: 90.048721\tBCE:72.0370\tKLD:18.0117\tC_loss:0.0000\n",
      "Train Epoch: 119 [58880/60000 (98%)]\tLoss: 87.550240\tBCE:70.0826\tKLD:17.4676\tC_loss:0.0000\n",
      "====> Epoch: 119 Average loss: 87.7701\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.1164\n",
      "Train Epoch: 120 [0/60000 (0%)]\tLoss: 86.075836\tBCE:68.7189\tKLD:17.3570\tC_loss:0.0000\n",
      "Train Epoch: 120 [2560/60000 (4%)]\tLoss: 87.609123\tBCE:69.8316\tKLD:17.7775\tC_loss:0.0000\n",
      "Train Epoch: 120 [5120/60000 (9%)]\tLoss: 86.980682\tBCE:69.4223\tKLD:17.5584\tC_loss:0.0000\n",
      "Train Epoch: 120 [7680/60000 (13%)]\tLoss: 88.105072\tBCE:70.6614\tKLD:17.4437\tC_loss:0.0000\n",
      "Train Epoch: 120 [10240/60000 (17%)]\tLoss: 87.400574\tBCE:69.9187\tKLD:17.4819\tC_loss:0.0000\n",
      "Train Epoch: 120 [12800/60000 (21%)]\tLoss: 87.539330\tBCE:69.5766\tKLD:17.9628\tC_loss:0.0000\n",
      "Train Epoch: 120 [15360/60000 (26%)]\tLoss: 88.607826\tBCE:70.7796\tKLD:17.8282\tC_loss:0.0000\n",
      "Train Epoch: 120 [17920/60000 (30%)]\tLoss: 86.031914\tBCE:68.4956\tKLD:17.5363\tC_loss:0.0000\n",
      "Train Epoch: 120 [20480/60000 (34%)]\tLoss: 86.435532\tBCE:68.6801\tKLD:17.7555\tC_loss:0.0000\n",
      "Train Epoch: 120 [23040/60000 (38%)]\tLoss: 86.625267\tBCE:68.6489\tKLD:17.9764\tC_loss:0.0000\n",
      "Train Epoch: 120 [25600/60000 (43%)]\tLoss: 88.773895\tBCE:70.9227\tKLD:17.8512\tC_loss:0.0000\n",
      "Train Epoch: 120 [28160/60000 (47%)]\tLoss: 88.323090\tBCE:70.6733\tKLD:17.6497\tC_loss:0.0000\n",
      "Train Epoch: 120 [30720/60000 (51%)]\tLoss: 89.803299\tBCE:71.7240\tKLD:18.0793\tC_loss:0.0000\n",
      "Train Epoch: 120 [33280/60000 (56%)]\tLoss: 87.697731\tBCE:70.0358\tKLD:17.6620\tC_loss:0.0000\n",
      "Train Epoch: 120 [35840/60000 (60%)]\tLoss: 87.021278\tBCE:69.2324\tKLD:17.7889\tC_loss:0.0000\n",
      "Train Epoch: 120 [38400/60000 (64%)]\tLoss: 89.243530\tBCE:71.7326\tKLD:17.5109\tC_loss:0.0000\n",
      "Train Epoch: 120 [40960/60000 (68%)]\tLoss: 85.444122\tBCE:68.4248\tKLD:17.0193\tC_loss:0.0000\n",
      "Train Epoch: 120 [43520/60000 (73%)]\tLoss: 87.959793\tBCE:70.3988\tKLD:17.5609\tC_loss:0.0000\n",
      "Train Epoch: 120 [46080/60000 (77%)]\tLoss: 89.275780\tBCE:71.0299\tKLD:18.2458\tC_loss:0.0000\n",
      "Train Epoch: 120 [48640/60000 (81%)]\tLoss: 86.066559\tBCE:68.7948\tKLD:17.2718\tC_loss:0.0000\n",
      "Train Epoch: 120 [51200/60000 (85%)]\tLoss: 86.041634\tBCE:68.6671\tKLD:17.3745\tC_loss:0.0000\n",
      "Train Epoch: 120 [53760/60000 (90%)]\tLoss: 89.795120\tBCE:72.0008\tKLD:17.7943\tC_loss:0.0000\n",
      "Train Epoch: 120 [56320/60000 (94%)]\tLoss: 89.019363\tBCE:70.9690\tKLD:18.0503\tC_loss:0.0000\n",
      "Train Epoch: 120 [58880/60000 (98%)]\tLoss: 87.325272\tBCE:69.6438\tKLD:17.6815\tC_loss:0.0000\n",
      "====> Epoch: 120 Average loss: 87.6727\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.3786\n",
      "Random number: 0\n",
      "Train Epoch: 121 [0/60000 (0%)]\tLoss: 87.803345\tBCE:69.8576\tKLD:17.9458\tC_loss:0.0000\n",
      "Train Epoch: 121 [2560/60000 (4%)]\tLoss: 89.323273\tBCE:71.1287\tKLD:18.1945\tC_loss:0.0000\n",
      "Train Epoch: 121 [5120/60000 (9%)]\tLoss: 86.753380\tBCE:68.9086\tKLD:17.8448\tC_loss:0.0000\n",
      "Train Epoch: 121 [7680/60000 (13%)]\tLoss: 87.067123\tBCE:69.3652\tKLD:17.7019\tC_loss:0.0000\n",
      "Train Epoch: 121 [10240/60000 (17%)]\tLoss: 89.497017\tBCE:71.4904\tKLD:18.0066\tC_loss:0.0000\n",
      "Train Epoch: 121 [12800/60000 (21%)]\tLoss: 88.681747\tBCE:71.1600\tKLD:17.5217\tC_loss:0.0000\n",
      "Train Epoch: 121 [15360/60000 (26%)]\tLoss: 86.822830\tBCE:68.9411\tKLD:17.8818\tC_loss:0.0000\n",
      "Train Epoch: 121 [17920/60000 (30%)]\tLoss: 87.046692\tBCE:69.0819\tKLD:17.9648\tC_loss:0.0000\n",
      "Train Epoch: 121 [20480/60000 (34%)]\tLoss: 84.979843\tBCE:67.6142\tKLD:17.3657\tC_loss:0.0000\n",
      "Train Epoch: 121 [23040/60000 (38%)]\tLoss: 89.866364\tBCE:71.5342\tKLD:18.3322\tC_loss:0.0000\n",
      "Train Epoch: 121 [25600/60000 (43%)]\tLoss: 87.834435\tBCE:69.7360\tKLD:18.0985\tC_loss:0.0000\n",
      "Train Epoch: 121 [28160/60000 (47%)]\tLoss: 86.279633\tBCE:68.4465\tKLD:17.8331\tC_loss:0.0000\n",
      "Train Epoch: 121 [30720/60000 (51%)]\tLoss: 86.728333\tBCE:69.1549\tKLD:17.5734\tC_loss:0.0000\n",
      "Train Epoch: 121 [33280/60000 (56%)]\tLoss: 89.604744\tBCE:71.8860\tKLD:17.7187\tC_loss:0.0000\n",
      "Train Epoch: 121 [35840/60000 (60%)]\tLoss: 88.881409\tBCE:70.5960\tKLD:18.2854\tC_loss:0.0000\n",
      "Train Epoch: 121 [38400/60000 (64%)]\tLoss: 88.871094\tBCE:71.3972\tKLD:17.4739\tC_loss:0.0000\n",
      "Train Epoch: 121 [40960/60000 (68%)]\tLoss: 87.706757\tBCE:69.4750\tKLD:18.2318\tC_loss:0.0000\n",
      "Train Epoch: 121 [43520/60000 (73%)]\tLoss: 89.887886\tBCE:71.9009\tKLD:17.9870\tC_loss:0.0000\n",
      "Train Epoch: 121 [46080/60000 (77%)]\tLoss: 86.387070\tBCE:68.6392\tKLD:17.7479\tC_loss:0.0000\n",
      "Train Epoch: 121 [48640/60000 (81%)]\tLoss: 88.196472\tBCE:70.8877\tKLD:17.3088\tC_loss:0.0000\n",
      "Train Epoch: 121 [51200/60000 (85%)]\tLoss: 87.794861\tBCE:70.4083\tKLD:17.3866\tC_loss:0.0000\n",
      "Train Epoch: 121 [53760/60000 (90%)]\tLoss: 91.219025\tBCE:73.0595\tKLD:18.1595\tC_loss:0.0000\n",
      "Train Epoch: 121 [56320/60000 (94%)]\tLoss: 88.427338\tBCE:70.6358\tKLD:17.7915\tC_loss:0.0000\n",
      "Train Epoch: 121 [58880/60000 (98%)]\tLoss: 87.638245\tBCE:69.9040\tKLD:17.7342\tC_loss:0.0000\n",
      "====> Epoch: 121 Average loss: 87.7089\tClassifier Accuracy: 99.9382\n",
      "====> Test set loss: 91.4026\n",
      "Train Epoch: 122 [0/60000 (0%)]\tLoss: 88.008606\tBCE:69.8876\tKLD:18.1210\tC_loss:0.0001\n",
      "Train Epoch: 122 [2560/60000 (4%)]\tLoss: 87.097870\tBCE:69.6802\tKLD:17.4177\tC_loss:0.0000\n",
      "Train Epoch: 122 [5120/60000 (9%)]\tLoss: 86.682571\tBCE:69.2209\tKLD:17.4617\tC_loss:0.0000\n",
      "Train Epoch: 122 [7680/60000 (13%)]\tLoss: 85.321648\tBCE:67.6414\tKLD:17.6802\tC_loss:0.0000\n",
      "Train Epoch: 122 [10240/60000 (17%)]\tLoss: 87.809601\tBCE:70.1205\tKLD:17.6890\tC_loss:0.0000\n",
      "Train Epoch: 122 [12800/60000 (21%)]\tLoss: 85.860626\tBCE:68.6034\tKLD:17.2572\tC_loss:0.0000\n",
      "Train Epoch: 122 [15360/60000 (26%)]\tLoss: 87.305420\tBCE:69.6334\tKLD:17.6720\tC_loss:0.0000\n",
      "Train Epoch: 122 [17920/60000 (30%)]\tLoss: 88.284218\tBCE:70.4552\tKLD:17.8290\tC_loss:0.0000\n",
      "Train Epoch: 122 [20480/60000 (34%)]\tLoss: 85.648163\tBCE:68.2600\tKLD:17.3881\tC_loss:0.0000\n",
      "Train Epoch: 122 [23040/60000 (38%)]\tLoss: 89.406082\tBCE:71.0774\tKLD:18.3287\tC_loss:0.0000\n",
      "Train Epoch: 122 [25600/60000 (43%)]\tLoss: 87.911064\tBCE:70.0890\tKLD:17.8220\tC_loss:0.0000\n",
      "Train Epoch: 122 [28160/60000 (47%)]\tLoss: 87.637619\tBCE:69.9005\tKLD:17.7372\tC_loss:0.0000\n",
      "Train Epoch: 122 [30720/60000 (51%)]\tLoss: 88.191925\tBCE:70.5415\tKLD:17.6504\tC_loss:0.0000\n",
      "Train Epoch: 122 [33280/60000 (56%)]\tLoss: 87.710274\tBCE:70.1424\tKLD:17.5678\tC_loss:0.0000\n",
      "Train Epoch: 122 [35840/60000 (60%)]\tLoss: 87.849342\tBCE:69.9565\tKLD:17.8929\tC_loss:0.0000\n",
      "Train Epoch: 122 [38400/60000 (64%)]\tLoss: 86.512199\tBCE:69.3503\tKLD:17.1619\tC_loss:0.0000\n",
      "Train Epoch: 122 [40960/60000 (68%)]\tLoss: 91.608047\tBCE:73.1530\tKLD:18.4550\tC_loss:0.0000\n",
      "Train Epoch: 122 [43520/60000 (73%)]\tLoss: 87.456734\tBCE:69.7024\tKLD:17.7543\tC_loss:0.0000\n",
      "Train Epoch: 122 [46080/60000 (77%)]\tLoss: 84.952682\tBCE:67.6148\tKLD:17.3379\tC_loss:0.0000\n",
      "Train Epoch: 122 [48640/60000 (81%)]\tLoss: 88.987549\tBCE:71.0257\tKLD:17.9619\tC_loss:0.0000\n",
      "Train Epoch: 122 [51200/60000 (85%)]\tLoss: 89.101990\tBCE:70.9910\tKLD:18.1110\tC_loss:0.0000\n",
      "Train Epoch: 122 [53760/60000 (90%)]\tLoss: 87.493332\tBCE:69.9370\tKLD:17.5563\tC_loss:0.0000\n",
      "Train Epoch: 122 [56320/60000 (94%)]\tLoss: 87.161530\tBCE:69.5885\tKLD:17.5730\tC_loss:0.0000\n",
      "Train Epoch: 122 [58880/60000 (98%)]\tLoss: 88.263885\tBCE:70.4942\tKLD:17.7696\tC_loss:0.0000\n",
      "====> Epoch: 122 Average loss: 87.6827\tClassifier Accuracy: 99.5994\n",
      "====> Test set loss: 92.6036\n",
      "Train Epoch: 123 [0/60000 (0%)]\tLoss: 88.219604\tBCE:70.6908\tKLD:17.5288\tC_loss:0.0000\n",
      "Train Epoch: 123 [2560/60000 (4%)]\tLoss: 86.302856\tBCE:68.8558\tKLD:17.4471\tC_loss:0.0000\n",
      "Train Epoch: 123 [5120/60000 (9%)]\tLoss: 87.991364\tBCE:70.0201\tKLD:17.9712\tC_loss:0.0000\n",
      "Train Epoch: 123 [7680/60000 (13%)]\tLoss: 92.644928\tBCE:74.4867\tKLD:18.1582\tC_loss:0.0000\n",
      "Train Epoch: 123 [10240/60000 (17%)]\tLoss: 88.440155\tBCE:70.1926\tKLD:18.2475\tC_loss:0.0000\n",
      "Train Epoch: 123 [12800/60000 (21%)]\tLoss: 88.674759\tBCE:70.6079\tKLD:18.0669\tC_loss:0.0000\n",
      "Train Epoch: 123 [15360/60000 (26%)]\tLoss: 88.675674\tBCE:70.7015\tKLD:17.9742\tC_loss:0.0000\n",
      "Train Epoch: 123 [17920/60000 (30%)]\tLoss: 87.362106\tBCE:69.3891\tKLD:17.9730\tC_loss:0.0000\n",
      "Train Epoch: 123 [20480/60000 (34%)]\tLoss: 87.500732\tBCE:69.9376\tKLD:17.5632\tC_loss:0.0000\n",
      "Train Epoch: 123 [23040/60000 (38%)]\tLoss: 86.759628\tBCE:69.0396\tKLD:17.7200\tC_loss:0.0000\n",
      "Train Epoch: 123 [25600/60000 (43%)]\tLoss: 86.970757\tBCE:69.4746\tKLD:17.4962\tC_loss:0.0000\n",
      "Train Epoch: 123 [28160/60000 (47%)]\tLoss: 89.116882\tBCE:71.2635\tKLD:17.8534\tC_loss:0.0000\n",
      "Train Epoch: 123 [30720/60000 (51%)]\tLoss: 86.749619\tBCE:68.9245\tKLD:17.8251\tC_loss:0.0000\n",
      "Train Epoch: 123 [33280/60000 (56%)]\tLoss: 91.352982\tBCE:73.0964\tKLD:18.2566\tC_loss:0.0000\n",
      "Train Epoch: 123 [35840/60000 (60%)]\tLoss: 88.279022\tBCE:70.0534\tKLD:18.2257\tC_loss:0.0000\n",
      "Train Epoch: 123 [38400/60000 (64%)]\tLoss: 87.558250\tBCE:70.1138\tKLD:17.4444\tC_loss:0.0000\n",
      "Train Epoch: 123 [40960/60000 (68%)]\tLoss: 90.712906\tBCE:72.7460\tKLD:17.9669\tC_loss:0.0000\n",
      "Train Epoch: 123 [43520/60000 (73%)]\tLoss: 88.410881\tBCE:70.0728\tKLD:18.3381\tC_loss:0.0000\n",
      "Train Epoch: 123 [46080/60000 (77%)]\tLoss: 87.015602\tBCE:69.4849\tKLD:17.5307\tC_loss:0.0000\n",
      "Train Epoch: 123 [48640/60000 (81%)]\tLoss: 89.960342\tBCE:72.1221\tKLD:17.8383\tC_loss:0.0000\n",
      "Train Epoch: 123 [51200/60000 (85%)]\tLoss: 88.980942\tBCE:71.0505\tKLD:17.9305\tC_loss:0.0000\n",
      "Train Epoch: 123 [53760/60000 (90%)]\tLoss: 90.593521\tBCE:72.3780\tKLD:18.2155\tC_loss:0.0000\n",
      "Train Epoch: 123 [56320/60000 (94%)]\tLoss: 90.575272\tBCE:72.5010\tKLD:18.0743\tC_loss:0.0000\n",
      "Train Epoch: 123 [58880/60000 (98%)]\tLoss: 85.277351\tBCE:68.0105\tKLD:17.2669\tC_loss:0.0000\n",
      "====> Epoch: 123 Average loss: 87.6446\tClassifier Accuracy: 99.9816\n",
      "====> Test set loss: 90.9309\n",
      "Train Epoch: 124 [0/60000 (0%)]\tLoss: 87.912605\tBCE:70.0406\tKLD:17.8720\tC_loss:0.0000\n",
      "Train Epoch: 124 [2560/60000 (4%)]\tLoss: 87.056183\tBCE:69.2565\tKLD:17.7997\tC_loss:0.0000\n",
      "Train Epoch: 124 [5120/60000 (9%)]\tLoss: 82.366226\tBCE:65.4062\tKLD:16.9600\tC_loss:0.0000\n",
      "Train Epoch: 124 [7680/60000 (13%)]\tLoss: 85.697800\tBCE:67.9193\tKLD:17.7785\tC_loss:0.0000\n",
      "Train Epoch: 124 [10240/60000 (17%)]\tLoss: 87.386269\tBCE:69.5003\tKLD:17.8860\tC_loss:0.0000\n",
      "Train Epoch: 124 [12800/60000 (21%)]\tLoss: 88.003090\tBCE:69.9700\tKLD:18.0331\tC_loss:0.0000\n",
      "Train Epoch: 124 [15360/60000 (26%)]\tLoss: 89.294678\tBCE:71.4581\tKLD:17.8366\tC_loss:0.0000\n",
      "Train Epoch: 124 [17920/60000 (30%)]\tLoss: 85.808296\tBCE:67.9652\tKLD:17.8431\tC_loss:0.0000\n",
      "Train Epoch: 124 [20480/60000 (34%)]\tLoss: 87.800598\tBCE:70.0983\tKLD:17.7023\tC_loss:0.0000\n",
      "Train Epoch: 124 [23040/60000 (38%)]\tLoss: 87.307037\tBCE:70.0118\tKLD:17.2952\tC_loss:0.0000\n",
      "Train Epoch: 124 [25600/60000 (43%)]\tLoss: 88.573395\tBCE:70.7713\tKLD:17.8021\tC_loss:0.0000\n",
      "Train Epoch: 124 [28160/60000 (47%)]\tLoss: 85.077873\tBCE:67.6652\tKLD:17.4127\tC_loss:0.0000\n",
      "Train Epoch: 124 [30720/60000 (51%)]\tLoss: 86.671974\tBCE:69.0977\tKLD:17.5743\tC_loss:0.0000\n",
      "Train Epoch: 124 [33280/60000 (56%)]\tLoss: 90.179993\tBCE:72.0075\tKLD:18.1725\tC_loss:0.0000\n",
      "Train Epoch: 124 [35840/60000 (60%)]\tLoss: 85.985443\tBCE:68.6101\tKLD:17.3753\tC_loss:0.0000\n",
      "Train Epoch: 124 [38400/60000 (64%)]\tLoss: 87.792068\tBCE:70.2872\tKLD:17.5049\tC_loss:0.0000\n",
      "Train Epoch: 124 [40960/60000 (68%)]\tLoss: 88.580635\tBCE:70.9347\tKLD:17.6460\tC_loss:0.0000\n",
      "Train Epoch: 124 [43520/60000 (73%)]\tLoss: 87.009949\tBCE:69.5042\tKLD:17.5057\tC_loss:0.0000\n",
      "Train Epoch: 124 [46080/60000 (77%)]\tLoss: 86.917831\tBCE:69.7665\tKLD:17.1513\tC_loss:0.0000\n",
      "Train Epoch: 124 [48640/60000 (81%)]\tLoss: 88.304085\tBCE:69.9656\tKLD:18.3385\tC_loss:0.0000\n",
      "Train Epoch: 124 [51200/60000 (85%)]\tLoss: 88.572464\tBCE:70.5295\tKLD:18.0430\tC_loss:0.0000\n",
      "Train Epoch: 124 [53760/60000 (90%)]\tLoss: 89.489563\tBCE:71.0683\tKLD:18.4213\tC_loss:0.0000\n",
      "Train Epoch: 124 [56320/60000 (94%)]\tLoss: 87.498375\tBCE:69.4030\tKLD:18.0954\tC_loss:0.0000\n",
      "Train Epoch: 124 [58880/60000 (98%)]\tLoss: 87.590546\tBCE:69.6781\tKLD:17.9124\tC_loss:0.0000\n",
      "====> Epoch: 124 Average loss: 87.5452\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.4187\n",
      "Train Epoch: 125 [0/60000 (0%)]\tLoss: 88.834503\tBCE:71.0429\tKLD:17.7917\tC_loss:0.0000\n",
      "Train Epoch: 125 [2560/60000 (4%)]\tLoss: 86.770615\tBCE:69.3994\tKLD:17.3712\tC_loss:0.0000\n",
      "Train Epoch: 125 [5120/60000 (9%)]\tLoss: 86.073868\tBCE:68.8042\tKLD:17.2697\tC_loss:0.0000\n",
      "Train Epoch: 125 [7680/60000 (13%)]\tLoss: 85.526337\tBCE:67.6352\tKLD:17.8911\tC_loss:0.0000\n",
      "Train Epoch: 125 [10240/60000 (17%)]\tLoss: 86.807709\tBCE:69.5605\tKLD:17.2472\tC_loss:0.0000\n",
      "Train Epoch: 125 [12800/60000 (21%)]\tLoss: 87.466660\tBCE:69.6121\tKLD:17.8546\tC_loss:0.0000\n",
      "Train Epoch: 125 [15360/60000 (26%)]\tLoss: 87.688873\tBCE:69.8839\tKLD:17.8049\tC_loss:0.0000\n",
      "Train Epoch: 125 [17920/60000 (30%)]\tLoss: 87.730820\tBCE:70.3438\tKLD:17.3870\tC_loss:0.0000\n",
      "Train Epoch: 125 [20480/60000 (34%)]\tLoss: 88.984818\tBCE:71.4264\tKLD:17.5584\tC_loss:0.0000\n",
      "Train Epoch: 125 [23040/60000 (38%)]\tLoss: 85.734879\tBCE:67.9661\tKLD:17.7688\tC_loss:0.0000\n",
      "Train Epoch: 125 [25600/60000 (43%)]\tLoss: 86.890991\tBCE:68.9272\tKLD:17.9638\tC_loss:0.0000\n",
      "Train Epoch: 125 [28160/60000 (47%)]\tLoss: 88.301094\tBCE:70.0480\tKLD:18.2531\tC_loss:0.0000\n",
      "Train Epoch: 125 [30720/60000 (51%)]\tLoss: 88.881584\tBCE:71.0708\tKLD:17.8108\tC_loss:0.0000\n",
      "Train Epoch: 125 [33280/60000 (56%)]\tLoss: 86.311600\tBCE:68.6171\tKLD:17.6945\tC_loss:0.0000\n",
      "Train Epoch: 125 [35840/60000 (60%)]\tLoss: 87.346275\tBCE:69.7183\tKLD:17.6280\tC_loss:0.0000\n",
      "Train Epoch: 125 [38400/60000 (64%)]\tLoss: 87.799660\tBCE:70.0655\tKLD:17.7342\tC_loss:0.0000\n",
      "Train Epoch: 125 [40960/60000 (68%)]\tLoss: 87.697289\tBCE:70.1568\tKLD:17.5405\tC_loss:0.0000\n",
      "Train Epoch: 125 [43520/60000 (73%)]\tLoss: 86.282669\tBCE:68.4764\tKLD:17.8063\tC_loss:0.0000\n",
      "Train Epoch: 125 [46080/60000 (77%)]\tLoss: 87.810097\tBCE:70.1556\tKLD:17.6545\tC_loss:0.0000\n",
      "Train Epoch: 125 [48640/60000 (81%)]\tLoss: 86.633690\tBCE:68.6672\tKLD:17.9664\tC_loss:0.0000\n",
      "Train Epoch: 125 [51200/60000 (85%)]\tLoss: 87.485207\tBCE:69.7891\tKLD:17.6961\tC_loss:0.0000\n",
      "Train Epoch: 125 [53760/60000 (90%)]\tLoss: 86.719406\tBCE:69.3096\tKLD:17.4098\tC_loss:0.0000\n",
      "Train Epoch: 125 [56320/60000 (94%)]\tLoss: 88.746811\tBCE:70.8883\tKLD:17.8585\tC_loss:0.0000\n",
      "Train Epoch: 125 [58880/60000 (98%)]\tLoss: 91.151146\tBCE:73.2523\tKLD:17.8988\tC_loss:0.0000\n",
      "====> Epoch: 125 Average loss: 87.5479\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 90.8959\n",
      "Random number: 0\n",
      "Train Epoch: 126 [0/60000 (0%)]\tLoss: 89.024620\tBCE:70.8021\tKLD:18.2225\tC_loss:0.0000\n",
      "Train Epoch: 126 [2560/60000 (4%)]\tLoss: 87.377419\tBCE:69.3930\tKLD:17.9844\tC_loss:0.0000\n",
      "Train Epoch: 126 [5120/60000 (9%)]\tLoss: 85.718018\tBCE:68.4688\tKLD:17.2492\tC_loss:0.0000\n",
      "Train Epoch: 126 [7680/60000 (13%)]\tLoss: 88.866615\tBCE:71.1436\tKLD:17.7230\tC_loss:0.0000\n",
      "Train Epoch: 126 [10240/60000 (17%)]\tLoss: 86.811081\tBCE:69.3713\tKLD:17.4398\tC_loss:0.0000\n",
      "Train Epoch: 126 [12800/60000 (21%)]\tLoss: 87.645821\tBCE:69.6150\tKLD:18.0309\tC_loss:0.0000\n",
      "Train Epoch: 126 [15360/60000 (26%)]\tLoss: 86.593613\tBCE:69.3857\tKLD:17.2080\tC_loss:0.0000\n",
      "Train Epoch: 126 [17920/60000 (30%)]\tLoss: 89.202423\tBCE:71.2575\tKLD:17.9449\tC_loss:0.0000\n",
      "Train Epoch: 126 [20480/60000 (34%)]\tLoss: 90.167648\tBCE:71.7345\tKLD:18.4332\tC_loss:0.0000\n",
      "Train Epoch: 126 [23040/60000 (38%)]\tLoss: 90.264343\tBCE:72.0735\tKLD:18.1909\tC_loss:0.0000\n",
      "Train Epoch: 126 [25600/60000 (43%)]\tLoss: 83.366325\tBCE:66.4829\tKLD:16.8834\tC_loss:0.0000\n",
      "Train Epoch: 126 [28160/60000 (47%)]\tLoss: 87.642029\tBCE:69.8336\tKLD:17.8084\tC_loss:0.0000\n",
      "Train Epoch: 126 [30720/60000 (51%)]\tLoss: 86.935135\tBCE:68.7633\tKLD:18.1718\tC_loss:0.0000\n",
      "Train Epoch: 126 [33280/60000 (56%)]\tLoss: 90.098465\tBCE:72.0619\tKLD:18.0366\tC_loss:0.0000\n",
      "Train Epoch: 126 [35840/60000 (60%)]\tLoss: 87.738358\tBCE:69.8585\tKLD:17.8799\tC_loss:0.0000\n",
      "Train Epoch: 126 [38400/60000 (64%)]\tLoss: 87.668655\tBCE:70.4189\tKLD:17.2498\tC_loss:0.0000\n",
      "Train Epoch: 126 [40960/60000 (68%)]\tLoss: 88.489349\tBCE:70.6203\tKLD:17.8691\tC_loss:0.0000\n",
      "Train Epoch: 126 [43520/60000 (73%)]\tLoss: 89.493736\tBCE:71.4872\tKLD:18.0065\tC_loss:0.0000\n",
      "Train Epoch: 126 [46080/60000 (77%)]\tLoss: 88.203697\tBCE:70.3815\tKLD:17.8222\tC_loss:0.0000\n",
      "Train Epoch: 126 [48640/60000 (81%)]\tLoss: 87.423851\tBCE:70.0557\tKLD:17.3682\tC_loss:0.0000\n",
      "Train Epoch: 126 [51200/60000 (85%)]\tLoss: 87.330688\tBCE:69.0619\tKLD:18.2688\tC_loss:0.0000\n",
      "Train Epoch: 126 [53760/60000 (90%)]\tLoss: 87.814926\tBCE:70.0940\tKLD:17.7209\tC_loss:0.0000\n",
      "Train Epoch: 126 [56320/60000 (94%)]\tLoss: 88.600037\tBCE:70.6455\tKLD:17.9546\tC_loss:0.0000\n",
      "Train Epoch: 126 [58880/60000 (98%)]\tLoss: 85.302086\tBCE:67.7019\tKLD:17.6002\tC_loss:0.0000\n",
      "====> Epoch: 126 Average loss: 87.6255\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0419\n",
      "Train Epoch: 127 [0/60000 (0%)]\tLoss: 87.451401\tBCE:69.9251\tKLD:17.5263\tC_loss:0.0000\n",
      "Train Epoch: 127 [2560/60000 (4%)]\tLoss: 85.529861\tBCE:68.0884\tKLD:17.4415\tC_loss:0.0000\n",
      "Train Epoch: 127 [5120/60000 (9%)]\tLoss: 87.007195\tBCE:69.8000\tKLD:17.2072\tC_loss:0.0000\n",
      "Train Epoch: 127 [7680/60000 (13%)]\tLoss: 87.566360\tBCE:69.4871\tKLD:18.0793\tC_loss:0.0000\n",
      "Train Epoch: 127 [10240/60000 (17%)]\tLoss: 85.752922\tBCE:68.1557\tKLD:17.5972\tC_loss:0.0000\n",
      "Train Epoch: 127 [12800/60000 (21%)]\tLoss: 87.377899\tBCE:69.4363\tKLD:17.9416\tC_loss:0.0000\n",
      "Train Epoch: 127 [15360/60000 (26%)]\tLoss: 89.596603\tBCE:71.8650\tKLD:17.7316\tC_loss:0.0000\n",
      "Train Epoch: 127 [17920/60000 (30%)]\tLoss: 86.335060\tBCE:68.7858\tKLD:17.5493\tC_loss:0.0000\n",
      "Train Epoch: 127 [20480/60000 (34%)]\tLoss: 87.625969\tBCE:69.9063\tKLD:17.7196\tC_loss:0.0000\n",
      "Train Epoch: 127 [23040/60000 (38%)]\tLoss: 86.491531\tBCE:68.6734\tKLD:17.8181\tC_loss:0.0000\n",
      "Train Epoch: 127 [25600/60000 (43%)]\tLoss: 82.739418\tBCE:65.9298\tKLD:16.8096\tC_loss:0.0000\n",
      "Train Epoch: 127 [28160/60000 (47%)]\tLoss: 88.235847\tBCE:70.1636\tKLD:18.0723\tC_loss:0.0000\n",
      "Train Epoch: 127 [30720/60000 (51%)]\tLoss: 86.390106\tBCE:68.8072\tKLD:17.5829\tC_loss:0.0000\n",
      "Train Epoch: 127 [33280/60000 (56%)]\tLoss: 89.209137\tBCE:70.8273\tKLD:18.3819\tC_loss:0.0000\n",
      "Train Epoch: 127 [35840/60000 (60%)]\tLoss: 87.366562\tBCE:69.6103\tKLD:17.7562\tC_loss:0.0000\n",
      "Train Epoch: 127 [38400/60000 (64%)]\tLoss: 91.492767\tBCE:73.5822\tKLD:17.9106\tC_loss:0.0000\n",
      "Train Epoch: 127 [40960/60000 (68%)]\tLoss: 89.198021\tBCE:71.2897\tKLD:17.9083\tC_loss:0.0000\n",
      "Train Epoch: 127 [43520/60000 (73%)]\tLoss: 85.828110\tBCE:68.3334\tKLD:17.4947\tC_loss:0.0000\n",
      "Train Epoch: 127 [46080/60000 (77%)]\tLoss: 87.139252\tBCE:69.3894\tKLD:17.7499\tC_loss:0.0000\n",
      "Train Epoch: 127 [48640/60000 (81%)]\tLoss: 87.983360\tBCE:70.1246\tKLD:17.8587\tC_loss:0.0000\n",
      "Train Epoch: 127 [51200/60000 (85%)]\tLoss: 87.860504\tBCE:70.3953\tKLD:17.4652\tC_loss:0.0000\n",
      "Train Epoch: 127 [53760/60000 (90%)]\tLoss: 85.710724\tBCE:68.3656\tKLD:17.3451\tC_loss:0.0000\n",
      "Train Epoch: 127 [56320/60000 (94%)]\tLoss: 87.093781\tBCE:69.5055\tKLD:17.5883\tC_loss:0.0000\n",
      "Train Epoch: 127 [58880/60000 (98%)]\tLoss: 87.537910\tBCE:70.0026\tKLD:17.5353\tC_loss:0.0000\n",
      "====> Epoch: 127 Average loss: 87.5320\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.7344\n",
      "Train Epoch: 128 [0/60000 (0%)]\tLoss: 86.885651\tBCE:68.9154\tKLD:17.9703\tC_loss:0.0000\n",
      "Train Epoch: 128 [2560/60000 (4%)]\tLoss: 85.579247\tBCE:68.0329\tKLD:17.5464\tC_loss:0.0000\n",
      "Train Epoch: 128 [5120/60000 (9%)]\tLoss: 89.153526\tBCE:71.0822\tKLD:18.0713\tC_loss:0.0000\n",
      "Train Epoch: 128 [7680/60000 (13%)]\tLoss: 88.630791\tBCE:70.2526\tKLD:18.3782\tC_loss:0.0000\n",
      "Train Epoch: 128 [10240/60000 (17%)]\tLoss: 87.331696\tBCE:69.5224\tKLD:17.8093\tC_loss:0.0000\n",
      "Train Epoch: 128 [12800/60000 (21%)]\tLoss: 88.448540\tBCE:70.3202\tKLD:18.1284\tC_loss:0.0000\n",
      "Train Epoch: 128 [15360/60000 (26%)]\tLoss: 88.612885\tBCE:70.8729\tKLD:17.7400\tC_loss:0.0000\n",
      "Train Epoch: 128 [17920/60000 (30%)]\tLoss: 86.702499\tBCE:69.5491\tKLD:17.1534\tC_loss:0.0000\n",
      "Train Epoch: 128 [20480/60000 (34%)]\tLoss: 85.874855\tBCE:67.9593\tKLD:17.9156\tC_loss:0.0000\n",
      "Train Epoch: 128 [23040/60000 (38%)]\tLoss: 87.733330\tBCE:70.0606\tKLD:17.6727\tC_loss:0.0000\n",
      "Train Epoch: 128 [25600/60000 (43%)]\tLoss: 86.374672\tBCE:68.8183\tKLD:17.5564\tC_loss:0.0000\n",
      "Train Epoch: 128 [28160/60000 (47%)]\tLoss: 86.888077\tBCE:69.0167\tKLD:17.8713\tC_loss:0.0000\n",
      "Train Epoch: 128 [30720/60000 (51%)]\tLoss: 86.659744\tBCE:69.1069\tKLD:17.5529\tC_loss:0.0000\n",
      "Train Epoch: 128 [33280/60000 (56%)]\tLoss: 89.826035\tBCE:71.7870\tKLD:18.0391\tC_loss:0.0000\n",
      "Train Epoch: 128 [35840/60000 (60%)]\tLoss: 88.051376\tBCE:70.4120\tKLD:17.6394\tC_loss:0.0000\n",
      "Train Epoch: 128 [38400/60000 (64%)]\tLoss: 85.759109\tBCE:68.4009\tKLD:17.3582\tC_loss:0.0000\n",
      "Train Epoch: 128 [40960/60000 (68%)]\tLoss: 85.116196\tBCE:67.9213\tKLD:17.1949\tC_loss:0.0000\n",
      "Train Epoch: 128 [43520/60000 (73%)]\tLoss: 87.167435\tBCE:69.2254\tKLD:17.9421\tC_loss:0.0000\n",
      "Train Epoch: 128 [46080/60000 (77%)]\tLoss: 86.851631\tBCE:69.5082\tKLD:17.3434\tC_loss:0.0000\n",
      "Train Epoch: 128 [48640/60000 (81%)]\tLoss: 89.633194\tBCE:71.5845\tKLD:18.0487\tC_loss:0.0000\n",
      "Train Epoch: 128 [51200/60000 (85%)]\tLoss: 88.337166\tBCE:70.0395\tKLD:18.2976\tC_loss:0.0000\n",
      "Train Epoch: 128 [53760/60000 (90%)]\tLoss: 85.998177\tBCE:68.2443\tKLD:17.7539\tC_loss:0.0000\n",
      "Train Epoch: 128 [56320/60000 (94%)]\tLoss: 86.361992\tBCE:68.9478\tKLD:17.4142\tC_loss:0.0000\n",
      "Train Epoch: 128 [58880/60000 (98%)]\tLoss: 85.604889\tBCE:68.3441\tKLD:17.2608\tC_loss:0.0000\n",
      "====> Epoch: 128 Average loss: 87.5387\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.7582\n",
      "Train Epoch: 129 [0/60000 (0%)]\tLoss: 89.667458\tBCE:71.9610\tKLD:17.7064\tC_loss:0.0000\n",
      "Train Epoch: 129 [2560/60000 (4%)]\tLoss: 86.533379\tBCE:68.8078\tKLD:17.7256\tC_loss:0.0000\n",
      "Train Epoch: 129 [5120/60000 (9%)]\tLoss: 87.692558\tBCE:69.4200\tKLD:18.2725\tC_loss:0.0000\n",
      "Train Epoch: 129 [7680/60000 (13%)]\tLoss: 89.593811\tBCE:72.0489\tKLD:17.5449\tC_loss:0.0000\n",
      "Train Epoch: 129 [10240/60000 (17%)]\tLoss: 89.061508\tBCE:70.9060\tKLD:18.1555\tC_loss:0.0000\n",
      "Train Epoch: 129 [12800/60000 (21%)]\tLoss: 88.081856\tBCE:70.1742\tKLD:17.9076\tC_loss:0.0000\n",
      "Train Epoch: 129 [15360/60000 (26%)]\tLoss: 90.196587\tBCE:72.0896\tKLD:18.1070\tC_loss:0.0000\n",
      "Train Epoch: 129 [17920/60000 (30%)]\tLoss: 88.499771\tBCE:70.9912\tKLD:17.5085\tC_loss:0.0000\n",
      "Train Epoch: 129 [20480/60000 (34%)]\tLoss: 89.757935\tBCE:71.6909\tKLD:18.0670\tC_loss:0.0000\n",
      "Train Epoch: 129 [23040/60000 (38%)]\tLoss: 88.604591\tBCE:70.7040\tKLD:17.9006\tC_loss:0.0000\n",
      "Train Epoch: 129 [25600/60000 (43%)]\tLoss: 85.600960\tBCE:68.2075\tKLD:17.3935\tC_loss:0.0000\n",
      "Train Epoch: 129 [28160/60000 (47%)]\tLoss: 89.555832\tBCE:71.4157\tKLD:18.1401\tC_loss:0.0000\n",
      "Train Epoch: 129 [30720/60000 (51%)]\tLoss: 86.452881\tBCE:69.1772\tKLD:17.2756\tC_loss:0.0000\n",
      "Train Epoch: 129 [33280/60000 (56%)]\tLoss: 86.237564\tBCE:68.5325\tKLD:17.7050\tC_loss:0.0000\n",
      "Train Epoch: 129 [35840/60000 (60%)]\tLoss: 85.326263\tBCE:67.8163\tKLD:17.5099\tC_loss:0.0000\n",
      "Train Epoch: 129 [38400/60000 (64%)]\tLoss: 86.284134\tBCE:69.1097\tKLD:17.1744\tC_loss:0.0000\n",
      "Train Epoch: 129 [40960/60000 (68%)]\tLoss: 87.023941\tBCE:69.1882\tKLD:17.8358\tC_loss:0.0000\n",
      "Train Epoch: 129 [43520/60000 (73%)]\tLoss: 88.930580\tBCE:70.1867\tKLD:18.7439\tC_loss:0.0000\n",
      "Train Epoch: 129 [46080/60000 (77%)]\tLoss: 85.625305\tBCE:68.1779\tKLD:17.4474\tC_loss:0.0000\n",
      "Train Epoch: 129 [48640/60000 (81%)]\tLoss: 88.033066\tBCE:70.2025\tKLD:17.8305\tC_loss:0.0000\n",
      "Train Epoch: 129 [51200/60000 (85%)]\tLoss: 85.207550\tBCE:68.1029\tKLD:17.1047\tC_loss:0.0000\n",
      "Train Epoch: 129 [53760/60000 (90%)]\tLoss: 88.910011\tBCE:70.9215\tKLD:17.9886\tC_loss:0.0000\n",
      "Train Epoch: 129 [56320/60000 (94%)]\tLoss: 86.328102\tBCE:68.3701\tKLD:17.9580\tC_loss:0.0000\n",
      "Train Epoch: 129 [58880/60000 (98%)]\tLoss: 87.068443\tBCE:68.8184\tKLD:18.2501\tC_loss:0.0000\n",
      "====> Epoch: 129 Average loss: 87.5879\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0785\n",
      "Train Epoch: 130 [0/60000 (0%)]\tLoss: 88.568443\tBCE:71.2251\tKLD:17.3433\tC_loss:0.0000\n",
      "Train Epoch: 130 [2560/60000 (4%)]\tLoss: 87.552811\tBCE:69.8959\tKLD:17.6570\tC_loss:0.0000\n",
      "Train Epoch: 130 [5120/60000 (9%)]\tLoss: 83.859680\tBCE:66.5187\tKLD:17.3410\tC_loss:0.0000\n",
      "Train Epoch: 130 [7680/60000 (13%)]\tLoss: 87.712158\tBCE:69.7429\tKLD:17.9693\tC_loss:0.0000\n",
      "Train Epoch: 130 [10240/60000 (17%)]\tLoss: 88.629623\tBCE:70.2647\tKLD:18.3649\tC_loss:0.0000\n",
      "Train Epoch: 130 [12800/60000 (21%)]\tLoss: 87.948166\tBCE:70.2540\tKLD:17.6942\tC_loss:0.0000\n",
      "Train Epoch: 130 [15360/60000 (26%)]\tLoss: 88.261711\tBCE:70.2062\tKLD:18.0555\tC_loss:0.0000\n",
      "Train Epoch: 130 [17920/60000 (30%)]\tLoss: 88.832764\tBCE:70.7940\tKLD:18.0387\tC_loss:0.0000\n",
      "Train Epoch: 130 [20480/60000 (34%)]\tLoss: 84.797714\tBCE:67.3483\tKLD:17.4495\tC_loss:0.0000\n",
      "Train Epoch: 130 [23040/60000 (38%)]\tLoss: 88.198250\tBCE:70.3958\tKLD:17.8025\tC_loss:0.0000\n",
      "Train Epoch: 130 [25600/60000 (43%)]\tLoss: 87.485107\tBCE:69.6724\tKLD:17.8127\tC_loss:0.0000\n",
      "Train Epoch: 130 [28160/60000 (47%)]\tLoss: 87.505524\tBCE:70.2731\tKLD:17.2325\tC_loss:0.0000\n",
      "Train Epoch: 130 [30720/60000 (51%)]\tLoss: 87.275719\tBCE:69.6035\tKLD:17.6722\tC_loss:0.0000\n",
      "Train Epoch: 130 [33280/60000 (56%)]\tLoss: 87.778564\tBCE:70.0428\tKLD:17.7358\tC_loss:0.0000\n",
      "Train Epoch: 130 [35840/60000 (60%)]\tLoss: 87.184982\tBCE:69.7283\tKLD:17.4566\tC_loss:0.0000\n",
      "Train Epoch: 130 [38400/60000 (64%)]\tLoss: 88.338837\tBCE:70.1487\tKLD:18.1902\tC_loss:0.0000\n",
      "Train Epoch: 130 [40960/60000 (68%)]\tLoss: 86.406372\tBCE:68.2831\tKLD:18.1233\tC_loss:0.0000\n",
      "Train Epoch: 130 [43520/60000 (73%)]\tLoss: 87.167130\tBCE:69.3875\tKLD:17.7797\tC_loss:0.0000\n",
      "Train Epoch: 130 [46080/60000 (77%)]\tLoss: 88.281204\tBCE:70.5728\tKLD:17.7084\tC_loss:0.0000\n",
      "Train Epoch: 130 [48640/60000 (81%)]\tLoss: 88.700821\tBCE:70.8948\tKLD:17.8061\tC_loss:0.0000\n",
      "Train Epoch: 130 [51200/60000 (85%)]\tLoss: 87.526421\tBCE:69.6331\tKLD:17.8933\tC_loss:0.0000\n",
      "Train Epoch: 130 [53760/60000 (90%)]\tLoss: 87.793144\tBCE:69.5721\tKLD:18.2210\tC_loss:0.0000\n",
      "Train Epoch: 130 [56320/60000 (94%)]\tLoss: 90.791855\tBCE:72.8329\tKLD:17.9589\tC_loss:0.0000\n",
      "Train Epoch: 130 [58880/60000 (98%)]\tLoss: 89.111450\tBCE:70.9282\tKLD:18.1833\tC_loss:0.0000\n",
      "====> Epoch: 130 Average loss: 87.5333\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.2294\n",
      "Random number: 6\n",
      "Train Epoch: 131 [0/60000 (0%)]\tLoss: 86.720764\tBCE:69.3439\tKLD:17.3769\tC_loss:0.0000\n",
      "Train Epoch: 131 [2560/60000 (4%)]\tLoss: 89.233727\tBCE:70.5089\tKLD:18.7248\tC_loss:0.0000\n",
      "Train Epoch: 131 [5120/60000 (9%)]\tLoss: 86.786102\tBCE:69.4949\tKLD:17.2912\tC_loss:0.0000\n",
      "Train Epoch: 131 [7680/60000 (13%)]\tLoss: 85.191818\tBCE:67.1846\tKLD:18.0072\tC_loss:0.0000\n",
      "Train Epoch: 131 [10240/60000 (17%)]\tLoss: 85.037735\tBCE:67.7596\tKLD:17.2782\tC_loss:0.0000\n",
      "Train Epoch: 131 [12800/60000 (21%)]\tLoss: 86.687729\tBCE:68.9086\tKLD:17.7791\tC_loss:0.0000\n",
      "Train Epoch: 131 [15360/60000 (26%)]\tLoss: 87.781532\tBCE:70.5079\tKLD:17.2736\tC_loss:0.0000\n",
      "Train Epoch: 131 [17920/60000 (30%)]\tLoss: 89.943466\tBCE:72.2645\tKLD:17.6789\tC_loss:0.0000\n",
      "Train Epoch: 131 [20480/60000 (34%)]\tLoss: 87.116402\tBCE:69.5140\tKLD:17.6024\tC_loss:0.0000\n",
      "Train Epoch: 131 [23040/60000 (38%)]\tLoss: 88.000763\tBCE:70.3316\tKLD:17.6692\tC_loss:0.0000\n",
      "Train Epoch: 131 [25600/60000 (43%)]\tLoss: 87.509094\tBCE:69.5436\tKLD:17.9655\tC_loss:0.0000\n",
      "Train Epoch: 131 [28160/60000 (47%)]\tLoss: 87.382301\tBCE:69.0833\tKLD:18.2990\tC_loss:0.0000\n",
      "Train Epoch: 131 [30720/60000 (51%)]\tLoss: 88.785172\tBCE:71.1660\tKLD:17.6192\tC_loss:0.0000\n",
      "Train Epoch: 131 [33280/60000 (56%)]\tLoss: 87.523308\tBCE:69.5498\tKLD:17.9735\tC_loss:0.0000\n",
      "Train Epoch: 131 [35840/60000 (60%)]\tLoss: 85.928833\tBCE:67.8601\tKLD:18.0688\tC_loss:0.0000\n",
      "Train Epoch: 131 [38400/60000 (64%)]\tLoss: 87.507446\tBCE:69.4219\tKLD:18.0855\tC_loss:0.0000\n",
      "Train Epoch: 131 [40960/60000 (68%)]\tLoss: 87.664589\tBCE:69.9251\tKLD:17.7395\tC_loss:0.0000\n",
      "Train Epoch: 131 [43520/60000 (73%)]\tLoss: 92.071976\tBCE:74.0579\tKLD:18.0140\tC_loss:0.0000\n",
      "Train Epoch: 131 [46080/60000 (77%)]\tLoss: 88.834885\tBCE:71.2921\tKLD:17.5428\tC_loss:0.0000\n",
      "Train Epoch: 131 [48640/60000 (81%)]\tLoss: 88.021683\tBCE:69.6007\tKLD:18.4209\tC_loss:0.0000\n",
      "Train Epoch: 131 [51200/60000 (85%)]\tLoss: 87.259842\tBCE:69.9256\tKLD:17.3343\tC_loss:0.0000\n",
      "Train Epoch: 131 [53760/60000 (90%)]\tLoss: 84.716599\tBCE:67.3276\tKLD:17.3890\tC_loss:0.0000\n",
      "Train Epoch: 131 [56320/60000 (94%)]\tLoss: 90.407852\tBCE:72.2930\tKLD:18.1149\tC_loss:0.0000\n",
      "Train Epoch: 131 [58880/60000 (98%)]\tLoss: 86.795502\tBCE:68.6507\tKLD:18.1448\tC_loss:0.0000\n",
      "====> Epoch: 131 Average loss: 87.3847\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.9875\n",
      "Train Epoch: 132 [0/60000 (0%)]\tLoss: 86.915817\tBCE:69.3436\tKLD:17.5722\tC_loss:0.0000\n",
      "Train Epoch: 132 [2560/60000 (4%)]\tLoss: 88.625702\tBCE:70.5397\tKLD:18.0860\tC_loss:0.0000\n",
      "Train Epoch: 132 [5120/60000 (9%)]\tLoss: 89.124573\tBCE:71.2488\tKLD:17.8757\tC_loss:0.0000\n",
      "Train Epoch: 132 [7680/60000 (13%)]\tLoss: 87.671425\tBCE:69.7689\tKLD:17.9025\tC_loss:0.0000\n",
      "Train Epoch: 132 [10240/60000 (17%)]\tLoss: 90.572647\tBCE:72.5233\tKLD:18.0493\tC_loss:0.0000\n",
      "Train Epoch: 132 [12800/60000 (21%)]\tLoss: 87.784454\tBCE:69.3849\tKLD:18.3995\tC_loss:0.0000\n",
      "Train Epoch: 132 [15360/60000 (26%)]\tLoss: 88.565819\tBCE:70.7371\tKLD:17.8287\tC_loss:0.0000\n",
      "Train Epoch: 132 [17920/60000 (30%)]\tLoss: 89.184738\tBCE:71.2178\tKLD:17.9670\tC_loss:0.0000\n",
      "Train Epoch: 132 [20480/60000 (34%)]\tLoss: 87.696907\tBCE:69.6713\tKLD:18.0256\tC_loss:0.0000\n",
      "Train Epoch: 132 [23040/60000 (38%)]\tLoss: 86.723328\tBCE:68.6998\tKLD:18.0236\tC_loss:0.0000\n",
      "Train Epoch: 132 [25600/60000 (43%)]\tLoss: 87.348557\tBCE:69.4283\tKLD:17.9203\tC_loss:0.0000\n",
      "Train Epoch: 132 [28160/60000 (47%)]\tLoss: 87.594116\tBCE:69.6799\tKLD:17.9142\tC_loss:0.0000\n",
      "Train Epoch: 132 [30720/60000 (51%)]\tLoss: 90.103699\tBCE:71.7696\tKLD:18.3341\tC_loss:0.0000\n",
      "Train Epoch: 132 [33280/60000 (56%)]\tLoss: 85.892731\tBCE:68.3771\tKLD:17.5156\tC_loss:0.0000\n",
      "Train Epoch: 132 [35840/60000 (60%)]\tLoss: 88.855232\tBCE:70.6175\tKLD:18.2377\tC_loss:0.0000\n",
      "Train Epoch: 132 [38400/60000 (64%)]\tLoss: 88.466415\tBCE:70.4809\tKLD:17.9855\tC_loss:0.0000\n",
      "Train Epoch: 132 [40960/60000 (68%)]\tLoss: 89.278046\tBCE:70.6785\tKLD:18.5995\tC_loss:0.0000\n",
      "Train Epoch: 132 [43520/60000 (73%)]\tLoss: 87.228134\tBCE:69.3557\tKLD:17.8724\tC_loss:0.0000\n",
      "Train Epoch: 132 [46080/60000 (77%)]\tLoss: 86.639191\tBCE:69.2635\tKLD:17.3757\tC_loss:0.0000\n",
      "Train Epoch: 132 [48640/60000 (81%)]\tLoss: 87.862732\tBCE:69.7347\tKLD:18.1280\tC_loss:0.0000\n",
      "Train Epoch: 132 [51200/60000 (85%)]\tLoss: 87.935654\tBCE:70.1121\tKLD:17.8235\tC_loss:0.0000\n",
      "Train Epoch: 132 [53760/60000 (90%)]\tLoss: 86.344589\tBCE:69.2741\tKLD:17.0705\tC_loss:0.0000\n",
      "Train Epoch: 132 [56320/60000 (94%)]\tLoss: 86.241158\tBCE:68.5758\tKLD:17.6653\tC_loss:0.0000\n",
      "Train Epoch: 132 [58880/60000 (98%)]\tLoss: 85.885704\tBCE:68.3886\tKLD:17.4971\tC_loss:0.0000\n",
      "====> Epoch: 132 Average loss: 87.4636\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.6120\n",
      "Train Epoch: 133 [0/60000 (0%)]\tLoss: 90.146133\tBCE:71.8469\tKLD:18.2992\tC_loss:0.0000\n",
      "Train Epoch: 133 [2560/60000 (4%)]\tLoss: 84.630493\tBCE:67.2105\tKLD:17.4200\tC_loss:0.0000\n",
      "Train Epoch: 133 [5120/60000 (9%)]\tLoss: 86.543091\tBCE:68.5201\tKLD:18.0230\tC_loss:0.0000\n",
      "Train Epoch: 133 [7680/60000 (13%)]\tLoss: 87.393867\tBCE:69.9158\tKLD:17.4781\tC_loss:0.0000\n",
      "Train Epoch: 133 [10240/60000 (17%)]\tLoss: 89.964737\tBCE:71.8599\tKLD:18.1048\tC_loss:0.0000\n",
      "Train Epoch: 133 [12800/60000 (21%)]\tLoss: 86.862465\tBCE:68.9678\tKLD:17.8947\tC_loss:0.0000\n",
      "Train Epoch: 133 [15360/60000 (26%)]\tLoss: 88.225273\tBCE:70.2841\tKLD:17.9412\tC_loss:0.0000\n",
      "Train Epoch: 133 [17920/60000 (30%)]\tLoss: 89.327942\tBCE:71.3744\tKLD:17.9535\tC_loss:0.0000\n",
      "Train Epoch: 133 [20480/60000 (34%)]\tLoss: 89.083572\tBCE:70.8557\tKLD:18.2278\tC_loss:0.0000\n",
      "Train Epoch: 133 [23040/60000 (38%)]\tLoss: 87.901245\tBCE:70.3921\tKLD:17.5092\tC_loss:0.0000\n",
      "Train Epoch: 133 [25600/60000 (43%)]\tLoss: 88.149017\tBCE:70.1943\tKLD:17.9547\tC_loss:0.0000\n",
      "Train Epoch: 133 [28160/60000 (47%)]\tLoss: 85.278275\tBCE:67.8416\tKLD:17.4367\tC_loss:0.0000\n",
      "Train Epoch: 133 [30720/60000 (51%)]\tLoss: 86.299515\tBCE:68.9409\tKLD:17.3586\tC_loss:0.0000\n",
      "Train Epoch: 133 [33280/60000 (56%)]\tLoss: 87.976715\tBCE:70.2115\tKLD:17.7652\tC_loss:0.0000\n",
      "Train Epoch: 133 [35840/60000 (60%)]\tLoss: 89.411148\tBCE:71.2648\tKLD:18.1463\tC_loss:0.0000\n",
      "Train Epoch: 133 [38400/60000 (64%)]\tLoss: 87.346771\tBCE:69.8920\tKLD:17.4548\tC_loss:0.0000\n",
      "Train Epoch: 133 [40960/60000 (68%)]\tLoss: 87.928741\tBCE:69.9633\tKLD:17.9654\tC_loss:0.0000\n",
      "Train Epoch: 133 [43520/60000 (73%)]\tLoss: 89.352806\tBCE:71.3546\tKLD:17.9982\tC_loss:0.0000\n",
      "Train Epoch: 133 [46080/60000 (77%)]\tLoss: 88.778030\tBCE:71.2392\tKLD:17.5389\tC_loss:0.0000\n",
      "Train Epoch: 133 [48640/60000 (81%)]\tLoss: 87.998779\tBCE:70.0153\tKLD:17.9835\tC_loss:0.0000\n",
      "Train Epoch: 133 [51200/60000 (85%)]\tLoss: 86.994843\tBCE:69.1744\tKLD:17.8205\tC_loss:0.0000\n",
      "Train Epoch: 133 [53760/60000 (90%)]\tLoss: 88.304787\tBCE:70.4403\tKLD:17.8644\tC_loss:0.0000\n",
      "Train Epoch: 133 [56320/60000 (94%)]\tLoss: 90.424774\tBCE:72.2116\tKLD:18.2132\tC_loss:0.0000\n",
      "Train Epoch: 133 [58880/60000 (98%)]\tLoss: 88.185745\tBCE:70.3260\tKLD:17.8597\tC_loss:0.0000\n",
      "====> Epoch: 133 Average loss: 87.4382\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.3184\n",
      "Train Epoch: 134 [0/60000 (0%)]\tLoss: 86.366928\tBCE:68.6443\tKLD:17.7226\tC_loss:0.0000\n",
      "Train Epoch: 134 [2560/60000 (4%)]\tLoss: 87.704651\tBCE:69.6720\tKLD:18.0327\tC_loss:0.0000\n",
      "Train Epoch: 134 [5120/60000 (9%)]\tLoss: 85.078995\tBCE:67.6338\tKLD:17.4452\tC_loss:0.0000\n",
      "Train Epoch: 134 [7680/60000 (13%)]\tLoss: 90.282944\tBCE:72.0539\tKLD:18.2291\tC_loss:0.0000\n",
      "Train Epoch: 134 [10240/60000 (17%)]\tLoss: 90.129471\tBCE:72.1178\tKLD:18.0117\tC_loss:0.0000\n",
      "Train Epoch: 134 [12800/60000 (21%)]\tLoss: 89.094887\tBCE:71.4470\tKLD:17.6479\tC_loss:0.0000\n",
      "Train Epoch: 134 [15360/60000 (26%)]\tLoss: 87.697548\tBCE:69.8703\tKLD:17.8273\tC_loss:0.0000\n",
      "Train Epoch: 134 [17920/60000 (30%)]\tLoss: 87.275833\tBCE:69.1428\tKLD:18.1330\tC_loss:0.0000\n",
      "Train Epoch: 134 [20480/60000 (34%)]\tLoss: 87.961304\tBCE:69.9124\tKLD:18.0490\tC_loss:0.0000\n",
      "Train Epoch: 134 [23040/60000 (38%)]\tLoss: 87.503349\tBCE:69.7861\tKLD:17.7172\tC_loss:0.0000\n",
      "Train Epoch: 134 [25600/60000 (43%)]\tLoss: 86.861526\tBCE:69.2964\tKLD:17.5651\tC_loss:0.0000\n",
      "Train Epoch: 134 [28160/60000 (47%)]\tLoss: 86.887482\tBCE:69.1832\tKLD:17.7043\tC_loss:0.0000\n",
      "Train Epoch: 134 [30720/60000 (51%)]\tLoss: 89.628899\tBCE:71.6953\tKLD:17.9336\tC_loss:0.0000\n",
      "Train Epoch: 134 [33280/60000 (56%)]\tLoss: 84.728340\tBCE:67.1954\tKLD:17.5329\tC_loss:0.0000\n",
      "Train Epoch: 134 [35840/60000 (60%)]\tLoss: 87.621841\tBCE:70.2812\tKLD:17.3407\tC_loss:0.0000\n",
      "Train Epoch: 134 [38400/60000 (64%)]\tLoss: 87.350510\tBCE:70.0395\tKLD:17.3110\tC_loss:0.0000\n",
      "Train Epoch: 134 [40960/60000 (68%)]\tLoss: 88.075127\tBCE:70.2544\tKLD:17.8208\tC_loss:0.0000\n",
      "Train Epoch: 134 [43520/60000 (73%)]\tLoss: 86.052101\tBCE:68.2854\tKLD:17.7667\tC_loss:0.0000\n",
      "Train Epoch: 134 [46080/60000 (77%)]\tLoss: 87.310631\tBCE:69.6434\tKLD:17.6673\tC_loss:0.0000\n",
      "Train Epoch: 134 [48640/60000 (81%)]\tLoss: 87.287666\tBCE:70.0711\tKLD:17.2166\tC_loss:0.0000\n",
      "Train Epoch: 134 [51200/60000 (85%)]\tLoss: 88.139511\tBCE:70.3546\tKLD:17.7850\tC_loss:0.0000\n",
      "Train Epoch: 134 [53760/60000 (90%)]\tLoss: 86.668137\tBCE:69.2973\tKLD:17.3709\tC_loss:0.0000\n",
      "Train Epoch: 134 [56320/60000 (94%)]\tLoss: 84.230064\tBCE:66.7212\tKLD:17.5089\tC_loss:0.0000\n",
      "Train Epoch: 134 [58880/60000 (98%)]\tLoss: 87.535942\tBCE:69.9645\tKLD:17.5714\tC_loss:0.0000\n",
      "====> Epoch: 134 Average loss: 87.5138\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.9649\n",
      "Train Epoch: 135 [0/60000 (0%)]\tLoss: 86.851837\tBCE:69.2038\tKLD:17.6481\tC_loss:0.0000\n",
      "Train Epoch: 135 [2560/60000 (4%)]\tLoss: 88.380730\tBCE:70.3024\tKLD:18.0783\tC_loss:0.0000\n",
      "Train Epoch: 135 [5120/60000 (9%)]\tLoss: 87.524055\tBCE:69.7049\tKLD:17.8191\tC_loss:0.0000\n",
      "Train Epoch: 135 [7680/60000 (13%)]\tLoss: 85.797440\tBCE:68.3812\tKLD:17.4162\tC_loss:0.0000\n",
      "Train Epoch: 135 [10240/60000 (17%)]\tLoss: 86.909744\tBCE:69.2712\tKLD:17.6385\tC_loss:0.0000\n",
      "Train Epoch: 135 [12800/60000 (21%)]\tLoss: 86.567184\tBCE:68.7494\tKLD:17.8178\tC_loss:0.0000\n",
      "Train Epoch: 135 [15360/60000 (26%)]\tLoss: 87.417847\tBCE:69.8270\tKLD:17.5909\tC_loss:0.0000\n",
      "Train Epoch: 135 [17920/60000 (30%)]\tLoss: 88.398651\tBCE:70.2377\tKLD:18.1609\tC_loss:0.0000\n",
      "Train Epoch: 135 [20480/60000 (34%)]\tLoss: 87.017273\tBCE:69.2765\tKLD:17.7408\tC_loss:0.0000\n",
      "Train Epoch: 135 [23040/60000 (38%)]\tLoss: 88.707504\tBCE:70.9681\tKLD:17.7394\tC_loss:0.0000\n",
      "Train Epoch: 135 [25600/60000 (43%)]\tLoss: 88.051331\tBCE:69.9040\tKLD:18.1474\tC_loss:0.0000\n",
      "Train Epoch: 135 [28160/60000 (47%)]\tLoss: 88.549095\tBCE:70.6379\tKLD:17.9112\tC_loss:0.0000\n",
      "Train Epoch: 135 [30720/60000 (51%)]\tLoss: 87.525764\tBCE:69.6079\tKLD:17.9178\tC_loss:0.0000\n",
      "Train Epoch: 135 [33280/60000 (56%)]\tLoss: 86.635338\tBCE:69.1700\tKLD:17.4653\tC_loss:0.0000\n",
      "Train Epoch: 135 [35840/60000 (60%)]\tLoss: 86.879868\tBCE:69.2717\tKLD:17.6081\tC_loss:0.0000\n",
      "Train Epoch: 135 [38400/60000 (64%)]\tLoss: 84.392517\tBCE:67.2265\tKLD:17.1660\tC_loss:0.0000\n",
      "Train Epoch: 135 [40960/60000 (68%)]\tLoss: 87.826576\tBCE:70.3793\tKLD:17.4473\tC_loss:0.0000\n",
      "Train Epoch: 135 [43520/60000 (73%)]\tLoss: 87.802414\tBCE:69.3682\tKLD:18.4342\tC_loss:0.0000\n",
      "Train Epoch: 135 [46080/60000 (77%)]\tLoss: 87.458664\tBCE:69.5244\tKLD:17.9343\tC_loss:0.0000\n",
      "Train Epoch: 135 [48640/60000 (81%)]\tLoss: 86.625877\tBCE:68.9805\tKLD:17.6453\tC_loss:0.0000\n",
      "Train Epoch: 135 [51200/60000 (85%)]\tLoss: 88.312447\tBCE:70.7050\tKLD:17.6075\tC_loss:0.0000\n",
      "Train Epoch: 135 [53760/60000 (90%)]\tLoss: 86.837929\tBCE:69.0173\tKLD:17.8206\tC_loss:0.0000\n",
      "Train Epoch: 135 [56320/60000 (94%)]\tLoss: 89.984451\tBCE:71.3406\tKLD:18.6439\tC_loss:0.0000\n",
      "Train Epoch: 135 [58880/60000 (98%)]\tLoss: 89.052406\tBCE:71.0968\tKLD:17.9556\tC_loss:0.0000\n",
      "====> Epoch: 135 Average loss: 87.3883\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 90.9130\n",
      "Random number: 6\n",
      "Train Epoch: 136 [0/60000 (0%)]\tLoss: 87.091270\tBCE:69.1782\tKLD:17.9131\tC_loss:0.0000\n",
      "Train Epoch: 136 [2560/60000 (4%)]\tLoss: 89.559570\tBCE:71.1866\tKLD:18.3729\tC_loss:0.0000\n",
      "Train Epoch: 136 [5120/60000 (9%)]\tLoss: 87.034348\tBCE:69.0363\tKLD:17.9980\tC_loss:0.0000\n",
      "Train Epoch: 136 [7680/60000 (13%)]\tLoss: 87.775818\tBCE:69.6799\tKLD:18.0959\tC_loss:0.0000\n",
      "Train Epoch: 136 [10240/60000 (17%)]\tLoss: 86.710083\tBCE:69.5175\tKLD:17.1926\tC_loss:0.0000\n",
      "Train Epoch: 136 [12800/60000 (21%)]\tLoss: 87.431885\tBCE:69.3755\tKLD:18.0564\tC_loss:0.0000\n",
      "Train Epoch: 136 [15360/60000 (26%)]\tLoss: 89.185745\tBCE:71.2572\tKLD:17.9286\tC_loss:0.0000\n",
      "Train Epoch: 136 [17920/60000 (30%)]\tLoss: 89.587479\tBCE:71.5667\tKLD:18.0208\tC_loss:0.0000\n",
      "Train Epoch: 136 [20480/60000 (34%)]\tLoss: 87.008682\tBCE:69.0504\tKLD:17.9583\tC_loss:0.0000\n",
      "Train Epoch: 136 [23040/60000 (38%)]\tLoss: 89.039963\tBCE:70.9064\tKLD:18.1336\tC_loss:0.0000\n",
      "Train Epoch: 136 [25600/60000 (43%)]\tLoss: 87.650635\tBCE:70.1188\tKLD:17.5319\tC_loss:0.0000\n",
      "Train Epoch: 136 [28160/60000 (47%)]\tLoss: 85.810867\tBCE:68.2390\tKLD:17.5719\tC_loss:0.0000\n",
      "Train Epoch: 136 [30720/60000 (51%)]\tLoss: 85.358917\tBCE:67.6614\tKLD:17.6975\tC_loss:0.0000\n",
      "Train Epoch: 136 [33280/60000 (56%)]\tLoss: 90.806076\tBCE:72.7563\tKLD:18.0498\tC_loss:0.0000\n",
      "Train Epoch: 136 [35840/60000 (60%)]\tLoss: 88.262154\tBCE:70.6045\tKLD:17.6576\tC_loss:0.0000\n",
      "Train Epoch: 136 [38400/60000 (64%)]\tLoss: 88.393372\tBCE:70.4472\tKLD:17.9462\tC_loss:0.0000\n",
      "Train Epoch: 136 [40960/60000 (68%)]\tLoss: 88.445450\tBCE:70.2728\tKLD:18.1726\tC_loss:0.0000\n",
      "Train Epoch: 136 [43520/60000 (73%)]\tLoss: 84.700577\tBCE:67.7844\tKLD:16.9162\tC_loss:0.0000\n",
      "Train Epoch: 136 [46080/60000 (77%)]\tLoss: 86.349625\tBCE:69.0369\tKLD:17.3127\tC_loss:0.0000\n",
      "Train Epoch: 136 [48640/60000 (81%)]\tLoss: 86.561134\tBCE:69.3137\tKLD:17.2475\tC_loss:0.0000\n",
      "Train Epoch: 136 [51200/60000 (85%)]\tLoss: 87.973343\tBCE:69.9206\tKLD:18.0528\tC_loss:0.0000\n",
      "Train Epoch: 136 [53760/60000 (90%)]\tLoss: 87.023514\tBCE:69.4604\tKLD:17.5631\tC_loss:0.0000\n",
      "Train Epoch: 136 [56320/60000 (94%)]\tLoss: 85.300583\tBCE:68.0726\tKLD:17.2280\tC_loss:0.0000\n",
      "Train Epoch: 136 [58880/60000 (98%)]\tLoss: 86.892204\tBCE:69.1448\tKLD:17.7474\tC_loss:0.0000\n",
      "====> Epoch: 136 Average loss: 87.3421\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.1039\n",
      "Train Epoch: 137 [0/60000 (0%)]\tLoss: 87.296204\tBCE:68.9430\tKLD:18.3532\tC_loss:0.0000\n",
      "Train Epoch: 137 [2560/60000 (4%)]\tLoss: 87.757149\tBCE:69.8720\tKLD:17.8852\tC_loss:0.0000\n",
      "Train Epoch: 137 [5120/60000 (9%)]\tLoss: 86.353996\tBCE:69.5274\tKLD:16.8266\tC_loss:0.0000\n",
      "Train Epoch: 137 [7680/60000 (13%)]\tLoss: 87.262169\tBCE:69.6229\tKLD:17.6393\tC_loss:0.0000\n",
      "Train Epoch: 137 [10240/60000 (17%)]\tLoss: 87.641724\tBCE:69.6769\tKLD:17.9648\tC_loss:0.0000\n",
      "Train Epoch: 137 [12800/60000 (21%)]\tLoss: 87.625984\tBCE:69.8862\tKLD:17.7398\tC_loss:0.0000\n",
      "Train Epoch: 137 [15360/60000 (26%)]\tLoss: 89.290192\tBCE:71.0953\tKLD:18.1949\tC_loss:0.0000\n",
      "Train Epoch: 137 [17920/60000 (30%)]\tLoss: 88.789963\tBCE:70.7051\tKLD:18.0848\tC_loss:0.0000\n",
      "Train Epoch: 137 [20480/60000 (34%)]\tLoss: 87.112892\tBCE:69.0312\tKLD:18.0817\tC_loss:0.0000\n",
      "Train Epoch: 137 [23040/60000 (38%)]\tLoss: 90.474655\tBCE:72.7301\tKLD:17.7445\tC_loss:0.0000\n",
      "Train Epoch: 137 [25600/60000 (43%)]\tLoss: 87.762161\tBCE:69.2806\tKLD:18.4815\tC_loss:0.0000\n",
      "Train Epoch: 137 [28160/60000 (47%)]\tLoss: 87.511581\tBCE:70.0898\tKLD:17.4218\tC_loss:0.0000\n",
      "Train Epoch: 137 [30720/60000 (51%)]\tLoss: 87.226738\tBCE:69.3428\tKLD:17.8839\tC_loss:0.0000\n",
      "Train Epoch: 137 [33280/60000 (56%)]\tLoss: 86.830841\tBCE:68.9751\tKLD:17.8557\tC_loss:0.0000\n",
      "Train Epoch: 137 [35840/60000 (60%)]\tLoss: 87.792130\tBCE:70.0825\tKLD:17.7097\tC_loss:0.0000\n",
      "Train Epoch: 137 [38400/60000 (64%)]\tLoss: 87.746216\tBCE:70.1066\tKLD:17.6396\tC_loss:0.0000\n",
      "Train Epoch: 137 [40960/60000 (68%)]\tLoss: 86.308090\tBCE:69.0030\tKLD:17.3051\tC_loss:0.0000\n",
      "Train Epoch: 137 [43520/60000 (73%)]\tLoss: 87.406342\tBCE:69.5560\tKLD:17.8503\tC_loss:0.0000\n",
      "Train Epoch: 137 [46080/60000 (77%)]\tLoss: 86.028366\tBCE:68.3472\tKLD:17.6812\tC_loss:0.0000\n",
      "Train Epoch: 137 [48640/60000 (81%)]\tLoss: 88.095840\tBCE:70.2359\tKLD:17.8599\tC_loss:0.0000\n",
      "Train Epoch: 137 [51200/60000 (85%)]\tLoss: 86.807526\tBCE:69.0132\tKLD:17.7944\tC_loss:0.0000\n",
      "Train Epoch: 137 [53760/60000 (90%)]\tLoss: 87.857330\tBCE:70.3035\tKLD:17.5538\tC_loss:0.0000\n",
      "Train Epoch: 137 [56320/60000 (94%)]\tLoss: 88.542038\tBCE:70.7957\tKLD:17.7463\tC_loss:0.0000\n",
      "Train Epoch: 137 [58880/60000 (98%)]\tLoss: 87.961990\tBCE:69.8114\tKLD:18.1506\tC_loss:0.0000\n",
      "====> Epoch: 137 Average loss: 87.3669\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.2279\n",
      "Train Epoch: 138 [0/60000 (0%)]\tLoss: 84.736168\tBCE:67.2146\tKLD:17.5216\tC_loss:0.0000\n",
      "Train Epoch: 138 [2560/60000 (4%)]\tLoss: 88.660019\tBCE:70.7941\tKLD:17.8659\tC_loss:0.0000\n",
      "Train Epoch: 138 [5120/60000 (9%)]\tLoss: 86.748901\tBCE:69.1645\tKLD:17.5844\tC_loss:0.0000\n",
      "Train Epoch: 138 [7680/60000 (13%)]\tLoss: 87.472832\tBCE:69.6531\tKLD:17.8198\tC_loss:0.0000\n",
      "Train Epoch: 138 [10240/60000 (17%)]\tLoss: 88.446701\tBCE:70.4512\tKLD:17.9955\tC_loss:0.0000\n",
      "Train Epoch: 138 [12800/60000 (21%)]\tLoss: 88.094513\tBCE:70.2354\tKLD:17.8592\tC_loss:0.0000\n",
      "Train Epoch: 138 [15360/60000 (26%)]\tLoss: 87.457047\tBCE:69.4397\tKLD:18.0173\tC_loss:0.0000\n",
      "Train Epoch: 138 [17920/60000 (30%)]\tLoss: 87.580017\tBCE:70.7161\tKLD:16.8639\tC_loss:0.0000\n",
      "Train Epoch: 138 [20480/60000 (34%)]\tLoss: 87.863998\tBCE:69.6685\tKLD:18.1955\tC_loss:0.0000\n",
      "Train Epoch: 138 [23040/60000 (38%)]\tLoss: 87.321777\tBCE:69.5215\tKLD:17.8003\tC_loss:0.0000\n",
      "Train Epoch: 138 [25600/60000 (43%)]\tLoss: 87.768036\tBCE:70.2558\tKLD:17.5122\tC_loss:0.0000\n",
      "Train Epoch: 138 [28160/60000 (47%)]\tLoss: 86.405861\tBCE:68.9415\tKLD:17.4643\tC_loss:0.0000\n",
      "Train Epoch: 138 [30720/60000 (51%)]\tLoss: 87.298904\tBCE:69.1938\tKLD:18.1051\tC_loss:0.0000\n",
      "Train Epoch: 138 [33280/60000 (56%)]\tLoss: 84.501923\tBCE:66.8956\tKLD:17.6063\tC_loss:0.0000\n",
      "Train Epoch: 138 [35840/60000 (60%)]\tLoss: 87.782074\tBCE:69.9950\tKLD:17.7870\tC_loss:0.0000\n",
      "Train Epoch: 138 [38400/60000 (64%)]\tLoss: 86.963394\tBCE:69.2318\tKLD:17.7315\tC_loss:0.0000\n",
      "Train Epoch: 138 [40960/60000 (68%)]\tLoss: 85.615440\tBCE:68.0149\tKLD:17.6005\tC_loss:0.0000\n",
      "Train Epoch: 138 [43520/60000 (73%)]\tLoss: 85.976128\tBCE:68.2707\tKLD:17.7054\tC_loss:0.0000\n",
      "Train Epoch: 138 [46080/60000 (77%)]\tLoss: 87.287956\tBCE:69.5052\tKLD:17.7827\tC_loss:0.0000\n",
      "Train Epoch: 138 [48640/60000 (81%)]\tLoss: 89.675262\tBCE:71.1517\tKLD:18.5235\tC_loss:0.0000\n",
      "Train Epoch: 138 [51200/60000 (85%)]\tLoss: 88.376625\tBCE:70.9183\tKLD:17.4583\tC_loss:0.0000\n",
      "Train Epoch: 138 [53760/60000 (90%)]\tLoss: 91.236618\tBCE:73.2055\tKLD:18.0312\tC_loss:0.0000\n",
      "Train Epoch: 138 [56320/60000 (94%)]\tLoss: 86.090729\tBCE:68.0904\tKLD:18.0003\tC_loss:0.0000\n",
      "Train Epoch: 138 [58880/60000 (98%)]\tLoss: 87.183800\tBCE:69.4958\tKLD:17.6880\tC_loss:0.0000\n",
      "====> Epoch: 138 Average loss: 87.3453\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0375\n",
      "Train Epoch: 139 [0/60000 (0%)]\tLoss: 88.567444\tBCE:70.2851\tKLD:18.2823\tC_loss:0.0000\n",
      "Train Epoch: 139 [2560/60000 (4%)]\tLoss: 86.767807\tBCE:68.5858\tKLD:18.1820\tC_loss:0.0000\n",
      "Train Epoch: 139 [5120/60000 (9%)]\tLoss: 87.294601\tBCE:69.9516\tKLD:17.3430\tC_loss:0.0000\n",
      "Train Epoch: 139 [7680/60000 (13%)]\tLoss: 88.248413\tBCE:69.9512\tKLD:18.2972\tC_loss:0.0000\n",
      "Train Epoch: 139 [10240/60000 (17%)]\tLoss: 85.908821\tBCE:68.4294\tKLD:17.4794\tC_loss:0.0000\n",
      "Train Epoch: 139 [12800/60000 (21%)]\tLoss: 86.771309\tBCE:68.7047\tKLD:18.0666\tC_loss:0.0000\n",
      "Train Epoch: 139 [15360/60000 (26%)]\tLoss: 86.524239\tBCE:68.6521\tKLD:17.8721\tC_loss:0.0000\n",
      "Train Epoch: 139 [17920/60000 (30%)]\tLoss: 88.335533\tBCE:70.6259\tKLD:17.7097\tC_loss:0.0000\n",
      "Train Epoch: 139 [20480/60000 (34%)]\tLoss: 87.558632\tBCE:69.8060\tKLD:17.7526\tC_loss:0.0000\n",
      "Train Epoch: 139 [23040/60000 (38%)]\tLoss: 86.731850\tBCE:68.8508\tKLD:17.8811\tC_loss:0.0000\n",
      "Train Epoch: 139 [25600/60000 (43%)]\tLoss: 85.766243\tBCE:67.9037\tKLD:17.8625\tC_loss:0.0000\n",
      "Train Epoch: 139 [28160/60000 (47%)]\tLoss: 87.417809\tBCE:69.6299\tKLD:17.7879\tC_loss:0.0000\n",
      "Train Epoch: 139 [30720/60000 (51%)]\tLoss: 88.269241\tBCE:70.5380\tKLD:17.7312\tC_loss:0.0000\n",
      "Train Epoch: 139 [33280/60000 (56%)]\tLoss: 89.720016\tBCE:71.3504\tKLD:18.3696\tC_loss:0.0000\n",
      "Train Epoch: 139 [35840/60000 (60%)]\tLoss: 84.262169\tBCE:66.5275\tKLD:17.7347\tC_loss:0.0000\n",
      "Train Epoch: 139 [38400/60000 (64%)]\tLoss: 86.109932\tBCE:68.6264\tKLD:17.4835\tC_loss:0.0000\n",
      "Train Epoch: 139 [40960/60000 (68%)]\tLoss: 86.933685\tBCE:69.5378\tKLD:17.3959\tC_loss:0.0000\n",
      "Train Epoch: 139 [43520/60000 (73%)]\tLoss: 88.186859\tBCE:70.0369\tKLD:18.1500\tC_loss:0.0000\n",
      "Train Epoch: 139 [46080/60000 (77%)]\tLoss: 85.354179\tBCE:67.8913\tKLD:17.4629\tC_loss:0.0000\n",
      "Train Epoch: 139 [48640/60000 (81%)]\tLoss: 88.306610\tBCE:70.8946\tKLD:17.4120\tC_loss:0.0000\n",
      "Train Epoch: 139 [51200/60000 (85%)]\tLoss: 86.549377\tBCE:68.9762\tKLD:17.5732\tC_loss:0.0000\n",
      "Train Epoch: 139 [53760/60000 (90%)]\tLoss: 88.784966\tBCE:70.7159\tKLD:18.0691\tC_loss:0.0000\n",
      "Train Epoch: 139 [56320/60000 (94%)]\tLoss: 89.447815\tBCE:70.9895\tKLD:18.4583\tC_loss:0.0000\n",
      "Train Epoch: 139 [58880/60000 (98%)]\tLoss: 86.088470\tBCE:68.0257\tKLD:18.0627\tC_loss:0.0000\n",
      "====> Epoch: 139 Average loss: 87.3748\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.2611\n",
      "Train Epoch: 140 [0/60000 (0%)]\tLoss: 86.057465\tBCE:68.3009\tKLD:17.7566\tC_loss:0.0000\n",
      "Train Epoch: 140 [2560/60000 (4%)]\tLoss: 88.013046\tBCE:69.9464\tKLD:18.0667\tC_loss:0.0000\n",
      "Train Epoch: 140 [5120/60000 (9%)]\tLoss: 87.157364\tBCE:69.6516\tKLD:17.5058\tC_loss:0.0000\n",
      "Train Epoch: 140 [7680/60000 (13%)]\tLoss: 89.759735\tBCE:71.2416\tKLD:18.5181\tC_loss:0.0000\n",
      "Train Epoch: 140 [10240/60000 (17%)]\tLoss: 87.528717\tBCE:69.9396\tKLD:17.5891\tC_loss:0.0000\n",
      "Train Epoch: 140 [12800/60000 (21%)]\tLoss: 86.964218\tBCE:69.1711\tKLD:17.7931\tC_loss:0.0000\n",
      "Train Epoch: 140 [15360/60000 (26%)]\tLoss: 87.263237\tBCE:69.3946\tKLD:17.8686\tC_loss:0.0000\n",
      "Train Epoch: 140 [17920/60000 (30%)]\tLoss: 86.772751\tBCE:69.0643\tKLD:17.7085\tC_loss:0.0000\n",
      "Train Epoch: 140 [20480/60000 (34%)]\tLoss: 87.892807\tBCE:70.0102\tKLD:17.8826\tC_loss:0.0000\n",
      "Train Epoch: 140 [23040/60000 (38%)]\tLoss: 88.235687\tBCE:70.3204\tKLD:17.9153\tC_loss:0.0000\n",
      "Train Epoch: 140 [25600/60000 (43%)]\tLoss: 87.853271\tBCE:70.3314\tKLD:17.5219\tC_loss:0.0000\n",
      "Train Epoch: 140 [28160/60000 (47%)]\tLoss: 90.141464\tBCE:72.0306\tKLD:18.1109\tC_loss:0.0000\n",
      "Train Epoch: 140 [30720/60000 (51%)]\tLoss: 87.719528\tBCE:70.1141\tKLD:17.6054\tC_loss:0.0000\n",
      "Train Epoch: 140 [33280/60000 (56%)]\tLoss: 88.488274\tBCE:70.2488\tKLD:18.2395\tC_loss:0.0000\n",
      "Train Epoch: 140 [35840/60000 (60%)]\tLoss: 89.071701\tBCE:71.0910\tKLD:17.9807\tC_loss:0.0000\n",
      "Train Epoch: 140 [38400/60000 (64%)]\tLoss: 85.485443\tBCE:68.4067\tKLD:17.0788\tC_loss:0.0000\n",
      "Train Epoch: 140 [40960/60000 (68%)]\tLoss: 89.416016\tBCE:71.6196\tKLD:17.7964\tC_loss:0.0000\n",
      "Train Epoch: 140 [43520/60000 (73%)]\tLoss: 86.587051\tBCE:68.1758\tKLD:18.4112\tC_loss:0.0000\n",
      "Train Epoch: 140 [46080/60000 (77%)]\tLoss: 84.774597\tBCE:67.1287\tKLD:17.6459\tC_loss:0.0000\n",
      "Train Epoch: 140 [48640/60000 (81%)]\tLoss: 88.491158\tBCE:70.4338\tKLD:18.0573\tC_loss:0.0000\n",
      "Train Epoch: 140 [51200/60000 (85%)]\tLoss: 88.541687\tBCE:70.4914\tKLD:18.0503\tC_loss:0.0000\n",
      "Train Epoch: 140 [53760/60000 (90%)]\tLoss: 88.763763\tBCE:70.7865\tKLD:17.9773\tC_loss:0.0000\n",
      "Train Epoch: 140 [56320/60000 (94%)]\tLoss: 85.513618\tBCE:67.8877\tKLD:17.6259\tC_loss:0.0000\n",
      "Train Epoch: 140 [58880/60000 (98%)]\tLoss: 88.432587\tBCE:70.0920\tKLD:18.3406\tC_loss:0.0000\n",
      "====> Epoch: 140 Average loss: 87.3368\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.1449\n",
      "Random number: 1\n",
      "Train Epoch: 141 [0/60000 (0%)]\tLoss: 88.350342\tBCE:70.1880\tKLD:18.1623\tC_loss:0.0000\n",
      "Train Epoch: 141 [2560/60000 (4%)]\tLoss: 87.143295\tBCE:69.2851\tKLD:17.8582\tC_loss:0.0000\n",
      "Train Epoch: 141 [5120/60000 (9%)]\tLoss: 87.035240\tBCE:69.4189\tKLD:17.6163\tC_loss:0.0000\n",
      "Train Epoch: 141 [7680/60000 (13%)]\tLoss: 85.134575\tBCE:67.1815\tKLD:17.9531\tC_loss:0.0000\n",
      "Train Epoch: 141 [10240/60000 (17%)]\tLoss: 85.657753\tBCE:68.2199\tKLD:17.4378\tC_loss:0.0000\n",
      "Train Epoch: 141 [12800/60000 (21%)]\tLoss: 88.494995\tBCE:70.4796\tKLD:18.0154\tC_loss:0.0000\n",
      "Train Epoch: 141 [15360/60000 (26%)]\tLoss: 88.484505\tBCE:71.0064\tKLD:17.4781\tC_loss:0.0000\n",
      "Train Epoch: 141 [17920/60000 (30%)]\tLoss: 87.020187\tBCE:69.4299\tKLD:17.5903\tC_loss:0.0000\n",
      "Train Epoch: 141 [20480/60000 (34%)]\tLoss: 88.138153\tBCE:70.1353\tKLD:18.0028\tC_loss:0.0000\n",
      "Train Epoch: 141 [23040/60000 (38%)]\tLoss: 86.336487\tBCE:69.1559\tKLD:17.1806\tC_loss:0.0000\n",
      "Train Epoch: 141 [25600/60000 (43%)]\tLoss: 86.128670\tBCE:68.2759\tKLD:17.8527\tC_loss:0.0000\n",
      "Train Epoch: 141 [28160/60000 (47%)]\tLoss: 86.406479\tBCE:69.0140\tKLD:17.3925\tC_loss:0.0000\n",
      "Train Epoch: 141 [30720/60000 (51%)]\tLoss: 87.288025\tBCE:69.6640\tKLD:17.6240\tC_loss:0.0000\n",
      "Train Epoch: 141 [33280/60000 (56%)]\tLoss: 87.418091\tBCE:69.8513\tKLD:17.5668\tC_loss:0.0000\n",
      "Train Epoch: 141 [35840/60000 (60%)]\tLoss: 88.609543\tBCE:70.4944\tKLD:18.1151\tC_loss:0.0000\n",
      "Train Epoch: 141 [38400/60000 (64%)]\tLoss: 88.156830\tBCE:70.1293\tKLD:18.0275\tC_loss:0.0000\n",
      "Train Epoch: 141 [40960/60000 (68%)]\tLoss: 88.090996\tBCE:70.4667\tKLD:17.6243\tC_loss:0.0000\n",
      "Train Epoch: 141 [43520/60000 (73%)]\tLoss: 89.383972\tBCE:71.5619\tKLD:17.8221\tC_loss:0.0000\n",
      "Train Epoch: 141 [46080/60000 (77%)]\tLoss: 89.317963\tBCE:71.1105\tKLD:18.2075\tC_loss:0.0000\n",
      "Train Epoch: 141 [48640/60000 (81%)]\tLoss: 87.139740\tBCE:69.4815\tKLD:17.6583\tC_loss:0.0000\n",
      "Train Epoch: 141 [51200/60000 (85%)]\tLoss: 86.481400\tBCE:69.0046\tKLD:17.4768\tC_loss:0.0000\n",
      "Train Epoch: 141 [53760/60000 (90%)]\tLoss: 87.276375\tBCE:69.2915\tKLD:17.9849\tC_loss:0.0000\n",
      "Train Epoch: 141 [56320/60000 (94%)]\tLoss: 88.404358\tBCE:70.7141\tKLD:17.6902\tC_loss:0.0000\n",
      "Train Epoch: 141 [58880/60000 (98%)]\tLoss: 86.740967\tBCE:69.2099\tKLD:17.5311\tC_loss:0.0000\n",
      "====> Epoch: 141 Average loss: 87.3902\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0475\n",
      "Train Epoch: 142 [0/60000 (0%)]\tLoss: 87.704536\tBCE:70.1340\tKLD:17.5706\tC_loss:0.0000\n",
      "Train Epoch: 142 [2560/60000 (4%)]\tLoss: 87.498680\tBCE:69.7504\tKLD:17.7483\tC_loss:0.0000\n",
      "Train Epoch: 142 [5120/60000 (9%)]\tLoss: 86.154083\tBCE:68.8059\tKLD:17.3482\tC_loss:0.0000\n",
      "Train Epoch: 142 [7680/60000 (13%)]\tLoss: 85.059723\tBCE:67.3595\tKLD:17.7002\tC_loss:0.0000\n",
      "Train Epoch: 142 [10240/60000 (17%)]\tLoss: 90.168419\tBCE:72.0195\tKLD:18.1489\tC_loss:0.0000\n",
      "Train Epoch: 142 [12800/60000 (21%)]\tLoss: 88.051651\tBCE:70.5941\tKLD:17.4576\tC_loss:0.0000\n",
      "Train Epoch: 142 [15360/60000 (26%)]\tLoss: 88.381302\tBCE:70.1284\tKLD:18.2529\tC_loss:0.0000\n",
      "Train Epoch: 142 [17920/60000 (30%)]\tLoss: 89.129333\tBCE:71.0971\tKLD:18.0322\tC_loss:0.0000\n",
      "Train Epoch: 142 [20480/60000 (34%)]\tLoss: 86.692917\tBCE:69.1863\tKLD:17.5066\tC_loss:0.0000\n",
      "Train Epoch: 142 [23040/60000 (38%)]\tLoss: 89.300323\tBCE:71.3037\tKLD:17.9966\tC_loss:0.0000\n",
      "Train Epoch: 142 [25600/60000 (43%)]\tLoss: 87.992226\tBCE:69.6800\tKLD:18.3122\tC_loss:0.0000\n",
      "Train Epoch: 142 [28160/60000 (47%)]\tLoss: 84.803337\tBCE:67.3472\tKLD:17.4562\tC_loss:0.0000\n",
      "Train Epoch: 142 [30720/60000 (51%)]\tLoss: 86.539803\tBCE:69.3621\tKLD:17.1777\tC_loss:0.0000\n",
      "Train Epoch: 142 [33280/60000 (56%)]\tLoss: 85.952843\tBCE:68.0069\tKLD:17.9460\tC_loss:0.0000\n",
      "Train Epoch: 142 [35840/60000 (60%)]\tLoss: 88.353394\tBCE:70.5195\tKLD:17.8339\tC_loss:0.0000\n",
      "Train Epoch: 142 [38400/60000 (64%)]\tLoss: 86.912689\tBCE:69.3656\tKLD:17.5471\tC_loss:0.0000\n",
      "Train Epoch: 142 [40960/60000 (68%)]\tLoss: 89.390717\tBCE:71.1234\tKLD:18.2673\tC_loss:0.0000\n",
      "Train Epoch: 142 [43520/60000 (73%)]\tLoss: 88.122307\tBCE:70.4625\tKLD:17.6598\tC_loss:0.0000\n",
      "Train Epoch: 142 [46080/60000 (77%)]\tLoss: 87.684151\tBCE:69.5540\tKLD:18.1301\tC_loss:0.0000\n",
      "Train Epoch: 142 [48640/60000 (81%)]\tLoss: 89.267769\tBCE:71.0468\tKLD:18.2210\tC_loss:0.0000\n",
      "Train Epoch: 142 [51200/60000 (85%)]\tLoss: 87.296181\tBCE:69.5171\tKLD:17.7791\tC_loss:0.0000\n",
      "Train Epoch: 142 [53760/60000 (90%)]\tLoss: 86.902763\tBCE:69.5612\tKLD:17.3415\tC_loss:0.0000\n",
      "Train Epoch: 142 [56320/60000 (94%)]\tLoss: 85.911934\tBCE:67.9814\tKLD:17.9306\tC_loss:0.0000\n",
      "Train Epoch: 142 [58880/60000 (98%)]\tLoss: 86.198311\tBCE:68.5875\tKLD:17.6108\tC_loss:0.0000\n",
      "====> Epoch: 142 Average loss: 87.2282\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.2902\n",
      "Train Epoch: 143 [0/60000 (0%)]\tLoss: 89.779770\tBCE:72.0911\tKLD:17.6887\tC_loss:0.0000\n",
      "Train Epoch: 143 [2560/60000 (4%)]\tLoss: 87.035446\tBCE:69.2838\tKLD:17.7516\tC_loss:0.0000\n",
      "Train Epoch: 143 [5120/60000 (9%)]\tLoss: 86.333298\tBCE:68.8040\tKLD:17.5293\tC_loss:0.0000\n",
      "Train Epoch: 143 [7680/60000 (13%)]\tLoss: 87.571182\tBCE:69.7888\tKLD:17.7824\tC_loss:0.0000\n",
      "Train Epoch: 143 [10240/60000 (17%)]\tLoss: 87.466599\tBCE:69.7544\tKLD:17.7122\tC_loss:0.0000\n",
      "Train Epoch: 143 [12800/60000 (21%)]\tLoss: 87.290451\tBCE:69.5939\tKLD:17.6966\tC_loss:0.0000\n",
      "Train Epoch: 143 [15360/60000 (26%)]\tLoss: 88.451218\tBCE:70.7958\tKLD:17.6554\tC_loss:0.0000\n",
      "Train Epoch: 143 [17920/60000 (30%)]\tLoss: 86.917007\tBCE:69.1872\tKLD:17.7298\tC_loss:0.0000\n",
      "Train Epoch: 143 [20480/60000 (34%)]\tLoss: 88.899773\tBCE:70.7883\tKLD:18.1115\tC_loss:0.0000\n",
      "Train Epoch: 143 [23040/60000 (38%)]\tLoss: 88.654419\tBCE:70.4748\tKLD:18.1796\tC_loss:0.0000\n",
      "Train Epoch: 143 [25600/60000 (43%)]\tLoss: 86.538490\tBCE:68.6155\tKLD:17.9229\tC_loss:0.0000\n",
      "Train Epoch: 143 [28160/60000 (47%)]\tLoss: 86.026436\tBCE:68.3696\tKLD:17.6568\tC_loss:0.0000\n",
      "Train Epoch: 143 [30720/60000 (51%)]\tLoss: 88.737679\tBCE:71.1860\tKLD:17.5517\tC_loss:0.0000\n",
      "Train Epoch: 143 [33280/60000 (56%)]\tLoss: 87.458450\tBCE:69.3277\tKLD:18.1307\tC_loss:0.0000\n",
      "Train Epoch: 143 [35840/60000 (60%)]\tLoss: 85.252319\tBCE:67.9743\tKLD:17.2780\tC_loss:0.0000\n",
      "Train Epoch: 143 [38400/60000 (64%)]\tLoss: 89.441422\tBCE:71.3281\tKLD:18.1133\tC_loss:0.0000\n",
      "Train Epoch: 143 [40960/60000 (68%)]\tLoss: 87.614929\tBCE:69.7579\tKLD:17.8570\tC_loss:0.0000\n",
      "Train Epoch: 143 [43520/60000 (73%)]\tLoss: 86.937325\tBCE:69.3405\tKLD:17.5968\tC_loss:0.0000\n",
      "Train Epoch: 143 [46080/60000 (77%)]\tLoss: 86.829117\tBCE:68.8448\tKLD:17.9843\tC_loss:0.0000\n",
      "Train Epoch: 143 [48640/60000 (81%)]\tLoss: 87.848755\tBCE:70.4825\tKLD:17.3663\tC_loss:0.0000\n",
      "Train Epoch: 143 [51200/60000 (85%)]\tLoss: 84.967957\tBCE:67.0546\tKLD:17.9134\tC_loss:0.0000\n",
      "Train Epoch: 143 [53760/60000 (90%)]\tLoss: 87.668076\tBCE:70.2150\tKLD:17.4531\tC_loss:0.0000\n",
      "Train Epoch: 143 [56320/60000 (94%)]\tLoss: 85.603058\tBCE:67.7542\tKLD:17.8488\tC_loss:0.0000\n",
      "Train Epoch: 143 [58880/60000 (98%)]\tLoss: 86.237030\tBCE:68.7976\tKLD:17.4395\tC_loss:0.0000\n",
      "====> Epoch: 143 Average loss: 87.2876\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.9472\n",
      "Train Epoch: 144 [0/60000 (0%)]\tLoss: 87.602623\tBCE:69.9235\tKLD:17.6791\tC_loss:0.0000\n",
      "Train Epoch: 144 [2560/60000 (4%)]\tLoss: 88.932487\tBCE:70.6038\tKLD:18.3287\tC_loss:0.0000\n",
      "Train Epoch: 144 [5120/60000 (9%)]\tLoss: 85.095673\tBCE:67.1913\tKLD:17.9043\tC_loss:0.0000\n",
      "Train Epoch: 144 [7680/60000 (13%)]\tLoss: 84.450775\tBCE:66.6500\tKLD:17.8008\tC_loss:0.0000\n",
      "Train Epoch: 144 [10240/60000 (17%)]\tLoss: 87.008194\tBCE:68.9116\tKLD:18.0966\tC_loss:0.0000\n",
      "Train Epoch: 144 [12800/60000 (21%)]\tLoss: 85.764832\tBCE:68.0856\tKLD:17.6792\tC_loss:0.0000\n",
      "Train Epoch: 144 [15360/60000 (26%)]\tLoss: 85.648598\tBCE:67.9098\tKLD:17.7388\tC_loss:0.0000\n",
      "Train Epoch: 144 [17920/60000 (30%)]\tLoss: 85.786217\tBCE:68.2169\tKLD:17.5693\tC_loss:0.0000\n",
      "Train Epoch: 144 [20480/60000 (34%)]\tLoss: 87.182129\tBCE:69.4206\tKLD:17.7615\tC_loss:0.0000\n",
      "Train Epoch: 144 [23040/60000 (38%)]\tLoss: 84.676559\tBCE:66.7949\tKLD:17.8817\tC_loss:0.0000\n",
      "Train Epoch: 144 [25600/60000 (43%)]\tLoss: 86.216072\tBCE:68.7572\tKLD:17.4589\tC_loss:0.0000\n",
      "Train Epoch: 144 [28160/60000 (47%)]\tLoss: 86.830208\tBCE:68.9491\tKLD:17.8812\tC_loss:0.0000\n",
      "Train Epoch: 144 [30720/60000 (51%)]\tLoss: 85.431053\tBCE:67.5169\tKLD:17.9142\tC_loss:0.0000\n",
      "Train Epoch: 144 [33280/60000 (56%)]\tLoss: 87.409302\tBCE:69.1694\tKLD:18.2399\tC_loss:0.0000\n",
      "Train Epoch: 144 [35840/60000 (60%)]\tLoss: 86.113518\tBCE:68.1312\tKLD:17.9823\tC_loss:0.0000\n",
      "Train Epoch: 144 [38400/60000 (64%)]\tLoss: 87.180450\tBCE:69.3315\tKLD:17.8490\tC_loss:0.0000\n",
      "Train Epoch: 144 [40960/60000 (68%)]\tLoss: 88.268204\tBCE:70.9967\tKLD:17.2715\tC_loss:0.0000\n",
      "Train Epoch: 144 [43520/60000 (73%)]\tLoss: 89.301903\tBCE:71.0737\tKLD:18.2282\tC_loss:0.0000\n",
      "Train Epoch: 144 [46080/60000 (77%)]\tLoss: 87.462189\tBCE:69.8747\tKLD:17.5875\tC_loss:0.0000\n",
      "Train Epoch: 144 [48640/60000 (81%)]\tLoss: 86.373886\tBCE:68.9090\tKLD:17.4649\tC_loss:0.0000\n",
      "Train Epoch: 144 [51200/60000 (85%)]\tLoss: 89.345505\tBCE:71.3698\tKLD:17.9757\tC_loss:0.0000\n",
      "Train Epoch: 144 [53760/60000 (90%)]\tLoss: 87.066406\tBCE:69.7032\tKLD:17.3632\tC_loss:0.0000\n",
      "Train Epoch: 144 [56320/60000 (94%)]\tLoss: 88.901825\tBCE:71.0720\tKLD:17.8298\tC_loss:0.0000\n",
      "Train Epoch: 144 [58880/60000 (98%)]\tLoss: 88.072754\tBCE:70.2845\tKLD:17.7882\tC_loss:0.0000\n",
      "====> Epoch: 144 Average loss: 87.2774\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.8556\n",
      "Train Epoch: 145 [0/60000 (0%)]\tLoss: 85.595505\tBCE:67.7943\tKLD:17.8012\tC_loss:0.0000\n",
      "Train Epoch: 145 [2560/60000 (4%)]\tLoss: 88.229713\tBCE:70.3533\tKLD:17.8764\tC_loss:0.0000\n",
      "Train Epoch: 145 [5120/60000 (9%)]\tLoss: 87.600853\tBCE:69.7842\tKLD:17.8167\tC_loss:0.0000\n",
      "Train Epoch: 145 [7680/60000 (13%)]\tLoss: 88.389503\tBCE:70.4583\tKLD:17.9312\tC_loss:0.0000\n",
      "Train Epoch: 145 [10240/60000 (17%)]\tLoss: 86.525497\tBCE:68.8003\tKLD:17.7252\tC_loss:0.0000\n",
      "Train Epoch: 145 [12800/60000 (21%)]\tLoss: 87.837921\tBCE:70.3347\tKLD:17.5032\tC_loss:0.0000\n",
      "Train Epoch: 145 [15360/60000 (26%)]\tLoss: 87.534752\tBCE:69.7988\tKLD:17.7360\tC_loss:0.0000\n",
      "Train Epoch: 145 [17920/60000 (30%)]\tLoss: 88.925217\tBCE:70.5062\tKLD:18.4190\tC_loss:0.0000\n",
      "Train Epoch: 145 [20480/60000 (34%)]\tLoss: 87.508911\tBCE:69.5322\tKLD:17.9767\tC_loss:0.0000\n",
      "Train Epoch: 145 [23040/60000 (38%)]\tLoss: 87.269722\tBCE:69.6497\tKLD:17.6200\tC_loss:0.0000\n",
      "Train Epoch: 145 [25600/60000 (43%)]\tLoss: 86.922394\tBCE:69.1861\tKLD:17.7363\tC_loss:0.0000\n",
      "Train Epoch: 145 [28160/60000 (47%)]\tLoss: 84.609634\tBCE:66.9797\tKLD:17.6300\tC_loss:0.0000\n",
      "Train Epoch: 145 [30720/60000 (51%)]\tLoss: 89.725815\tBCE:71.7196\tKLD:18.0062\tC_loss:0.0000\n",
      "Train Epoch: 145 [33280/60000 (56%)]\tLoss: 90.106522\tBCE:71.7872\tKLD:18.3194\tC_loss:0.0000\n",
      "Train Epoch: 145 [35840/60000 (60%)]\tLoss: 85.817703\tBCE:68.4160\tKLD:17.4018\tC_loss:0.0000\n",
      "Train Epoch: 145 [38400/60000 (64%)]\tLoss: 86.142487\tBCE:68.1671\tKLD:17.9754\tC_loss:0.0000\n",
      "Train Epoch: 145 [40960/60000 (68%)]\tLoss: 87.053558\tBCE:68.9953\tKLD:18.0583\tC_loss:0.0000\n",
      "Train Epoch: 145 [43520/60000 (73%)]\tLoss: 86.091148\tBCE:68.3909\tKLD:17.7003\tC_loss:0.0000\n",
      "Train Epoch: 145 [46080/60000 (77%)]\tLoss: 85.897079\tBCE:68.6345\tKLD:17.2626\tC_loss:0.0000\n",
      "Train Epoch: 145 [48640/60000 (81%)]\tLoss: 86.702026\tBCE:69.3607\tKLD:17.3414\tC_loss:0.0000\n",
      "Train Epoch: 145 [51200/60000 (85%)]\tLoss: 86.460251\tBCE:69.0085\tKLD:17.4518\tC_loss:0.0000\n",
      "Train Epoch: 145 [53760/60000 (90%)]\tLoss: 86.087158\tBCE:68.7085\tKLD:17.3787\tC_loss:0.0000\n",
      "Train Epoch: 145 [56320/60000 (94%)]\tLoss: 86.109756\tBCE:69.0912\tKLD:17.0185\tC_loss:0.0000\n",
      "Train Epoch: 145 [58880/60000 (98%)]\tLoss: 87.238785\tBCE:69.5850\tKLD:17.6538\tC_loss:0.0000\n",
      "====> Epoch: 145 Average loss: 87.2294\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.3641\n",
      "Random number: 3\n",
      "Train Epoch: 146 [0/60000 (0%)]\tLoss: 84.715218\tBCE:67.6869\tKLD:17.0284\tC_loss:0.0000\n",
      "Train Epoch: 146 [2560/60000 (4%)]\tLoss: 86.118744\tBCE:68.1942\tKLD:17.9246\tC_loss:0.0000\n",
      "Train Epoch: 146 [5120/60000 (9%)]\tLoss: 89.024841\tBCE:71.1071\tKLD:17.9177\tC_loss:0.0000\n",
      "Train Epoch: 146 [7680/60000 (13%)]\tLoss: 86.727470\tBCE:69.0057\tKLD:17.7218\tC_loss:0.0000\n",
      "Train Epoch: 146 [10240/60000 (17%)]\tLoss: 89.772720\tBCE:71.3707\tKLD:18.4020\tC_loss:0.0000\n",
      "Train Epoch: 146 [12800/60000 (21%)]\tLoss: 85.758942\tBCE:68.5867\tKLD:17.1722\tC_loss:0.0000\n",
      "Train Epoch: 146 [15360/60000 (26%)]\tLoss: 86.517532\tBCE:68.8717\tKLD:17.6458\tC_loss:0.0000\n",
      "Train Epoch: 146 [17920/60000 (30%)]\tLoss: 88.207512\tBCE:70.0803\tKLD:18.1272\tC_loss:0.0000\n",
      "Train Epoch: 146 [20480/60000 (34%)]\tLoss: 88.103897\tBCE:70.4221\tKLD:17.6819\tC_loss:0.0000\n",
      "Train Epoch: 146 [23040/60000 (38%)]\tLoss: 85.715271\tBCE:67.9477\tKLD:17.7676\tC_loss:0.0000\n",
      "Train Epoch: 146 [25600/60000 (43%)]\tLoss: 89.667305\tBCE:71.4090\tKLD:18.2583\tC_loss:0.0000\n",
      "Train Epoch: 146 [28160/60000 (47%)]\tLoss: 87.867439\tBCE:70.0352\tKLD:17.8323\tC_loss:0.0000\n",
      "Train Epoch: 146 [30720/60000 (51%)]\tLoss: 86.937096\tBCE:69.4223\tKLD:17.5148\tC_loss:0.0000\n",
      "Train Epoch: 146 [33280/60000 (56%)]\tLoss: 88.622017\tBCE:70.2308\tKLD:18.3912\tC_loss:0.0000\n",
      "Train Epoch: 146 [35840/60000 (60%)]\tLoss: 89.045029\tBCE:71.0878\tKLD:17.9573\tC_loss:0.0000\n",
      "Train Epoch: 146 [38400/60000 (64%)]\tLoss: 85.113861\tBCE:67.6456\tKLD:17.4683\tC_loss:0.0000\n",
      "Train Epoch: 146 [40960/60000 (68%)]\tLoss: 90.267570\tBCE:71.8438\tKLD:18.4237\tC_loss:0.0000\n",
      "Train Epoch: 146 [43520/60000 (73%)]\tLoss: 86.951439\tBCE:69.2898\tKLD:17.6617\tC_loss:0.0000\n",
      "Train Epoch: 146 [46080/60000 (77%)]\tLoss: 85.950371\tBCE:68.4494\tKLD:17.5010\tC_loss:0.0000\n",
      "Train Epoch: 146 [48640/60000 (81%)]\tLoss: 88.118820\tBCE:70.0165\tKLD:18.1023\tC_loss:0.0000\n",
      "Train Epoch: 146 [51200/60000 (85%)]\tLoss: 88.764130\tBCE:70.5041\tKLD:18.2600\tC_loss:0.0000\n",
      "Train Epoch: 146 [53760/60000 (90%)]\tLoss: 85.136200\tBCE:68.0244\tKLD:17.1118\tC_loss:0.0000\n",
      "Train Epoch: 146 [56320/60000 (94%)]\tLoss: 88.594872\tBCE:70.9327\tKLD:17.6621\tC_loss:0.0000\n",
      "Train Epoch: 146 [58880/60000 (98%)]\tLoss: 87.911865\tBCE:70.1110\tKLD:17.8009\tC_loss:0.0000\n",
      "====> Epoch: 146 Average loss: 87.1573\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.3531\n",
      "Train Epoch: 147 [0/60000 (0%)]\tLoss: 86.877037\tBCE:68.7588\tKLD:18.1183\tC_loss:0.0000\n",
      "Train Epoch: 147 [2560/60000 (4%)]\tLoss: 86.653534\tBCE:68.7440\tKLD:17.9095\tC_loss:0.0000\n",
      "Train Epoch: 147 [5120/60000 (9%)]\tLoss: 88.158142\tBCE:70.1832\tKLD:17.9749\tC_loss:0.0000\n",
      "Train Epoch: 147 [7680/60000 (13%)]\tLoss: 86.459373\tBCE:68.5795\tKLD:17.8799\tC_loss:0.0000\n",
      "Train Epoch: 147 [10240/60000 (17%)]\tLoss: 88.101326\tBCE:69.8904\tKLD:18.2109\tC_loss:0.0000\n",
      "Train Epoch: 147 [12800/60000 (21%)]\tLoss: 88.579536\tBCE:70.3974\tKLD:18.1822\tC_loss:0.0000\n",
      "Train Epoch: 147 [15360/60000 (26%)]\tLoss: 86.705063\tBCE:69.2108\tKLD:17.4943\tC_loss:0.0000\n",
      "Train Epoch: 147 [17920/60000 (30%)]\tLoss: 87.710564\tBCE:69.3687\tKLD:18.3419\tC_loss:0.0000\n",
      "Train Epoch: 147 [20480/60000 (34%)]\tLoss: 87.225143\tBCE:69.6380\tKLD:17.5871\tC_loss:0.0000\n",
      "Train Epoch: 147 [23040/60000 (38%)]\tLoss: 86.215889\tBCE:68.3918\tKLD:17.8240\tC_loss:0.0000\n",
      "Train Epoch: 147 [25600/60000 (43%)]\tLoss: 87.217545\tBCE:69.3896\tKLD:17.8280\tC_loss:0.0000\n",
      "Train Epoch: 147 [28160/60000 (47%)]\tLoss: 84.863472\tBCE:67.9386\tKLD:16.9248\tC_loss:0.0000\n",
      "Train Epoch: 147 [30720/60000 (51%)]\tLoss: 88.168617\tBCE:70.9069\tKLD:17.2617\tC_loss:0.0000\n",
      "Train Epoch: 147 [33280/60000 (56%)]\tLoss: 86.964340\tBCE:68.7650\tKLD:18.1993\tC_loss:0.0000\n",
      "Train Epoch: 147 [35840/60000 (60%)]\tLoss: 88.695328\tBCE:70.9811\tKLD:17.7143\tC_loss:0.0000\n",
      "Train Epoch: 147 [38400/60000 (64%)]\tLoss: 88.965271\tBCE:70.3906\tKLD:18.5746\tC_loss:0.0000\n",
      "Train Epoch: 147 [40960/60000 (68%)]\tLoss: 89.532234\tBCE:71.6595\tKLD:17.8727\tC_loss:0.0000\n",
      "Train Epoch: 147 [43520/60000 (73%)]\tLoss: 87.655128\tBCE:69.5355\tKLD:18.1197\tC_loss:0.0000\n",
      "Train Epoch: 147 [46080/60000 (77%)]\tLoss: 86.272629\tBCE:68.3739\tKLD:17.8987\tC_loss:0.0000\n",
      "Train Epoch: 147 [48640/60000 (81%)]\tLoss: 85.992378\tBCE:68.3970\tKLD:17.5954\tC_loss:0.0000\n",
      "Train Epoch: 147 [51200/60000 (85%)]\tLoss: 87.690323\tBCE:70.0820\tKLD:17.6083\tC_loss:0.0000\n",
      "Train Epoch: 147 [53760/60000 (90%)]\tLoss: 86.339546\tBCE:68.6889\tKLD:17.6506\tC_loss:0.0000\n",
      "Train Epoch: 147 [56320/60000 (94%)]\tLoss: 89.177368\tBCE:70.8300\tKLD:18.3473\tC_loss:0.0000\n",
      "Train Epoch: 147 [58880/60000 (98%)]\tLoss: 87.971176\tBCE:69.9536\tKLD:18.0176\tC_loss:0.0000\n",
      "====> Epoch: 147 Average loss: 87.2101\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.3694\n",
      "Train Epoch: 148 [0/60000 (0%)]\tLoss: 88.422546\tBCE:70.1104\tKLD:18.3121\tC_loss:0.0000\n",
      "Train Epoch: 148 [2560/60000 (4%)]\tLoss: 88.416595\tBCE:69.9929\tKLD:18.4237\tC_loss:0.0000\n",
      "Train Epoch: 148 [5120/60000 (9%)]\tLoss: 88.457336\tBCE:69.9933\tKLD:18.4641\tC_loss:0.0000\n",
      "Train Epoch: 148 [7680/60000 (13%)]\tLoss: 87.110138\tBCE:69.5268\tKLD:17.5833\tC_loss:0.0000\n",
      "Train Epoch: 148 [10240/60000 (17%)]\tLoss: 88.797043\tBCE:70.4906\tKLD:18.3064\tC_loss:0.0000\n",
      "Train Epoch: 148 [12800/60000 (21%)]\tLoss: 89.009811\tBCE:70.9847\tKLD:18.0251\tC_loss:0.0000\n",
      "Train Epoch: 148 [15360/60000 (26%)]\tLoss: 84.481148\tBCE:67.2531\tKLD:17.2280\tC_loss:0.0000\n",
      "Train Epoch: 148 [17920/60000 (30%)]\tLoss: 87.135452\tBCE:69.0514\tKLD:18.0841\tC_loss:0.0000\n",
      "Train Epoch: 148 [20480/60000 (34%)]\tLoss: 88.340485\tBCE:70.1616\tKLD:18.1789\tC_loss:0.0000\n",
      "Train Epoch: 148 [23040/60000 (38%)]\tLoss: 88.182968\tBCE:70.3318\tKLD:17.8512\tC_loss:0.0000\n",
      "Train Epoch: 148 [25600/60000 (43%)]\tLoss: 86.225311\tBCE:68.6106\tKLD:17.6147\tC_loss:0.0000\n",
      "Train Epoch: 148 [28160/60000 (47%)]\tLoss: 85.913010\tBCE:67.9674\tKLD:17.9456\tC_loss:0.0000\n",
      "Train Epoch: 148 [30720/60000 (51%)]\tLoss: 88.450722\tBCE:70.3589\tKLD:18.0918\tC_loss:0.0000\n",
      "Train Epoch: 148 [33280/60000 (56%)]\tLoss: 88.492836\tBCE:70.4502\tKLD:18.0426\tC_loss:0.0000\n",
      "Train Epoch: 148 [35840/60000 (60%)]\tLoss: 84.826706\tBCE:67.4032\tKLD:17.4235\tC_loss:0.0000\n",
      "Train Epoch: 148 [38400/60000 (64%)]\tLoss: 90.906975\tBCE:72.5391\tKLD:18.3679\tC_loss:0.0000\n",
      "Train Epoch: 148 [40960/60000 (68%)]\tLoss: 88.876183\tBCE:70.8360\tKLD:18.0401\tC_loss:0.0000\n",
      "Train Epoch: 148 [43520/60000 (73%)]\tLoss: 88.366150\tBCE:70.1559\tKLD:18.2103\tC_loss:0.0000\n",
      "Train Epoch: 148 [46080/60000 (77%)]\tLoss: 83.630882\tBCE:66.7002\tKLD:16.9307\tC_loss:0.0000\n",
      "Train Epoch: 148 [48640/60000 (81%)]\tLoss: 85.600281\tBCE:67.7684\tKLD:17.8319\tC_loss:0.0000\n",
      "Train Epoch: 148 [51200/60000 (85%)]\tLoss: 85.937561\tBCE:68.0551\tKLD:17.8825\tC_loss:0.0000\n",
      "Train Epoch: 148 [53760/60000 (90%)]\tLoss: 88.237549\tBCE:70.9512\tKLD:17.2864\tC_loss:0.0000\n",
      "Train Epoch: 148 [56320/60000 (94%)]\tLoss: 87.590286\tBCE:69.8717\tKLD:17.7186\tC_loss:0.0000\n",
      "Train Epoch: 148 [58880/60000 (98%)]\tLoss: 87.892174\tBCE:69.6075\tKLD:18.2847\tC_loss:0.0000\n",
      "====> Epoch: 148 Average loss: 87.1162\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.1263\n",
      "Train Epoch: 149 [0/60000 (0%)]\tLoss: 85.521942\tBCE:68.3539\tKLD:17.1681\tC_loss:0.0000\n",
      "Train Epoch: 149 [2560/60000 (4%)]\tLoss: 87.694267\tBCE:69.5014\tKLD:18.1929\tC_loss:0.0000\n",
      "Train Epoch: 149 [5120/60000 (9%)]\tLoss: 87.468124\tBCE:69.6219\tKLD:17.8462\tC_loss:0.0000\n",
      "Train Epoch: 149 [7680/60000 (13%)]\tLoss: 89.525383\tBCE:71.3887\tKLD:18.1367\tC_loss:0.0000\n",
      "Train Epoch: 149 [10240/60000 (17%)]\tLoss: 86.785828\tBCE:69.2492\tKLD:17.5367\tC_loss:0.0000\n",
      "Train Epoch: 149 [12800/60000 (21%)]\tLoss: 88.110970\tBCE:69.9591\tKLD:18.1519\tC_loss:0.0000\n",
      "Train Epoch: 149 [15360/60000 (26%)]\tLoss: 87.606888\tBCE:69.3312\tKLD:18.2757\tC_loss:0.0000\n",
      "Train Epoch: 149 [17920/60000 (30%)]\tLoss: 87.751114\tBCE:70.2412\tKLD:17.5099\tC_loss:0.0000\n",
      "Train Epoch: 149 [20480/60000 (34%)]\tLoss: 86.912354\tBCE:68.6070\tKLD:18.3053\tC_loss:0.0000\n",
      "Train Epoch: 149 [23040/60000 (38%)]\tLoss: 85.566795\tBCE:68.3278\tKLD:17.2390\tC_loss:0.0000\n",
      "Train Epoch: 149 [25600/60000 (43%)]\tLoss: 88.462646\tBCE:70.3786\tKLD:18.0840\tC_loss:0.0000\n",
      "Train Epoch: 149 [28160/60000 (47%)]\tLoss: 86.226326\tBCE:68.6060\tKLD:17.6204\tC_loss:0.0000\n",
      "Train Epoch: 149 [30720/60000 (51%)]\tLoss: 86.611252\tBCE:69.0250\tKLD:17.5863\tC_loss:0.0000\n",
      "Train Epoch: 149 [33280/60000 (56%)]\tLoss: 88.628891\tBCE:70.6650\tKLD:17.9639\tC_loss:0.0000\n",
      "Train Epoch: 149 [35840/60000 (60%)]\tLoss: 87.663742\tBCE:69.7268\tKLD:17.9370\tC_loss:0.0000\n",
      "Train Epoch: 149 [38400/60000 (64%)]\tLoss: 88.133392\tBCE:69.6814\tKLD:18.4520\tC_loss:0.0000\n",
      "Train Epoch: 149 [40960/60000 (68%)]\tLoss: 85.517059\tBCE:68.0498\tKLD:17.4673\tC_loss:0.0000\n",
      "Train Epoch: 149 [43520/60000 (73%)]\tLoss: 84.402969\tBCE:66.9873\tKLD:17.4156\tC_loss:0.0000\n",
      "Train Epoch: 149 [46080/60000 (77%)]\tLoss: 86.042458\tBCE:68.4511\tKLD:17.5914\tC_loss:0.0000\n",
      "Train Epoch: 149 [48640/60000 (81%)]\tLoss: 88.120155\tBCE:69.9597\tKLD:18.1605\tC_loss:0.0000\n",
      "Train Epoch: 149 [51200/60000 (85%)]\tLoss: 88.262741\tBCE:70.4214\tKLD:17.8413\tC_loss:0.0000\n",
      "Train Epoch: 149 [53760/60000 (90%)]\tLoss: 88.054535\tBCE:69.9869\tKLD:18.0677\tC_loss:0.0000\n",
      "Train Epoch: 149 [56320/60000 (94%)]\tLoss: 85.359406\tBCE:68.0414\tKLD:17.3180\tC_loss:0.0000\n",
      "Train Epoch: 149 [58880/60000 (98%)]\tLoss: 89.052818\tBCE:70.9371\tKLD:18.1157\tC_loss:0.0000\n",
      "====> Epoch: 149 Average loss: 87.0837\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.8208\n",
      "Train Epoch: 150 [0/60000 (0%)]\tLoss: 86.462769\tBCE:68.6239\tKLD:17.8388\tC_loss:0.0000\n",
      "Train Epoch: 150 [2560/60000 (4%)]\tLoss: 85.558792\tBCE:67.8056\tKLD:17.7532\tC_loss:0.0000\n",
      "Train Epoch: 150 [5120/60000 (9%)]\tLoss: 86.596680\tBCE:68.9848\tKLD:17.6119\tC_loss:0.0000\n",
      "Train Epoch: 150 [7680/60000 (13%)]\tLoss: 89.312057\tBCE:71.0350\tKLD:18.2771\tC_loss:0.0000\n",
      "Train Epoch: 150 [10240/60000 (17%)]\tLoss: 88.270760\tBCE:70.3702\tKLD:17.9006\tC_loss:0.0000\n",
      "Train Epoch: 150 [12800/60000 (21%)]\tLoss: 86.499802\tBCE:68.6655\tKLD:17.8343\tC_loss:0.0000\n",
      "Train Epoch: 150 [15360/60000 (26%)]\tLoss: 88.461639\tBCE:70.6531\tKLD:17.8086\tC_loss:0.0000\n",
      "Train Epoch: 150 [17920/60000 (30%)]\tLoss: 87.305511\tBCE:69.3903\tKLD:17.9152\tC_loss:0.0000\n",
      "Train Epoch: 150 [20480/60000 (34%)]\tLoss: 84.775940\tBCE:67.0004\tKLD:17.7755\tC_loss:0.0000\n",
      "Train Epoch: 150 [23040/60000 (38%)]\tLoss: 87.615158\tBCE:69.4073\tKLD:18.2079\tC_loss:0.0000\n",
      "Train Epoch: 150 [25600/60000 (43%)]\tLoss: 87.448669\tBCE:69.6897\tKLD:17.7590\tC_loss:0.0000\n",
      "Train Epoch: 150 [28160/60000 (47%)]\tLoss: 87.741776\tBCE:69.5836\tKLD:18.1582\tC_loss:0.0000\n",
      "Train Epoch: 150 [30720/60000 (51%)]\tLoss: 85.115387\tBCE:67.7662\tKLD:17.3492\tC_loss:0.0000\n",
      "Train Epoch: 150 [33280/60000 (56%)]\tLoss: 87.303894\tBCE:69.4113\tKLD:17.8926\tC_loss:0.0000\n",
      "Train Epoch: 150 [35840/60000 (60%)]\tLoss: 86.947792\tBCE:69.0638\tKLD:17.8840\tC_loss:0.0000\n",
      "Train Epoch: 150 [38400/60000 (64%)]\tLoss: 86.165855\tBCE:67.9923\tKLD:18.1735\tC_loss:0.0000\n",
      "Train Epoch: 150 [40960/60000 (68%)]\tLoss: 87.256607\tBCE:69.2648\tKLD:17.9918\tC_loss:0.0000\n",
      "Train Epoch: 150 [43520/60000 (73%)]\tLoss: 86.770424\tBCE:69.4508\tKLD:17.3196\tC_loss:0.0000\n",
      "Train Epoch: 150 [46080/60000 (77%)]\tLoss: 89.842796\tBCE:72.0064\tKLD:17.8364\tC_loss:0.0000\n",
      "Train Epoch: 150 [48640/60000 (81%)]\tLoss: 84.503609\tBCE:67.1168\tKLD:17.3868\tC_loss:0.0000\n",
      "Train Epoch: 150 [51200/60000 (85%)]\tLoss: 87.172089\tBCE:69.1350\tKLD:18.0371\tC_loss:0.0000\n",
      "Train Epoch: 150 [53760/60000 (90%)]\tLoss: 85.613739\tBCE:68.2332\tKLD:17.3805\tC_loss:0.0000\n",
      "Train Epoch: 150 [56320/60000 (94%)]\tLoss: 88.296631\tBCE:70.1674\tKLD:18.1292\tC_loss:0.0000\n",
      "Train Epoch: 150 [58880/60000 (98%)]\tLoss: 86.183914\tBCE:68.4003\tKLD:17.7836\tC_loss:0.0000\n",
      "====> Epoch: 150 Average loss: 87.1763\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.1507\n",
      "Random number: 8\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "    # Generate random digits every n epochs\n",
    "    with torch.inference_mode():\n",
    "        if epoch%5==0:\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "        \n",
    "            c = np.zeros(shape=(sample.shape[0],))\n",
    "            rand = np.random.randint(0, 10)\n",
    "            print(f\"Random number: {rand}\")\n",
    "            c[:] = rand\n",
    "            c = torch.FloatTensor(c)\n",
    "            c = c.to(torch.int64)\n",
    "            c = c.to(device)\n",
    "            c = F.one_hot(c, cond_shape)\n",
    "            sample = model.decoder((sample, c)).cpu()\n",
    "            \n",
    "            generated_image = sample[:, 0:sample.shape[1]]\n",
    "            \n",
    "            \n",
    "            save_image(generated_image.view(64, 1, 28, 28),\n",
    "                    'results/sample_' + str(epoch) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    sample = torch.randn(64, 20).to(device)\n",
    "        \n",
    "    c = np.zeros(shape=(sample.shape[0],))\n",
    "    num = i\n",
    "    c[:] = num\n",
    "    c = torch.FloatTensor(c)\n",
    "    c = c.to(torch.int64)\n",
    "    c = c.to(device)\n",
    "    c = F.one_hot(c, cond_shape)\n",
    "    sample = model.decoder((sample, c)).cpu()\n",
    "\n",
    "    generated_image = sample[:, 0:sample.shape[1]]\n",
    "\n",
    "\n",
    "    save_image(generated_image.view(64, 1, 28, 28),\n",
    "            'results/generated_conv_' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the digit \"2\" from gaussian noise\n",
      "Classifier says the below image is a 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f47e52f3640>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "i = random.randint(0, 9)\n",
    "sample = torch.randn(1, 20).to(device)\n",
    "c = np.zeros(shape=(sample.shape[0],))\n",
    "num = i\n",
    "print(f\"Generating the digit \\\"{i}\\\" from gaussian noise\")\n",
    "c[:] = num\n",
    "c = torch.FloatTensor(c)\n",
    "c = c.to(torch.int64)\n",
    "c = c.to(device)\n",
    "c = F.one_hot(c, cond_shape)\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    sample = model.decoder((sample, c))\n",
    "    sample = sample.reshape([1, 1, 28, 28])\n",
    "    c_out = model.classifier(sample)\n",
    "\n",
    "c_out = torch.argmax(c_out).item()\n",
    "print(f\"Classifier says the below image is a {c_out}\")\n",
    "plt.imshow(sample[0].cpu().squeeze())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
