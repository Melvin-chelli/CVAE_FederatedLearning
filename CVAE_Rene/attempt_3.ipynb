{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f20acb07a50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.distributions import Independent, Normal\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure as ssim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "PRINT_REQ= False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "BATCH_SIZE=256\n",
    "EPOCHS=50\n",
    "\n",
    "cond_shape=10\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size = 128\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_data = datasets.MNIST(root=\"../data\", train=True, \n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(root=\"../data\", train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=1)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True, num_workers=1)\n",
    "\n",
    "flat_img=torch.flatten(train_data[0][0])\n",
    "flat_shape=list(flat_img.shape)\n",
    "flat_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_debug(data):\n",
    "    if PRINT_REQ:\n",
    "        print(data)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_x, dim_y, dim_z):\n",
    "\n",
    "        super().__init__()\n",
    "         # Encoder layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=11, out_channels=32, kernel_size=5, stride=1,padding='same')\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=2,padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1,padding='same')\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, stride=2,padding=0)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=80, kernel_size=7, stride=1,padding='valid')\n",
    "        self.lin1 = nn.Linear(in_features=80, out_features=20)\n",
    "        self.lin2 = nn.Linear(in_features=80, out_features=20)\n",
    "\n",
    "        # reparameterization\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = inputs[0].to(device)\n",
    "        y = inputs[1].to(device)\n",
    "        \n",
    "        # y = F.one_hot(y, 10).to(device)\n",
    "        y = y.view(-1, 10, 1, 1).to(device)\n",
    "        \n",
    "        ones = torch.ones(x.size()[0], \n",
    "                            10,\n",
    "                            x.size()[2], \n",
    "                            x.size()[3], \n",
    "                            dtype=x.dtype).to(device)\n",
    "        y = ones * y\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        \n",
    "        print_debug(f\"input shape: {x.shape}\")\n",
    "        x = F.relu(self.conv1(x))\n",
    "        print_debug(x.shape)\n",
    "        # 32, 28, 28\n",
    "        x = F.pad(x, (0,3,0,3))\n",
    "        print_debug(x.shape)\n",
    "        # 32, 31, 31\n",
    "        x = F.relu(self.conv2(x))\n",
    "        print_debug(x.shape)\n",
    "        # 32, 14, 14\n",
    "        x = F.relu(self.conv3(x))\n",
    "        print_debug(x.shape)\n",
    "        # 64, 14, 14\n",
    "        x = F.pad(x, (0,3,0,3))\n",
    "        print_debug(x.shape)\n",
    "        # 64, 17, 17\n",
    "        x = F.relu(self.conv4(x))\n",
    "        print_debug(x.shape)\n",
    "        # 64, 7, 7\n",
    "        x = F.relu(self.conv5(x))\n",
    "        print_debug(x.shape)\n",
    "        # 80, 1, 1\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "        print_debug(f\"After flatten shape: {x.shape}\")\n",
    "        # 80\n",
    "        # print_debug(f\"Concatenating {x.shape} with {y.shape}\")\n",
    "        # concat = torch.cat([x, y], dim=-1)\n",
    "        # print_debug(f\"After concatenation shape: {concat.shape}\")\n",
    "        # 90\n",
    "        # loc=torch.zeros(mu_logvar.shape)\n",
    "        # scale=torch.ones(mu_logvar.shape)\n",
    "        # diagn = Independent(Normal(loc, scale), 1)\n",
    "        mu = self.lin1(x)\n",
    "        print_debug(f\"mu shape: {mu.shape}\")\n",
    "        # 20\n",
    "        logvar = self.lin2(x)\n",
    "        print_debug(f\"logvar shape: {logvar.shape}\")\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        print_debug(f\"Returning shape {z.shape}\")\n",
    "        return  mu, logvar, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim_y, dim_z):\n",
    "        super().__init__()\n",
    "        self.dim_z = dim_z\n",
    "        self.dim_y = dim_y\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=30, out_channels=64, kernel_size=7, stride=1, padding=0) # valid means no pad\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=5, stride=1, padding=2)\n",
    "        self.deconv3 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=5, stride=2, padding=2, output_padding=1) # pad operation added in forward\n",
    "        self.deconv4 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.deconv5 = nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=5, stride=2, padding=2, output_padding=1) # pad operation added in forward\n",
    "        self.deconv6 = nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=5, stride=1,padding='same')\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs[0].to(device)#.unsqueeze(dim=0)\n",
    "        y = inputs[1].to(device)\n",
    "        print_debug(f\"latent space shape: {x.shape}, labels shape: {y.shape}\")\n",
    "        x = torch.cat([x, y], dim=1)\n",
    "        x = torch.reshape(x, (-1, self.dim_z+self.dim_y, 1, 1))\n",
    "        print_debug(f\"After concatenation shape: {x.shape}\")\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        print_debug(f\"ConvTrans1 output shape: {x.shape}\")\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        print_debug(f\"ConvTrans2 output shape: {x.shape}\")\n",
    "        x = F.pad(x, (0,0,0,0))\n",
    "        x = F.relu(self.deconv3(x))\n",
    "        print_debug(f\"ConvTrans3 output shape: {x.shape}\")\n",
    "        x = F.relu(self.deconv4(x))\n",
    "        print_debug(f\"ConvTrans4 output shape: {x.shape}\")\n",
    "        # x = F.pad(x, (0,3,0,3))\n",
    "        x = F.relu(self.deconv5(x))\n",
    "        print_debug(f\"ConvTrans5 output shape: {x.shape}\")\n",
    "        x = F.relu(self.deconv6(x))\n",
    "        print_debug(f\"ConvTrans6 output shape: {x.shape}\")\n",
    "        x = torch.sigmoid(self.conv(x))\n",
    "        print_debug(f\"Conv output shape: {x.shape}\")\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "        print_debug(f\"After flatten shape: {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, dim_x, dim_y, dim_z):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=flat_shape[0], out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=dim_y),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        #Encoder \n",
    "        self.encoder = Encoder(dim_x=dim_x, dim_y=dim_y, dim_z=dim_z)\n",
    "\n",
    "        #Decoder\n",
    "        self.decoder = Decoder(dim_y=dim_y, dim_z=dim_z)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x, y = inputs      \n",
    "        x = x.to(device)\n",
    "        y = F.one_hot(y, 10).to(device)  \n",
    "        print_debug(f\"Inputs shape: {x.shape} and labels: {y.shape}\")\n",
    "        c_out = self.classifier(x)\n",
    "        mu, logvar, z = self.encoder((x,y))\n",
    "        out = self.decoder((z, y))\n",
    "        print_debug(f\"decoder output shape is: {out.shape}\")\n",
    "        return mu, logvar, out, c_out\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CVAE(dim_x=(28, 28, 1), dim_y=10, dim_z=20).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_fn(recon, x, mu, logvar, c_out, y_onehot):\n",
    "    y_onehot1 = y_onehot.type(torch.FloatTensor).to(device)\n",
    "    # print(c_out.shape, y_onehot.shape, c_out.dtype, y_onehot.dtype)\n",
    "    classif_loss = torch.nn.BCELoss()(c_out, y_onehot1)\n",
    "    BCE = F.binary_cross_entropy(recon, x, reduction='sum')        \n",
    "    KLD = -0.5*torch.sum(1+logvar-mu.pow(2)-logvar.exp())\n",
    "    return classif_loss+BCE+KLD, classif_loss, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one iteration to validate output shapes with PRINT_DEBUG = True\n",
    "for i, (x, y) in enumerate(train_dataloader):    \n",
    "    model((x,y))\n",
    "    # x = x.to(device)\n",
    "    # print(f\"ysghape is {y.shape}\")\n",
    "    # y = F.one_hot(y, 10).to(device)\n",
    "    # y = y.view(-1, 10, 1, 1).to(device)\n",
    "    \n",
    "    # ones = torch.ones(x.size()[0], \n",
    "    #                     10,\n",
    "    #                     x.size()[2], \n",
    "    #                     x.size()[3], \n",
    "    #                     dtype=x.dtype).to(device)\n",
    "    # y = ones * y\n",
    "    # print(ones.shape, y.shape)\n",
    "    # x = torch.cat((x, y), dim=1)\n",
    "    # print(x.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Truth labels for predictions.\n",
    "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
    "\n",
    "    Returns:\n",
    "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
    "    \"\"\"\n",
    "    \n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    classif_accuracy = 0\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X = X.to(device) #[64, 1, 28, 28]\n",
    "        y = y.to(device)\n",
    "        \n",
    "        \n",
    "        # 1. Forward pass\n",
    "        mu, logvar, recon_batch, c_out = model((X, y))\n",
    "        # print(f\"---------------{torch.argmax(c_out, dim=1).shape}\")\n",
    "        # print(f\"---------------{y.shape}\")\n",
    "        flat_data = X.view(-1, flat_shape[0]).to(device)                            \n",
    "        y_onehot = F.one_hot(y, cond_shape).to(device)\n",
    "        inp = torch.cat((flat_data, y_onehot), 1)\n",
    "        \n",
    "        # 2. Calculate loss\n",
    "        loss, C_loss, BCE, KLD = loss_fn(recon_batch, flat_data, mu, logvar, c_out, y_onehot)\n",
    "        train_loss += loss.item()\n",
    "        classif_accuracy += accuracy_fn(y, torch.argmax(c_out, dim=1))\n",
    "        \n",
    "        \n",
    "\n",
    "        # 3. Zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Backprop\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Step\n",
    "        optimizer.step()\n",
    "    \n",
    "        if batch % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tBCE:{:.4f}\\tKLD:{:.4f}\\tC_loss:{:.4f}'.format(\n",
    "                epoch,\n",
    "                batch * len(X),\n",
    "                len(train_dataloader.dataset),\n",
    "                100. * batch / len(train_dataloader),\n",
    "                loss.item() / len(X), BCE.item() / len(X), KLD.item() / len(X), C_loss.item() / len(X)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}\\tClassifier Accuracy: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_dataloader.dataset), classif_accuracy/len(train_dataloader)))\n",
    "    return train_loss/len(train_dataloader.dataset), classif_accuracy/len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    #Sets the module in evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    classif_accuracy = 0\n",
    "    with torch.inference_mode():\n",
    "        for i, (X, y) in enumerate(test_dataloader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # 1. Forward pass\n",
    "            mu, logvar, recon_batch, c_out = model((X, y))\n",
    "            \n",
    "            flat_data = X.view(-1, flat_shape[0]).to(device)\n",
    "            y_onehot = F.one_hot(y, cond_shape).to(device)\n",
    "            inp = torch.cat((flat_data, y_onehot), 1)\n",
    "\n",
    "            # 2. Loss\n",
    "            tot_loss, C_loss, BCE, KLD = loss_fn(recon_batch, flat_data, mu, logvar, c_out, y_onehot)\n",
    "            test_loss += tot_loss.item()\n",
    "            classif_accuracy += accuracy_fn(y, torch.argmax(c_out, dim=1))\n",
    "\n",
    "            # 3. Save images\n",
    "            if epoch%5==0 and i == 0:\n",
    "                n = min(X.size(0), 8)\n",
    "                recon_image = recon_batch[:, 0:recon_batch.shape[1]]\n",
    "                print(recon_image.shape)\n",
    "                recon_image = recon_image.view(BATCH_SIZE, 1, 28,28)\n",
    "                print('---',recon_image.shape)\n",
    "                comparison = torch.cat([X[:n],\n",
    "                                      recon_image.view(BATCH_SIZE, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss, classif_accuracy/len(test_dataloader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.261414\tBCE:550.1899\tKLD:0.0702\tC_loss:0.0013\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 406.933014\tBCE:406.8797\tKLD:0.0523\tC_loss:0.0010\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 351.606262\tBCE:351.5446\tKLD:0.0609\tC_loss:0.0007\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 286.365601\tBCE:286.2919\tKLD:0.0732\tC_loss:0.0005\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 232.246979\tBCE:232.1556\tKLD:0.0910\tC_loss:0.0004\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 210.703827\tBCE:210.6088\tKLD:0.0947\tC_loss:0.0003\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 200.329895\tBCE:200.2605\tKLD:0.0692\tC_loss:0.0003\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 195.462509\tBCE:195.3416\tKLD:0.1206\tC_loss:0.0003\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 192.307419\tBCE:192.0788\tKLD:0.2284\tC_loss:0.0002\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 191.885056\tBCE:191.8043\tKLD:0.0805\tC_loss:0.0003\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 184.028336\tBCE:183.9592\tKLD:0.0689\tC_loss:0.0002\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 183.457642\tBCE:183.3035\tKLD:0.1539\tC_loss:0.0003\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 179.428207\tBCE:179.2841\tKLD:0.1439\tC_loss:0.0002\n",
      "Train Epoch: 1 [33280/60000 (56%)]\tLoss: 182.090042\tBCE:181.8938\tKLD:0.1960\tC_loss:0.0002\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 178.421951\tBCE:176.8305\tKLD:1.5912\tC_loss:0.0003\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 173.745209\tBCE:170.4093\tKLD:3.3356\tC_loss:0.0002\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 167.504410\tBCE:163.5847\tKLD:3.9196\tC_loss:0.0002\n",
      "Train Epoch: 1 [43520/60000 (73%)]\tLoss: 161.637787\tBCE:156.2178\tKLD:5.4198\tC_loss:0.0002\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 153.669907\tBCE:147.7527\tKLD:5.9170\tC_loss:0.0002\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 157.853470\tBCE:151.3983\tKLD:6.4550\tC_loss:0.0002\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 149.218353\tBCE:142.8637\tKLD:6.3544\tC_loss:0.0002\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 152.553482\tBCE:145.0402\tKLD:7.5131\tC_loss:0.0002\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 143.400864\tBCE:134.8285\tKLD:8.5722\tC_loss:0.0002\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 140.315979\tBCE:131.7299\tKLD:8.5859\tC_loss:0.0002\n",
      "====> Epoch: 1 Average loss: 206.8193\tClassifier Accuracy: 86.6403\n",
      "====> Test set loss: 142.3559\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 142.907761\tBCE:134.3887\tKLD:8.5189\tC_loss:0.0002\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 143.279358\tBCE:134.6701\tKLD:8.6090\tC_loss:0.0002\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 140.778854\tBCE:131.8727\tKLD:8.9061\tC_loss:0.0001\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 133.626953\tBCE:124.7402\tKLD:8.8866\tC_loss:0.0001\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 135.762177\tBCE:126.0767\tKLD:9.6853\tC_loss:0.0002\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 131.794617\tBCE:121.0217\tKLD:10.7727\tC_loss:0.0002\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 133.224716\tBCE:122.1859\tKLD:11.0386\tC_loss:0.0002\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 132.527481\tBCE:121.0371\tKLD:11.4902\tC_loss:0.0002\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 128.000427\tBCE:116.0971\tKLD:11.9031\tC_loss:0.0002\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 129.412964\tBCE:117.4490\tKLD:11.9638\tC_loss:0.0002\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 127.548233\tBCE:115.4900\tKLD:12.0580\tC_loss:0.0002\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 127.505157\tBCE:115.9295\tKLD:11.5755\tC_loss:0.0002\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 125.989204\tBCE:112.9734\tKLD:13.0156\tC_loss:0.0001\n",
      "Train Epoch: 2 [33280/60000 (56%)]\tLoss: 126.059891\tBCE:113.6609\tKLD:12.3988\tC_loss:0.0002\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 124.203629\tBCE:111.5685\tKLD:12.6350\tC_loss:0.0001\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 120.229065\tBCE:107.1249\tKLD:13.1040\tC_loss:0.0001\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 120.756516\tBCE:106.9703\tKLD:13.7861\tC_loss:0.0002\n",
      "Train Epoch: 2 [43520/60000 (73%)]\tLoss: 113.166245\tBCE:100.7055\tKLD:12.4606\tC_loss:0.0001\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 117.817848\tBCE:104.2306\tKLD:13.5871\tC_loss:0.0002\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 118.175209\tBCE:105.0199\tKLD:13.1552\tC_loss:0.0001\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 114.953903\tBCE:101.7244\tKLD:13.2293\tC_loss:0.0001\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 113.099831\tBCE:99.9016\tKLD:13.1981\tC_loss:0.0001\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 116.183243\tBCE:102.4724\tKLD:13.7107\tC_loss:0.0001\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 116.327057\tBCE:102.2145\tKLD:14.1124\tC_loss:0.0001\n",
      "====> Epoch: 2 Average loss: 126.1981\tClassifier Accuracy: 93.4412\n",
      "====> Test set loss: 114.1333\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 113.756805\tBCE:100.5123\tKLD:13.2444\tC_loss:0.0001\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 117.252655\tBCE:102.7275\tKLD:14.5250\tC_loss:0.0001\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 114.949928\tBCE:100.9614\tKLD:13.9884\tC_loss:0.0001\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 114.569000\tBCE:99.7607\tKLD:14.8082\tC_loss:0.0001\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 118.727539\tBCE:104.9980\tKLD:13.7293\tC_loss:0.0002\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 112.433304\tBCE:98.5218\tKLD:13.9113\tC_loss:0.0002\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 113.661270\tBCE:99.0655\tKLD:14.5956\tC_loss:0.0001\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 113.200737\tBCE:98.5133\tKLD:14.6874\tC_loss:0.0001\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 113.831772\tBCE:99.0984\tKLD:14.7332\tC_loss:0.0001\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 112.597687\tBCE:97.4420\tKLD:15.1555\tC_loss:0.0001\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 112.128853\tBCE:96.4131\tKLD:15.7156\tC_loss:0.0001\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 109.017410\tBCE:94.5845\tKLD:14.4328\tC_loss:0.0001\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 111.634415\tBCE:96.3322\tKLD:15.3021\tC_loss:0.0001\n",
      "Train Epoch: 3 [33280/60000 (56%)]\tLoss: 109.385971\tBCE:94.5001\tKLD:14.8858\tC_loss:0.0001\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 110.929955\tBCE:96.1645\tKLD:14.7653\tC_loss:0.0001\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 108.306000\tBCE:92.6735\tKLD:15.6324\tC_loss:0.0001\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 110.116211\tBCE:95.6617\tKLD:14.4544\tC_loss:0.0001\n",
      "Train Epoch: 3 [43520/60000 (73%)]\tLoss: 108.165428\tBCE:93.4377\tKLD:14.7276\tC_loss:0.0001\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 107.618210\tBCE:91.9926\tKLD:15.6255\tC_loss:0.0001\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 112.308167\tBCE:96.2601\tKLD:16.0479\tC_loss:0.0001\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 106.865829\tBCE:92.2130\tKLD:14.6528\tC_loss:0.0001\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 105.255798\tBCE:89.8981\tKLD:15.3576\tC_loss:0.0001\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 105.053787\tBCE:89.7691\tKLD:15.2846\tC_loss:0.0001\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 107.035553\tBCE:91.7309\tKLD:15.3045\tC_loss:0.0001\n",
      "====> Epoch: 3 Average loss: 110.8379\tClassifier Accuracy: 94.9870\n",
      "====> Test set loss: 108.6550\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 110.118118\tBCE:94.3891\tKLD:15.7289\tC_loss:0.0001\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 109.658096\tBCE:94.2107\tKLD:15.4472\tC_loss:0.0001\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 111.261833\tBCE:95.7460\tKLD:15.5157\tC_loss:0.0001\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 106.933670\tBCE:91.2568\tKLD:15.6768\tC_loss:0.0001\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 106.549149\tBCE:91.1405\tKLD:15.4086\tC_loss:0.0001\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 104.323128\tBCE:88.6856\tKLD:15.6374\tC_loss:0.0001\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 104.509590\tBCE:88.9069\tKLD:15.6026\tC_loss:0.0001\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 106.147369\tBCE:90.3495\tKLD:15.7978\tC_loss:0.0001\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 106.335236\tBCE:91.0025\tKLD:15.3327\tC_loss:0.0001\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 105.378006\tBCE:89.9411\tKLD:15.4369\tC_loss:0.0001\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 105.567711\tBCE:90.3132\tKLD:15.2544\tC_loss:0.0001\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 105.084290\tBCE:88.9044\tKLD:16.1798\tC_loss:0.0001\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 101.560921\tBCE:85.7439\tKLD:15.8170\tC_loss:0.0001\n",
      "Train Epoch: 4 [33280/60000 (56%)]\tLoss: 104.097092\tBCE:88.4328\tKLD:15.6642\tC_loss:0.0001\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 108.194359\tBCE:92.6095\tKLD:15.5848\tC_loss:0.0001\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 104.332565\tBCE:88.8643\tKLD:15.4682\tC_loss:0.0001\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 105.986084\tBCE:90.2672\tKLD:15.7188\tC_loss:0.0001\n",
      "Train Epoch: 4 [43520/60000 (73%)]\tLoss: 103.080826\tBCE:87.8099\tKLD:15.2709\tC_loss:0.0001\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 104.160805\tBCE:87.7330\tKLD:16.4277\tC_loss:0.0001\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 105.470154\tBCE:89.5080\tKLD:15.9621\tC_loss:0.0001\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 102.194496\tBCE:87.1687\tKLD:15.0258\tC_loss:0.0001\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 102.794624\tBCE:87.1080\tKLD:15.6866\tC_loss:0.0001\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 104.745377\tBCE:88.5959\tKLD:16.1493\tC_loss:0.0001\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 103.861504\tBCE:87.8683\tKLD:15.9931\tC_loss:0.0001\n",
      "====> Epoch: 4 Average loss: 105.2441\tClassifier Accuracy: 95.8634\n",
      "====> Test set loss: 103.0941\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 104.763885\tBCE:88.6153\tKLD:16.1485\tC_loss:0.0001\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 104.117325\tBCE:88.1002\tKLD:16.0170\tC_loss:0.0001\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 99.479759\tBCE:84.3868\tKLD:15.0929\tC_loss:0.0001\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 100.389389\tBCE:84.2298\tKLD:16.1596\tC_loss:0.0001\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 103.557198\tBCE:87.8180\tKLD:15.7391\tC_loss:0.0001\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 108.349548\tBCE:92.2900\tKLD:16.0595\tC_loss:0.0001\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 102.488449\tBCE:86.3038\tKLD:16.1846\tC_loss:0.0000\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 102.989120\tBCE:86.9904\tKLD:15.9986\tC_loss:0.0001\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 104.003418\tBCE:87.7603\tKLD:16.2430\tC_loss:0.0001\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 106.531952\tBCE:90.2867\tKLD:16.2451\tC_loss:0.0001\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 102.914070\tBCE:87.3060\tKLD:15.6079\tC_loss:0.0001\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 104.355835\tBCE:88.1382\tKLD:16.2175\tC_loss:0.0001\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 99.666763\tBCE:83.7694\tKLD:15.8973\tC_loss:0.0001\n",
      "Train Epoch: 5 [33280/60000 (56%)]\tLoss: 102.501289\tBCE:86.3883\tKLD:16.1129\tC_loss:0.0001\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 99.183533\tBCE:83.5798\tKLD:15.6037\tC_loss:0.0001\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 105.956604\tBCE:89.7075\tKLD:16.2490\tC_loss:0.0001\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 103.000015\tBCE:86.3581\tKLD:16.6419\tC_loss:0.0001\n",
      "Train Epoch: 5 [43520/60000 (73%)]\tLoss: 106.116035\tBCE:89.6936\tKLD:16.4224\tC_loss:0.0001\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 100.892563\tBCE:85.2553\tKLD:15.6372\tC_loss:0.0001\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 102.299904\tBCE:86.3545\tKLD:15.9453\tC_loss:0.0001\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 104.398979\tBCE:88.2585\tKLD:16.1404\tC_loss:0.0001\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 98.973312\tBCE:83.1726\tKLD:15.8006\tC_loss:0.0001\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 103.239700\tBCE:87.5964\tKLD:15.6432\tC_loss:0.0001\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 104.707108\tBCE:88.9385\tKLD:15.7686\tC_loss:0.0001\n",
      "====> Epoch: 5 Average loss: 102.5467\tClassifier Accuracy: 96.5278\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 101.0799\n",
      "Random number: 4\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 100.039871\tBCE:84.1259\tKLD:15.9139\tC_loss:0.0001\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 101.550194\tBCE:85.5374\tKLD:16.0128\tC_loss:0.0001\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 99.114029\tBCE:82.8367\tKLD:16.2772\tC_loss:0.0001\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 102.127975\tBCE:86.9063\tKLD:15.2216\tC_loss:0.0001\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 102.546913\tBCE:86.0595\tKLD:16.4874\tC_loss:0.0000\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 104.317703\tBCE:88.1731\tKLD:16.1446\tC_loss:0.0001\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 99.865044\tBCE:84.0944\tKLD:15.7706\tC_loss:0.0001\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 99.555153\tBCE:83.5246\tKLD:16.0305\tC_loss:0.0001\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 99.979767\tBCE:83.4921\tKLD:16.4876\tC_loss:0.0001\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 97.838600\tBCE:81.4178\tKLD:16.4207\tC_loss:0.0001\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 101.401138\tBCE:85.5857\tKLD:15.8154\tC_loss:0.0001\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 101.963409\tBCE:85.3520\tKLD:16.6113\tC_loss:0.0001\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 103.595177\tBCE:86.7167\tKLD:16.8784\tC_loss:0.0001\n",
      "Train Epoch: 6 [33280/60000 (56%)]\tLoss: 101.779442\tBCE:85.3913\tKLD:16.3880\tC_loss:0.0001\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 99.879921\tBCE:83.6601\tKLD:16.2197\tC_loss:0.0001\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 101.559311\tBCE:85.4484\tKLD:16.1109\tC_loss:0.0001\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 102.117149\tBCE:85.8315\tKLD:16.2855\tC_loss:0.0001\n",
      "Train Epoch: 6 [43520/60000 (73%)]\tLoss: 101.197243\tBCE:85.2794\tKLD:15.9177\tC_loss:0.0001\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 103.170074\tBCE:86.8473\tKLD:16.3227\tC_loss:0.0001\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 97.002762\tBCE:80.6754\tKLD:16.3274\tC_loss:0.0000\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 101.890778\tBCE:85.8303\tKLD:16.0604\tC_loss:0.0001\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 99.072021\tBCE:83.0123\tKLD:16.0596\tC_loss:0.0001\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 99.564865\tBCE:83.0418\tKLD:16.5230\tC_loss:0.0001\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 104.092804\tBCE:87.4196\tKLD:16.6731\tC_loss:0.0001\n",
      "====> Epoch: 6 Average loss: 100.7033\tClassifier Accuracy: 97.0837\n",
      "====> Test set loss: 99.1071\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 97.477325\tBCE:80.7591\tKLD:16.7181\tC_loss:0.0001\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 98.392990\tBCE:82.0875\tKLD:16.3054\tC_loss:0.0001\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 98.511581\tBCE:81.8564\tKLD:16.6551\tC_loss:0.0000\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 97.203659\tBCE:80.8647\tKLD:16.3389\tC_loss:0.0001\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 97.474792\tBCE:80.7167\tKLD:16.7580\tC_loss:0.0001\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 98.779709\tBCE:82.2221\tKLD:16.5576\tC_loss:0.0000\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 100.528633\tBCE:84.1448\tKLD:16.3838\tC_loss:0.0000\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 100.836441\tBCE:84.3978\tKLD:16.4386\tC_loss:0.0001\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 99.244949\tBCE:82.5012\tKLD:16.7437\tC_loss:0.0001\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 101.263840\tBCE:84.6220\tKLD:16.6418\tC_loss:0.0001\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 101.194328\tBCE:84.5856\tKLD:16.6087\tC_loss:0.0000\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 99.675827\tBCE:83.4837\tKLD:16.1921\tC_loss:0.0001\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 101.789398\tBCE:84.8694\tKLD:16.9199\tC_loss:0.0001\n",
      "Train Epoch: 7 [33280/60000 (56%)]\tLoss: 100.131149\tBCE:83.0780\tKLD:17.0531\tC_loss:0.0001\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 100.199944\tBCE:84.0522\tKLD:16.1476\tC_loss:0.0001\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 100.525002\tBCE:83.9852\tKLD:16.5397\tC_loss:0.0001\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 99.090607\tBCE:81.6944\tKLD:17.3962\tC_loss:0.0001\n",
      "Train Epoch: 7 [43520/60000 (73%)]\tLoss: 102.468994\tBCE:86.0461\tKLD:16.4228\tC_loss:0.0001\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 99.942795\tBCE:83.1651\tKLD:16.7777\tC_loss:0.0001\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 103.580727\tBCE:86.9334\tKLD:16.6472\tC_loss:0.0001\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 98.880295\tBCE:82.7868\tKLD:16.0934\tC_loss:0.0001\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 96.607483\tBCE:79.8380\tKLD:16.7694\tC_loss:0.0000\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 99.715057\tBCE:83.2742\tKLD:16.4408\tC_loss:0.0001\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 100.436798\tBCE:83.6620\tKLD:16.7748\tC_loss:0.0000\n",
      "====> Epoch: 7 Average loss: 99.2043\tClassifier Accuracy: 97.5060\n",
      "====> Test set loss: 98.9811\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 98.394043\tBCE:81.9089\tKLD:16.4851\tC_loss:0.0000\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 96.015953\tBCE:79.3305\tKLD:16.6854\tC_loss:0.0000\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 97.719070\tBCE:80.4737\tKLD:17.2453\tC_loss:0.0001\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 100.450394\tBCE:83.6990\tKLD:16.7514\tC_loss:0.0000\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 100.140869\tBCE:83.6551\tKLD:16.4857\tC_loss:0.0001\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 98.827309\tBCE:81.6844\tKLD:17.1428\tC_loss:0.0001\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 98.976715\tBCE:82.3007\tKLD:16.6760\tC_loss:0.0001\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 99.338715\tBCE:82.1236\tKLD:17.2150\tC_loss:0.0001\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 100.504898\tBCE:83.4985\tKLD:17.0064\tC_loss:0.0001\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 96.305763\tBCE:79.7517\tKLD:16.5540\tC_loss:0.0001\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 99.375580\tBCE:81.6333\tKLD:17.7422\tC_loss:0.0000\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 102.026062\tBCE:84.9156\tKLD:17.1104\tC_loss:0.0001\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 97.850945\tBCE:81.4021\tKLD:16.4488\tC_loss:0.0000\n",
      "Train Epoch: 8 [33280/60000 (56%)]\tLoss: 99.819588\tBCE:83.4378\tKLD:16.3817\tC_loss:0.0001\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 99.048561\tBCE:81.8485\tKLD:17.2000\tC_loss:0.0001\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 98.229012\tBCE:80.8001\tKLD:17.4288\tC_loss:0.0000\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 97.739906\tBCE:80.9071\tKLD:16.8328\tC_loss:0.0000\n",
      "Train Epoch: 8 [43520/60000 (73%)]\tLoss: 96.160759\tBCE:79.4144\tKLD:16.7463\tC_loss:0.0001\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 101.315720\tBCE:84.7793\tKLD:16.5364\tC_loss:0.0000\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 100.094131\tBCE:82.8133\tKLD:17.2807\tC_loss:0.0000\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 98.247467\tBCE:81.7441\tKLD:16.5033\tC_loss:0.0000\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 97.121651\tBCE:80.4006\tKLD:16.7210\tC_loss:0.0000\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 96.850052\tBCE:79.8703\tKLD:16.9797\tC_loss:0.0001\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 97.105202\tBCE:80.3884\tKLD:16.7168\tC_loss:0.0000\n",
      "====> Epoch: 8 Average loss: 98.1715\tClassifier Accuracy: 97.7865\n",
      "====> Test set loss: 97.3809\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 98.747101\tBCE:81.6339\tKLD:17.1131\tC_loss:0.0001\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 98.173309\tBCE:81.3762\tKLD:16.7971\tC_loss:0.0000\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 97.593071\tBCE:80.2143\tKLD:17.3787\tC_loss:0.0000\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 98.689423\tBCE:81.6127\tKLD:17.0766\tC_loss:0.0000\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 95.810577\tBCE:79.2821\tKLD:16.5284\tC_loss:0.0000\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 96.538406\tBCE:79.9343\tKLD:16.6041\tC_loss:0.0000\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 97.142754\tBCE:80.0836\tKLD:17.0591\tC_loss:0.0000\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 97.750793\tBCE:80.8918\tKLD:16.8590\tC_loss:0.0001\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 98.106720\tBCE:80.4358\tKLD:17.6708\tC_loss:0.0000\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 97.827240\tBCE:80.9938\tKLD:16.8334\tC_loss:0.0000\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 94.785103\tBCE:77.8700\tKLD:16.9150\tC_loss:0.0001\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 99.999268\tBCE:82.8631\tKLD:17.1361\tC_loss:0.0001\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 97.540184\tBCE:80.8064\tKLD:16.7338\tC_loss:0.0000\n",
      "Train Epoch: 9 [33280/60000 (56%)]\tLoss: 94.438919\tBCE:77.5746\tKLD:16.8643\tC_loss:0.0000\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 100.523895\tBCE:83.0395\tKLD:17.4844\tC_loss:0.0000\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 98.463989\tBCE:80.6596\tKLD:17.8043\tC_loss:0.0001\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 94.335915\tBCE:78.0147\tKLD:16.3211\tC_loss:0.0000\n",
      "Train Epoch: 9 [43520/60000 (73%)]\tLoss: 96.341728\tBCE:79.3392\tKLD:17.0024\tC_loss:0.0001\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 96.080719\tBCE:79.5874\tKLD:16.4933\tC_loss:0.0000\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 97.186424\tBCE:81.1977\tKLD:15.9887\tC_loss:0.0000\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 95.980545\tBCE:79.1943\tKLD:16.7862\tC_loss:0.0000\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 94.040344\tBCE:77.2235\tKLD:16.8168\tC_loss:0.0000\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 98.708809\tBCE:81.8280\tKLD:16.8808\tC_loss:0.0000\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 97.664246\tBCE:81.3328\tKLD:16.3314\tC_loss:0.0000\n",
      "====> Epoch: 9 Average loss: 97.2830\tClassifier Accuracy: 98.0753\n",
      "====> Test set loss: 96.4477\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 97.666779\tBCE:80.5505\tKLD:17.1162\tC_loss:0.0001\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 95.562408\tBCE:78.7278\tKLD:16.8346\tC_loss:0.0000\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 98.036095\tBCE:80.6703\tKLD:17.3657\tC_loss:0.0000\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 99.147842\tBCE:82.1809\tKLD:16.9669\tC_loss:0.0001\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 95.767662\tBCE:79.3113\tKLD:16.4563\tC_loss:0.0000\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 95.690880\tBCE:79.1700\tKLD:16.5209\tC_loss:0.0000\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 95.511360\tBCE:78.2174\tKLD:17.2939\tC_loss:0.0000\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 96.296097\tBCE:79.1425\tKLD:17.1536\tC_loss:0.0000\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 95.912537\tBCE:78.5240\tKLD:17.3885\tC_loss:0.0000\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 94.508438\tBCE:78.0361\tKLD:16.4723\tC_loss:0.0000\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 95.598671\tBCE:78.7534\tKLD:16.8452\tC_loss:0.0000\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 97.757332\tBCE:80.3921\tKLD:17.3652\tC_loss:0.0000\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 95.126350\tBCE:78.3502\tKLD:16.7761\tC_loss:0.0000\n",
      "Train Epoch: 10 [33280/60000 (56%)]\tLoss: 96.043259\tBCE:79.0946\tKLD:16.9486\tC_loss:0.0001\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 96.315933\tBCE:79.2896\tKLD:17.0263\tC_loss:0.0000\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 91.618866\tBCE:74.9817\tKLD:16.6371\tC_loss:0.0000\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 98.968948\tBCE:81.8592\tKLD:17.1097\tC_loss:0.0001\n",
      "Train Epoch: 10 [43520/60000 (73%)]\tLoss: 97.383850\tBCE:80.2742\tKLD:17.1097\tC_loss:0.0000\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 96.610443\tBCE:79.7397\tKLD:16.8707\tC_loss:0.0001\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 100.106224\tBCE:82.6375\tKLD:17.4687\tC_loss:0.0000\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 97.305794\tBCE:79.9545\tKLD:17.3512\tC_loss:0.0001\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 97.480698\tBCE:80.2114\tKLD:17.2692\tC_loss:0.0000\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 95.745705\tBCE:78.8076\tKLD:16.9381\tC_loss:0.0000\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 96.309982\tBCE:78.9459\tKLD:17.3641\tC_loss:0.0000\n",
      "====> Epoch: 10 Average loss: 96.4130\tClassifier Accuracy: 98.3156\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 96.1949\n",
      "Random number: 4\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 97.409119\tBCE:80.3901\tKLD:17.0189\tC_loss:0.0000\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 99.167908\tBCE:81.2307\tKLD:17.9372\tC_loss:0.0000\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 94.405212\tBCE:77.6928\tKLD:16.7123\tC_loss:0.0000\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 95.594055\tBCE:78.6382\tKLD:16.9558\tC_loss:0.0000\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 95.109390\tBCE:77.8540\tKLD:17.2554\tC_loss:0.0000\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 93.686424\tBCE:76.7912\tKLD:16.8952\tC_loss:0.0000\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 96.293869\tBCE:78.5224\tKLD:17.7715\tC_loss:0.0000\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 95.114929\tBCE:78.1179\tKLD:16.9970\tC_loss:0.0000\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 96.293777\tBCE:78.8540\tKLD:17.4397\tC_loss:0.0001\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 95.586914\tBCE:78.2982\tKLD:17.2886\tC_loss:0.0000\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 97.414345\tBCE:79.9940\tKLD:17.4203\tC_loss:0.0001\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 98.966248\tBCE:81.2947\tKLD:17.6715\tC_loss:0.0001\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 94.427422\tBCE:77.2314\tKLD:17.1960\tC_loss:0.0000\n",
      "Train Epoch: 11 [33280/60000 (56%)]\tLoss: 94.346657\tBCE:77.2512\tKLD:17.0955\tC_loss:0.0000\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 93.756927\tBCE:76.6226\tKLD:17.1343\tC_loss:0.0000\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 95.813171\tBCE:78.5625\tKLD:17.2507\tC_loss:0.0000\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 100.523239\tBCE:82.8155\tKLD:17.7077\tC_loss:0.0000\n",
      "Train Epoch: 11 [43520/60000 (73%)]\tLoss: 97.697784\tBCE:80.4035\tKLD:17.2943\tC_loss:0.0000\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 97.174164\tBCE:80.0649\tKLD:17.1092\tC_loss:0.0001\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 93.628494\tBCE:76.9518\tKLD:16.6766\tC_loss:0.0000\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 97.136269\tBCE:79.6198\tKLD:17.5165\tC_loss:0.0000\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 98.401031\tBCE:81.0044\tKLD:17.3966\tC_loss:0.0001\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 96.928070\tBCE:79.8880\tKLD:17.0401\tC_loss:0.0000\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 95.188034\tBCE:78.5984\tKLD:16.5896\tC_loss:0.0000\n",
      "====> Epoch: 11 Average loss: 95.8231\tClassifier Accuracy: 98.4909\n",
      "====> Test set loss: 96.0697\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 96.885971\tBCE:79.5515\tKLD:17.3344\tC_loss:0.0000\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 93.657219\tBCE:76.8567\tKLD:16.8005\tC_loss:0.0000\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 94.748337\tBCE:77.7126\tKLD:17.0357\tC_loss:0.0000\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 94.134407\tBCE:76.7212\tKLD:17.4131\tC_loss:0.0000\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 93.851555\tBCE:76.8728\tKLD:16.9787\tC_loss:0.0000\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 98.357178\tBCE:80.9625\tKLD:17.3946\tC_loss:0.0000\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 92.748520\tBCE:76.2500\tKLD:16.4985\tC_loss:0.0000\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 92.553612\tBCE:75.9153\tKLD:16.6382\tC_loss:0.0000\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 94.810867\tBCE:77.6227\tKLD:17.1881\tC_loss:0.0000\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 95.669403\tBCE:78.5400\tKLD:17.1294\tC_loss:0.0000\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 93.834244\tBCE:77.2970\tKLD:16.5372\tC_loss:0.0000\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 94.032379\tBCE:76.6586\tKLD:17.3738\tC_loss:0.0000\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 98.372520\tBCE:81.1433\tKLD:17.2292\tC_loss:0.0000\n",
      "Train Epoch: 12 [33280/60000 (56%)]\tLoss: 96.349808\tBCE:79.1931\tKLD:17.1567\tC_loss:0.0000\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 95.086273\tBCE:78.3376\tKLD:16.7486\tC_loss:0.0000\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 94.403793\tBCE:77.1183\tKLD:17.2855\tC_loss:0.0000\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 95.733864\tBCE:78.7289\tKLD:17.0049\tC_loss:0.0000\n",
      "Train Epoch: 12 [43520/60000 (73%)]\tLoss: 94.725052\tBCE:77.0645\tKLD:17.6605\tC_loss:0.0000\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 94.269447\tBCE:77.4216\tKLD:16.8478\tC_loss:0.0000\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 93.313660\tBCE:76.4855\tKLD:16.8281\tC_loss:0.0000\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 93.656883\tBCE:77.2696\tKLD:16.3873\tC_loss:0.0000\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 92.815689\tBCE:76.1789\tKLD:16.6368\tC_loss:0.0000\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 94.639183\tBCE:77.7296\tKLD:16.9095\tC_loss:0.0000\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 94.358498\tBCE:76.6534\tKLD:17.7051\tC_loss:0.0000\n",
      "====> Epoch: 12 Average loss: 95.2528\tClassifier Accuracy: 98.7046\n",
      "====> Test set loss: 94.9726\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 96.694046\tBCE:79.3220\tKLD:17.3720\tC_loss:0.0000\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 95.098160\tBCE:77.5764\tKLD:17.5218\tC_loss:0.0000\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 93.469009\tBCE:76.7645\tKLD:16.7044\tC_loss:0.0000\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 94.967621\tBCE:77.9336\tKLD:17.0340\tC_loss:0.0000\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 93.439430\tBCE:76.0130\tKLD:17.4264\tC_loss:0.0000\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 95.170052\tBCE:77.4473\tKLD:17.7227\tC_loss:0.0000\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 94.913132\tBCE:77.9724\tKLD:16.9407\tC_loss:0.0000\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 96.098030\tBCE:78.6995\tKLD:17.3985\tC_loss:0.0000\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 95.065971\tBCE:77.7520\tKLD:17.3140\tC_loss:0.0000\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 95.875977\tBCE:78.7293\tKLD:17.1467\tC_loss:0.0000\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 93.953918\tBCE:77.0279\tKLD:16.9260\tC_loss:0.0000\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 99.060654\tBCE:81.8377\tKLD:17.2229\tC_loss:0.0000\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 93.960037\tBCE:76.7545\tKLD:17.2055\tC_loss:0.0000\n",
      "Train Epoch: 13 [33280/60000 (56%)]\tLoss: 98.372795\tBCE:80.7745\tKLD:17.5982\tC_loss:0.0000\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 96.022324\tBCE:78.1622\tKLD:17.8600\tC_loss:0.0000\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 96.817261\tBCE:79.9276\tKLD:16.8896\tC_loss:0.0000\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 95.711731\tBCE:78.7730\tKLD:16.9387\tC_loss:0.0000\n",
      "Train Epoch: 13 [43520/60000 (73%)]\tLoss: 96.019012\tBCE:78.2749\tKLD:17.7441\tC_loss:0.0000\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 95.025024\tBCE:77.7258\tKLD:17.2992\tC_loss:0.0000\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 96.693825\tBCE:78.9920\tKLD:17.7018\tC_loss:0.0000\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 95.728912\tBCE:78.2791\tKLD:17.4497\tC_loss:0.0000\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 92.186447\tBCE:75.7491\tKLD:16.4374\tC_loss:0.0000\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 96.776215\tBCE:78.8986\tKLD:17.8776\tC_loss:0.0000\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 93.512085\tBCE:76.5175\tKLD:16.9945\tC_loss:0.0000\n",
      "====> Epoch: 13 Average loss: 94.9496\tClassifier Accuracy: 98.8932\n",
      "====> Test set loss: 94.2988\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 91.062897\tBCE:73.5432\tKLD:17.5196\tC_loss:0.0000\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 94.362244\tBCE:77.5028\tKLD:16.8595\tC_loss:0.0000\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 94.509247\tBCE:77.6363\tKLD:16.8729\tC_loss:0.0001\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 94.422882\tBCE:76.8802\tKLD:17.5426\tC_loss:0.0000\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 94.289322\tBCE:76.9678\tKLD:17.3215\tC_loss:0.0000\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 97.286812\tBCE:79.7072\tKLD:17.5796\tC_loss:0.0000\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 93.207779\tBCE:75.9883\tKLD:17.2195\tC_loss:0.0000\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 92.562584\tBCE:75.3812\tKLD:17.1813\tC_loss:0.0000\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 93.685837\tBCE:76.9002\tKLD:16.7856\tC_loss:0.0000\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 93.979172\tBCE:76.6729\tKLD:17.3063\tC_loss:0.0000\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 92.831833\tBCE:75.0677\tKLD:17.7641\tC_loss:0.0000\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 95.805283\tBCE:78.6293\tKLD:17.1759\tC_loss:0.0000\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 94.543655\tBCE:77.1364\tKLD:17.4072\tC_loss:0.0000\n",
      "Train Epoch: 14 [33280/60000 (56%)]\tLoss: 94.685242\tBCE:77.6311\tKLD:17.0541\tC_loss:0.0000\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 96.473640\tBCE:79.1036\tKLD:17.3701\tC_loss:0.0000\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 94.985832\tBCE:77.6976\tKLD:17.2882\tC_loss:0.0000\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 95.820808\tBCE:78.2324\tKLD:17.5884\tC_loss:0.0000\n",
      "Train Epoch: 14 [43520/60000 (73%)]\tLoss: 93.609146\tBCE:76.0759\tKLD:17.5332\tC_loss:0.0000\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 96.646927\tBCE:79.0470\tKLD:17.5999\tC_loss:0.0000\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 95.099655\tBCE:78.0011\tKLD:17.0985\tC_loss:0.0000\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 96.906700\tBCE:79.1081\tKLD:17.7986\tC_loss:0.0000\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 97.043106\tBCE:79.3580\tKLD:17.6851\tC_loss:0.0000\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 93.602654\tBCE:76.4584\tKLD:17.1442\tC_loss:0.0000\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 96.421471\tBCE:78.8262\tKLD:17.5953\tC_loss:0.0000\n",
      "====> Epoch: 14 Average loss: 94.4684\tClassifier Accuracy: 99.0268\n",
      "====> Test set loss: 94.0817\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 94.269577\tBCE:77.0752\tKLD:17.1943\tC_loss:0.0000\n",
      "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 95.127151\tBCE:78.1610\tKLD:16.9661\tC_loss:0.0000\n",
      "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 92.407722\tBCE:75.5252\tKLD:16.8825\tC_loss:0.0000\n",
      "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 94.633125\tBCE:77.1903\tKLD:17.4428\tC_loss:0.0000\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 94.542648\tBCE:77.1358\tKLD:17.4069\tC_loss:0.0000\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 94.086990\tBCE:77.0310\tKLD:17.0559\tC_loss:0.0000\n",
      "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 95.882950\tBCE:77.9658\tKLD:17.9171\tC_loss:0.0000\n",
      "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 90.822380\tBCE:74.0783\tKLD:16.7440\tC_loss:0.0000\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 97.402824\tBCE:80.1400\tKLD:17.2628\tC_loss:0.0000\n",
      "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 94.828995\tBCE:77.6503\tKLD:17.1787\tC_loss:0.0000\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 95.722092\tBCE:78.1404\tKLD:17.5816\tC_loss:0.0000\n",
      "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 94.850319\tBCE:77.7972\tKLD:17.0531\tC_loss:0.0000\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 93.031960\tBCE:75.8227\tKLD:17.2093\tC_loss:0.0000\n",
      "Train Epoch: 15 [33280/60000 (56%)]\tLoss: 96.319580\tBCE:78.8379\tKLD:17.4817\tC_loss:0.0000\n",
      "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 93.609161\tBCE:76.3899\tKLD:17.2192\tC_loss:0.0000\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 94.994095\tBCE:78.0017\tKLD:16.9924\tC_loss:0.0000\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 93.604477\tBCE:77.0368\tKLD:16.5676\tC_loss:0.0000\n",
      "Train Epoch: 15 [43520/60000 (73%)]\tLoss: 95.542648\tBCE:78.2159\tKLD:17.3267\tC_loss:0.0000\n",
      "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 93.847198\tBCE:76.2202\tKLD:17.6269\tC_loss:0.0000\n",
      "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 95.342461\tBCE:78.0130\tKLD:17.3295\tC_loss:0.0000\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 93.366333\tBCE:76.1190\tKLD:17.2473\tC_loss:0.0000\n",
      "Train Epoch: 15 [53760/60000 (90%)]\tLoss: 95.104874\tBCE:77.5799\tKLD:17.5249\tC_loss:0.0000\n",
      "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 93.014320\tBCE:75.8713\tKLD:17.1430\tC_loss:0.0000\n",
      "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 96.564407\tBCE:79.4149\tKLD:17.1495\tC_loss:0.0000\n",
      "====> Epoch: 15 Average loss: 94.1335\tClassifier Accuracy: 99.1169\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 94.4893\n",
      "Random number: 2\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 93.946243\tBCE:76.5139\tKLD:17.4324\tC_loss:0.0000\n",
      "Train Epoch: 16 [2560/60000 (4%)]\tLoss: 95.366791\tBCE:77.8618\tKLD:17.5050\tC_loss:0.0000\n",
      "Train Epoch: 16 [5120/60000 (9%)]\tLoss: 93.698685\tBCE:76.5164\tKLD:17.1823\tC_loss:0.0000\n",
      "Train Epoch: 16 [7680/60000 (13%)]\tLoss: 92.287399\tBCE:74.7527\tKLD:17.5346\tC_loss:0.0000\n",
      "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 93.116928\tBCE:75.7792\tKLD:17.3377\tC_loss:0.0000\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 96.526215\tBCE:78.9203\tKLD:17.6059\tC_loss:0.0000\n",
      "Train Epoch: 16 [15360/60000 (26%)]\tLoss: 95.335670\tBCE:78.4668\tKLD:16.8688\tC_loss:0.0000\n",
      "Train Epoch: 16 [17920/60000 (30%)]\tLoss: 95.166313\tBCE:77.8453\tKLD:17.3210\tC_loss:0.0000\n",
      "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 92.697266\tBCE:75.4347\tKLD:17.2625\tC_loss:0.0000\n",
      "Train Epoch: 16 [23040/60000 (38%)]\tLoss: 91.402290\tBCE:74.4213\tKLD:16.9810\tC_loss:0.0000\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 96.496231\tBCE:78.8461\tKLD:17.6501\tC_loss:0.0000\n",
      "Train Epoch: 16 [28160/60000 (47%)]\tLoss: 96.538879\tBCE:78.9921\tKLD:17.5467\tC_loss:0.0000\n",
      "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 96.130966\tBCE:78.7033\tKLD:17.4276\tC_loss:0.0000\n",
      "Train Epoch: 16 [33280/60000 (56%)]\tLoss: 94.535027\tBCE:77.0164\tKLD:17.5186\tC_loss:0.0000\n",
      "Train Epoch: 16 [35840/60000 (60%)]\tLoss: 94.268738\tBCE:77.3051\tKLD:16.9636\tC_loss:0.0000\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 96.041977\tBCE:78.6469\tKLD:17.3950\tC_loss:0.0000\n",
      "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 93.531944\tBCE:76.3070\tKLD:17.2249\tC_loss:0.0000\n",
      "Train Epoch: 16 [43520/60000 (73%)]\tLoss: 93.346420\tBCE:76.2822\tKLD:17.0643\tC_loss:0.0000\n",
      "Train Epoch: 16 [46080/60000 (77%)]\tLoss: 95.124962\tBCE:77.4444\tKLD:17.6806\tC_loss:0.0000\n",
      "Train Epoch: 16 [48640/60000 (81%)]\tLoss: 97.079132\tBCE:79.7106\tKLD:17.3685\tC_loss:0.0000\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 95.509239\tBCE:77.9933\tKLD:17.5159\tC_loss:0.0000\n",
      "Train Epoch: 16 [53760/60000 (90%)]\tLoss: 95.165314\tBCE:77.8998\tKLD:17.2655\tC_loss:0.0000\n",
      "Train Epoch: 16 [56320/60000 (94%)]\tLoss: 93.555527\tBCE:76.8446\tKLD:16.7109\tC_loss:0.0000\n",
      "Train Epoch: 16 [58880/60000 (98%)]\tLoss: 93.691833\tBCE:76.1347\tKLD:17.5571\tC_loss:0.0000\n",
      "====> Epoch: 16 Average loss: 93.8330\tClassifier Accuracy: 99.2455\n",
      "====> Test set loss: 93.5518\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 95.435669\tBCE:78.2225\tKLD:17.2131\tC_loss:0.0000\n",
      "Train Epoch: 17 [2560/60000 (4%)]\tLoss: 93.804825\tBCE:77.0762\tKLD:16.7286\tC_loss:0.0000\n",
      "Train Epoch: 17 [5120/60000 (9%)]\tLoss: 93.836845\tBCE:75.9321\tKLD:17.9048\tC_loss:0.0000\n",
      "Train Epoch: 17 [7680/60000 (13%)]\tLoss: 95.364258\tBCE:78.1519\tKLD:17.2123\tC_loss:0.0000\n",
      "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 95.131485\tBCE:77.3221\tKLD:17.8093\tC_loss:0.0000\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 94.819237\tBCE:78.0206\tKLD:16.7987\tC_loss:0.0000\n",
      "Train Epoch: 17 [15360/60000 (26%)]\tLoss: 95.617134\tBCE:77.9516\tKLD:17.6655\tC_loss:0.0000\n",
      "Train Epoch: 17 [17920/60000 (30%)]\tLoss: 93.341568\tBCE:76.0882\tKLD:17.2534\tC_loss:0.0000\n",
      "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 93.624252\tBCE:76.0020\tKLD:17.6222\tC_loss:0.0000\n",
      "Train Epoch: 17 [23040/60000 (38%)]\tLoss: 91.762360\tBCE:74.7538\tKLD:17.0085\tC_loss:0.0000\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 90.148834\tBCE:73.3590\tKLD:16.7898\tC_loss:0.0000\n",
      "Train Epoch: 17 [28160/60000 (47%)]\tLoss: 94.377701\tBCE:76.9938\tKLD:17.3839\tC_loss:0.0000\n",
      "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 93.449219\tBCE:76.3357\tKLD:17.1135\tC_loss:0.0000\n",
      "Train Epoch: 17 [33280/60000 (56%)]\tLoss: 94.448486\tBCE:76.9221\tKLD:17.5263\tC_loss:0.0000\n",
      "Train Epoch: 17 [35840/60000 (60%)]\tLoss: 92.811241\tBCE:75.5097\tKLD:17.3015\tC_loss:0.0000\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 90.786110\tBCE:73.8200\tKLD:16.9660\tC_loss:0.0000\n",
      "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 95.629135\tBCE:78.4886\tKLD:17.1405\tC_loss:0.0000\n",
      "Train Epoch: 17 [43520/60000 (73%)]\tLoss: 93.318855\tBCE:75.4987\tKLD:17.8201\tC_loss:0.0000\n",
      "Train Epoch: 17 [46080/60000 (77%)]\tLoss: 97.959259\tBCE:80.0696\tKLD:17.8896\tC_loss:0.0000\n",
      "Train Epoch: 17 [48640/60000 (81%)]\tLoss: 92.675606\tBCE:74.8893\tKLD:17.7863\tC_loss:0.0000\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 94.275543\tBCE:77.1432\tKLD:17.1323\tC_loss:0.0000\n",
      "Train Epoch: 17 [53760/60000 (90%)]\tLoss: 91.709595\tBCE:74.6587\tKLD:17.0509\tC_loss:0.0000\n",
      "Train Epoch: 17 [56320/60000 (94%)]\tLoss: 91.209732\tBCE:73.7533\tKLD:17.4564\tC_loss:0.0000\n",
      "Train Epoch: 17 [58880/60000 (98%)]\tLoss: 93.388275\tBCE:75.7366\tKLD:17.6517\tC_loss:0.0000\n",
      "====> Epoch: 17 Average loss: 93.3996\tClassifier Accuracy: 99.3289\n",
      "====> Test set loss: 92.9694\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 92.352188\tBCE:75.4095\tKLD:16.9427\tC_loss:0.0000\n",
      "Train Epoch: 18 [2560/60000 (4%)]\tLoss: 91.687988\tBCE:74.9347\tKLD:16.7533\tC_loss:0.0000\n",
      "Train Epoch: 18 [5120/60000 (9%)]\tLoss: 92.561157\tBCE:75.0772\tKLD:17.4840\tC_loss:0.0000\n",
      "Train Epoch: 18 [7680/60000 (13%)]\tLoss: 91.092667\tBCE:73.9273\tKLD:17.1653\tC_loss:0.0000\n",
      "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 94.740311\tBCE:77.4697\tKLD:17.2706\tC_loss:0.0000\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 92.070984\tBCE:75.2681\tKLD:16.8029\tC_loss:0.0000\n",
      "Train Epoch: 18 [15360/60000 (26%)]\tLoss: 92.390564\tBCE:75.1613\tKLD:17.2293\tC_loss:0.0000\n",
      "Train Epoch: 18 [17920/60000 (30%)]\tLoss: 93.969101\tBCE:76.2203\tKLD:17.7488\tC_loss:0.0000\n",
      "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 94.365974\tBCE:77.2589\tKLD:17.1071\tC_loss:0.0000\n",
      "Train Epoch: 18 [23040/60000 (38%)]\tLoss: 92.060944\tBCE:75.6271\tKLD:16.4338\tC_loss:0.0000\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 93.933128\tBCE:76.3365\tKLD:17.5966\tC_loss:0.0000\n",
      "Train Epoch: 18 [28160/60000 (47%)]\tLoss: 93.774948\tBCE:76.1596\tKLD:17.6153\tC_loss:0.0000\n",
      "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 94.411064\tBCE:77.0036\tKLD:17.4074\tC_loss:0.0000\n",
      "Train Epoch: 18 [33280/60000 (56%)]\tLoss: 93.928772\tBCE:76.5873\tKLD:17.3415\tC_loss:0.0000\n",
      "Train Epoch: 18 [35840/60000 (60%)]\tLoss: 93.076828\tBCE:75.6777\tKLD:17.3992\tC_loss:0.0000\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 93.431778\tBCE:76.4252\tKLD:17.0065\tC_loss:0.0000\n",
      "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 92.979179\tBCE:75.7964\tKLD:17.1828\tC_loss:0.0000\n",
      "Train Epoch: 18 [43520/60000 (73%)]\tLoss: 94.335144\tBCE:76.8517\tKLD:17.4834\tC_loss:0.0000\n",
      "Train Epoch: 18 [46080/60000 (77%)]\tLoss: 95.006462\tBCE:77.3970\tKLD:17.6095\tC_loss:0.0000\n",
      "Train Epoch: 18 [48640/60000 (81%)]\tLoss: 94.491333\tBCE:76.6806\tKLD:17.8107\tC_loss:0.0000\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 89.709396\tBCE:72.9085\tKLD:16.8009\tC_loss:0.0000\n",
      "Train Epoch: 18 [53760/60000 (90%)]\tLoss: 94.846397\tBCE:77.4059\tKLD:17.4405\tC_loss:0.0000\n",
      "Train Epoch: 18 [56320/60000 (94%)]\tLoss: 93.301315\tBCE:75.8310\tKLD:17.4703\tC_loss:0.0000\n",
      "Train Epoch: 18 [58880/60000 (98%)]\tLoss: 97.001541\tBCE:79.3916\tKLD:17.6100\tC_loss:0.0000\n",
      "====> Epoch: 18 Average loss: 93.2936\tClassifier Accuracy: 99.4207\n",
      "====> Test set loss: 92.8928\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 93.883987\tBCE:76.2191\tKLD:17.6649\tC_loss:0.0000\n",
      "Train Epoch: 19 [2560/60000 (4%)]\tLoss: 91.658783\tBCE:74.4315\tKLD:17.2273\tC_loss:0.0000\n",
      "Train Epoch: 19 [5120/60000 (9%)]\tLoss: 94.054146\tBCE:76.6516\tKLD:17.4025\tC_loss:0.0000\n",
      "Train Epoch: 19 [7680/60000 (13%)]\tLoss: 94.235245\tBCE:76.4863\tKLD:17.7489\tC_loss:0.0000\n",
      "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 93.727066\tBCE:75.9465\tKLD:17.7805\tC_loss:0.0000\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 93.832184\tBCE:76.9281\tKLD:16.9041\tC_loss:0.0000\n",
      "Train Epoch: 19 [15360/60000 (26%)]\tLoss: 91.171631\tBCE:73.6089\tKLD:17.5627\tC_loss:0.0000\n",
      "Train Epoch: 19 [17920/60000 (30%)]\tLoss: 92.864426\tBCE:75.6405\tKLD:17.2239\tC_loss:0.0000\n",
      "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 89.494675\tBCE:72.3299\tKLD:17.1648\tC_loss:0.0000\n",
      "Train Epoch: 19 [23040/60000 (38%)]\tLoss: 93.311333\tBCE:75.2519\tKLD:18.0594\tC_loss:0.0000\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 92.557320\tBCE:75.4252\tKLD:17.1321\tC_loss:0.0000\n",
      "Train Epoch: 19 [28160/60000 (47%)]\tLoss: 93.718376\tBCE:76.6995\tKLD:17.0188\tC_loss:0.0000\n",
      "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 90.353325\tBCE:73.4885\tKLD:16.8648\tC_loss:0.0000\n",
      "Train Epoch: 19 [33280/60000 (56%)]\tLoss: 93.492088\tBCE:76.4898\tKLD:17.0023\tC_loss:0.0000\n",
      "Train Epoch: 19 [35840/60000 (60%)]\tLoss: 92.524200\tBCE:75.2142\tKLD:17.3100\tC_loss:0.0000\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 93.892975\tBCE:76.6243\tKLD:17.2687\tC_loss:0.0000\n",
      "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 93.893135\tBCE:76.0254\tKLD:17.8678\tC_loss:0.0000\n",
      "Train Epoch: 19 [43520/60000 (73%)]\tLoss: 91.420273\tBCE:74.2719\tKLD:17.1484\tC_loss:0.0000\n",
      "Train Epoch: 19 [46080/60000 (77%)]\tLoss: 92.024139\tBCE:74.7611\tKLD:17.2630\tC_loss:0.0000\n",
      "Train Epoch: 19 [48640/60000 (81%)]\tLoss: 91.480186\tBCE:74.1434\tKLD:17.3368\tC_loss:0.0000\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 93.786171\tBCE:76.0097\tKLD:17.7764\tC_loss:0.0000\n",
      "Train Epoch: 19 [53760/60000 (90%)]\tLoss: 91.092865\tBCE:74.3945\tKLD:16.6983\tC_loss:0.0000\n",
      "Train Epoch: 19 [56320/60000 (94%)]\tLoss: 94.854111\tBCE:77.3249\tKLD:17.5291\tC_loss:0.0000\n",
      "Train Epoch: 19 [58880/60000 (98%)]\tLoss: 95.882607\tBCE:77.8574\tKLD:18.0252\tC_loss:0.0000\n",
      "====> Epoch: 19 Average loss: 92.8232\tClassifier Accuracy: 99.5209\n",
      "====> Test set loss: 94.6660\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 94.836243\tBCE:77.5099\tKLD:17.3263\tC_loss:0.0000\n",
      "Train Epoch: 20 [2560/60000 (4%)]\tLoss: 91.347496\tBCE:74.5468\tKLD:16.8007\tC_loss:0.0000\n",
      "Train Epoch: 20 [5120/60000 (9%)]\tLoss: 94.145996\tBCE:76.8369\tKLD:17.3091\tC_loss:0.0000\n",
      "Train Epoch: 20 [7680/60000 (13%)]\tLoss: 95.463806\tBCE:78.0363\tKLD:17.4275\tC_loss:0.0000\n",
      "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 90.409714\tBCE:73.5515\tKLD:16.8582\tC_loss:0.0000\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 93.722878\tBCE:76.4325\tKLD:17.2904\tC_loss:0.0000\n",
      "Train Epoch: 20 [15360/60000 (26%)]\tLoss: 95.446732\tBCE:77.8945\tKLD:17.5522\tC_loss:0.0000\n",
      "Train Epoch: 20 [17920/60000 (30%)]\tLoss: 92.174477\tBCE:75.2550\tKLD:16.9194\tC_loss:0.0000\n",
      "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 91.993950\tBCE:74.4530\tKLD:17.5409\tC_loss:0.0000\n",
      "Train Epoch: 20 [23040/60000 (38%)]\tLoss: 94.854340\tBCE:77.6448\tKLD:17.2095\tC_loss:0.0000\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 92.197945\tBCE:75.1630\tKLD:17.0350\tC_loss:0.0000\n",
      "Train Epoch: 20 [28160/60000 (47%)]\tLoss: 93.014465\tBCE:75.5681\tKLD:17.4463\tC_loss:0.0000\n",
      "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 89.159904\tBCE:72.0447\tKLD:17.1152\tC_loss:0.0000\n",
      "Train Epoch: 20 [33280/60000 (56%)]\tLoss: 89.857597\tBCE:72.9392\tKLD:16.9183\tC_loss:0.0000\n",
      "Train Epoch: 20 [35840/60000 (60%)]\tLoss: 91.500443\tBCE:74.2901\tKLD:17.2103\tC_loss:0.0000\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 93.830124\tBCE:76.6786\tKLD:17.1515\tC_loss:0.0000\n",
      "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 95.090500\tBCE:77.5869\tKLD:17.5036\tC_loss:0.0000\n",
      "Train Epoch: 20 [43520/60000 (73%)]\tLoss: 90.897804\tBCE:73.5864\tKLD:17.3114\tC_loss:0.0000\n",
      "Train Epoch: 20 [46080/60000 (77%)]\tLoss: 91.460464\tBCE:74.4180\tKLD:17.0424\tC_loss:0.0000\n",
      "Train Epoch: 20 [48640/60000 (81%)]\tLoss: 90.936081\tBCE:73.8879\tKLD:17.0482\tC_loss:0.0000\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 93.233215\tBCE:75.5106\tKLD:17.7226\tC_loss:0.0000\n",
      "Train Epoch: 20 [53760/60000 (90%)]\tLoss: 95.956223\tBCE:79.0525\tKLD:16.9037\tC_loss:0.0000\n",
      "Train Epoch: 20 [56320/60000 (94%)]\tLoss: 94.512169\tBCE:76.8634\tKLD:17.6488\tC_loss:0.0000\n",
      "Train Epoch: 20 [58880/60000 (98%)]\tLoss: 91.293808\tBCE:74.7065\tKLD:16.5873\tC_loss:0.0000\n",
      "====> Epoch: 20 Average loss: 92.8353\tClassifier Accuracy: 99.5459\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 92.7632\n",
      "Random number: 4\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 91.383469\tBCE:73.9452\tKLD:17.4382\tC_loss:0.0000\n",
      "Train Epoch: 21 [2560/60000 (4%)]\tLoss: 92.914902\tBCE:75.2585\tKLD:17.6564\tC_loss:0.0000\n",
      "Train Epoch: 21 [5120/60000 (9%)]\tLoss: 92.009575\tBCE:74.8920\tKLD:17.1176\tC_loss:0.0000\n",
      "Train Epoch: 21 [7680/60000 (13%)]\tLoss: 92.714096\tBCE:75.1154\tKLD:17.5986\tC_loss:0.0000\n",
      "Train Epoch: 21 [10240/60000 (17%)]\tLoss: 91.596474\tBCE:74.3784\tKLD:17.2180\tC_loss:0.0000\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 94.478027\tBCE:76.8730\tKLD:17.6050\tC_loss:0.0000\n",
      "Train Epoch: 21 [15360/60000 (26%)]\tLoss: 93.916214\tBCE:76.3957\tKLD:17.5205\tC_loss:0.0000\n",
      "Train Epoch: 21 [17920/60000 (30%)]\tLoss: 89.541351\tBCE:72.6607\tKLD:16.8806\tC_loss:0.0000\n",
      "Train Epoch: 21 [20480/60000 (34%)]\tLoss: 91.456436\tBCE:74.4857\tKLD:16.9707\tC_loss:0.0000\n",
      "Train Epoch: 21 [23040/60000 (38%)]\tLoss: 91.349213\tBCE:74.6056\tKLD:16.7436\tC_loss:0.0000\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 93.895401\tBCE:76.2419\tKLD:17.6535\tC_loss:0.0000\n",
      "Train Epoch: 21 [28160/60000 (47%)]\tLoss: 93.020477\tBCE:75.4453\tKLD:17.5752\tC_loss:0.0000\n",
      "Train Epoch: 21 [30720/60000 (51%)]\tLoss: 91.755768\tBCE:74.3195\tKLD:17.4363\tC_loss:0.0000\n",
      "Train Epoch: 21 [33280/60000 (56%)]\tLoss: 94.562271\tBCE:77.1400\tKLD:17.4223\tC_loss:0.0000\n",
      "Train Epoch: 21 [35840/60000 (60%)]\tLoss: 90.754539\tBCE:73.1744\tKLD:17.5801\tC_loss:0.0000\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 91.266693\tBCE:74.0600\tKLD:17.2067\tC_loss:0.0000\n",
      "Train Epoch: 21 [40960/60000 (68%)]\tLoss: 92.774544\tBCE:75.4038\tKLD:17.3707\tC_loss:0.0000\n",
      "Train Epoch: 21 [43520/60000 (73%)]\tLoss: 90.969315\tBCE:73.4982\tKLD:17.4711\tC_loss:0.0000\n",
      "Train Epoch: 21 [46080/60000 (77%)]\tLoss: 95.307999\tBCE:77.8627\tKLD:17.4453\tC_loss:0.0000\n",
      "Train Epoch: 21 [48640/60000 (81%)]\tLoss: 92.989792\tBCE:75.2792\tKLD:17.7106\tC_loss:0.0000\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 96.999397\tBCE:80.1566\tKLD:16.8428\tC_loss:0.0000\n",
      "Train Epoch: 21 [53760/60000 (90%)]\tLoss: 92.007195\tBCE:74.7006\tKLD:17.3065\tC_loss:0.0000\n",
      "Train Epoch: 21 [56320/60000 (94%)]\tLoss: 94.732376\tBCE:77.0569\tKLD:17.6754\tC_loss:0.0000\n",
      "Train Epoch: 21 [58880/60000 (98%)]\tLoss: 91.080772\tBCE:74.1842\tKLD:16.8966\tC_loss:0.0000\n",
      "====> Epoch: 21 Average loss: 92.4675\tClassifier Accuracy: 99.6461\n",
      "====> Test set loss: 92.7028\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 91.279343\tBCE:74.0012\tKLD:17.2782\tC_loss:0.0000\n",
      "Train Epoch: 22 [2560/60000 (4%)]\tLoss: 95.242279\tBCE:77.4812\tKLD:17.7610\tC_loss:0.0000\n",
      "Train Epoch: 22 [5120/60000 (9%)]\tLoss: 91.920052\tBCE:74.2839\tKLD:17.6361\tC_loss:0.0000\n",
      "Train Epoch: 22 [7680/60000 (13%)]\tLoss: 95.806961\tBCE:77.8997\tKLD:17.9072\tC_loss:0.0000\n",
      "Train Epoch: 22 [10240/60000 (17%)]\tLoss: 91.261398\tBCE:74.3207\tKLD:16.9406\tC_loss:0.0000\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 94.123489\tBCE:76.7240\tKLD:17.3994\tC_loss:0.0000\n",
      "Train Epoch: 22 [15360/60000 (26%)]\tLoss: 92.184586\tBCE:74.9740\tKLD:17.2105\tC_loss:0.0000\n",
      "Train Epoch: 22 [17920/60000 (30%)]\tLoss: 90.909973\tBCE:73.8844\tKLD:17.0256\tC_loss:0.0000\n",
      "Train Epoch: 22 [20480/60000 (34%)]\tLoss: 92.455254\tBCE:75.0023\tKLD:17.4529\tC_loss:0.0000\n",
      "Train Epoch: 22 [23040/60000 (38%)]\tLoss: 92.502945\tBCE:74.8490\tKLD:17.6539\tC_loss:0.0000\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 91.219704\tBCE:74.4298\tKLD:16.7899\tC_loss:0.0000\n",
      "Train Epoch: 22 [28160/60000 (47%)]\tLoss: 93.070068\tBCE:75.9023\tKLD:17.1677\tC_loss:0.0000\n",
      "Train Epoch: 22 [30720/60000 (51%)]\tLoss: 90.665100\tBCE:73.1969\tKLD:17.4682\tC_loss:0.0000\n",
      "Train Epoch: 22 [33280/60000 (56%)]\tLoss: 95.474464\tBCE:78.3375\tKLD:17.1369\tC_loss:0.0000\n",
      "Train Epoch: 22 [35840/60000 (60%)]\tLoss: 91.778763\tBCE:74.3614\tKLD:17.4173\tC_loss:0.0000\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 91.253380\tBCE:74.4500\tKLD:16.8033\tC_loss:0.0000\n",
      "Train Epoch: 22 [40960/60000 (68%)]\tLoss: 93.075935\tBCE:75.7327\tKLD:17.3432\tC_loss:0.0000\n",
      "Train Epoch: 22 [43520/60000 (73%)]\tLoss: 90.425499\tBCE:73.5587\tKLD:16.8668\tC_loss:0.0000\n",
      "Train Epoch: 22 [46080/60000 (77%)]\tLoss: 92.714554\tBCE:74.7264\tKLD:17.9882\tC_loss:0.0000\n",
      "Train Epoch: 22 [48640/60000 (81%)]\tLoss: 91.501160\tBCE:74.0509\tKLD:17.4503\tC_loss:0.0000\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 89.991486\tBCE:73.2006\tKLD:16.7908\tC_loss:0.0000\n",
      "Train Epoch: 22 [53760/60000 (90%)]\tLoss: 91.467949\tBCE:73.7131\tKLD:17.7548\tC_loss:0.0000\n",
      "Train Epoch: 22 [56320/60000 (94%)]\tLoss: 91.135490\tBCE:73.5189\tKLD:17.6166\tC_loss:0.0000\n",
      "Train Epoch: 22 [58880/60000 (98%)]\tLoss: 90.684708\tBCE:73.2745\tKLD:17.4102\tC_loss:0.0000\n",
      "====> Epoch: 22 Average loss: 92.2752\tClassifier Accuracy: 99.6895\n",
      "====> Test set loss: 92.4425\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 90.423370\tBCE:73.4560\tKLD:16.9673\tC_loss:0.0000\n",
      "Train Epoch: 23 [2560/60000 (4%)]\tLoss: 91.108604\tBCE:73.2399\tKLD:17.8686\tC_loss:0.0000\n",
      "Train Epoch: 23 [5120/60000 (9%)]\tLoss: 93.931526\tBCE:76.8990\tKLD:17.0325\tC_loss:0.0000\n",
      "Train Epoch: 23 [7680/60000 (13%)]\tLoss: 94.441040\tBCE:77.1813\tKLD:17.2598\tC_loss:0.0000\n",
      "Train Epoch: 23 [10240/60000 (17%)]\tLoss: 93.819733\tBCE:76.4651\tKLD:17.3546\tC_loss:0.0000\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 92.306450\tBCE:74.9911\tKLD:17.3154\tC_loss:0.0000\n",
      "Train Epoch: 23 [15360/60000 (26%)]\tLoss: 91.535522\tBCE:74.8787\tKLD:16.6568\tC_loss:0.0000\n",
      "Train Epoch: 23 [17920/60000 (30%)]\tLoss: 94.787323\tBCE:76.8250\tKLD:17.9623\tC_loss:0.0000\n",
      "Train Epoch: 23 [20480/60000 (34%)]\tLoss: 91.399323\tBCE:74.1313\tKLD:17.2680\tC_loss:0.0000\n",
      "Train Epoch: 23 [23040/60000 (38%)]\tLoss: 95.089760\tBCE:77.5364\tKLD:17.5533\tC_loss:0.0000\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 95.052322\tBCE:76.8667\tKLD:18.1856\tC_loss:0.0000\n",
      "Train Epoch: 23 [28160/60000 (47%)]\tLoss: 90.899292\tBCE:73.8245\tKLD:17.0748\tC_loss:0.0000\n",
      "Train Epoch: 23 [30720/60000 (51%)]\tLoss: 93.241318\tBCE:75.0642\tKLD:18.1771\tC_loss:0.0000\n",
      "Train Epoch: 23 [33280/60000 (56%)]\tLoss: 92.213333\tBCE:74.8535\tKLD:17.3598\tC_loss:0.0000\n",
      "Train Epoch: 23 [35840/60000 (60%)]\tLoss: 93.542419\tBCE:76.0208\tKLD:17.5216\tC_loss:0.0000\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 89.633636\tBCE:72.5343\tKLD:17.0993\tC_loss:0.0000\n",
      "Train Epoch: 23 [40960/60000 (68%)]\tLoss: 89.234451\tBCE:71.9751\tKLD:17.2594\tC_loss:0.0000\n",
      "Train Epoch: 23 [43520/60000 (73%)]\tLoss: 92.454788\tBCE:75.5499\tKLD:16.9049\tC_loss:0.0000\n",
      "Train Epoch: 23 [46080/60000 (77%)]\tLoss: 90.995575\tBCE:73.7465\tKLD:17.2491\tC_loss:0.0000\n",
      "Train Epoch: 23 [48640/60000 (81%)]\tLoss: 93.019516\tBCE:75.6087\tKLD:17.4108\tC_loss:0.0000\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 92.332932\tBCE:75.1551\tKLD:17.1778\tC_loss:0.0000\n",
      "Train Epoch: 23 [53760/60000 (90%)]\tLoss: 92.114609\tBCE:74.6372\tKLD:17.4774\tC_loss:0.0000\n",
      "Train Epoch: 23 [56320/60000 (94%)]\tLoss: 91.683144\tBCE:74.6759\tKLD:17.0072\tC_loss:0.0000\n",
      "Train Epoch: 23 [58880/60000 (98%)]\tLoss: 94.380569\tBCE:76.5959\tKLD:17.7846\tC_loss:0.0000\n",
      "====> Epoch: 23 Average loss: 92.1701\tClassifier Accuracy: 99.7346\n",
      "====> Test set loss: 92.2482\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 91.995773\tBCE:74.5588\tKLD:17.4370\tC_loss:0.0000\n",
      "Train Epoch: 24 [2560/60000 (4%)]\tLoss: 94.212486\tBCE:76.2416\tKLD:17.9709\tC_loss:0.0000\n",
      "Train Epoch: 24 [5120/60000 (9%)]\tLoss: 88.766884\tBCE:71.7782\tKLD:16.9887\tC_loss:0.0000\n",
      "Train Epoch: 24 [7680/60000 (13%)]\tLoss: 92.292046\tBCE:74.8078\tKLD:17.4843\tC_loss:0.0000\n",
      "Train Epoch: 24 [10240/60000 (17%)]\tLoss: 87.026901\tBCE:70.1051\tKLD:16.9218\tC_loss:0.0000\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 94.919128\tBCE:77.1975\tKLD:17.7216\tC_loss:0.0000\n",
      "Train Epoch: 24 [15360/60000 (26%)]\tLoss: 92.231964\tBCE:74.7839\tKLD:17.4481\tC_loss:0.0000\n",
      "Train Epoch: 24 [17920/60000 (30%)]\tLoss: 90.182159\tBCE:73.3373\tKLD:16.8448\tC_loss:0.0000\n",
      "Train Epoch: 24 [20480/60000 (34%)]\tLoss: 95.082275\tBCE:77.2478\tKLD:17.8344\tC_loss:0.0000\n",
      "Train Epoch: 24 [23040/60000 (38%)]\tLoss: 91.959564\tBCE:74.3408\tKLD:17.6188\tC_loss:0.0000\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 93.862602\tBCE:76.4212\tKLD:17.4414\tC_loss:0.0000\n",
      "Train Epoch: 24 [28160/60000 (47%)]\tLoss: 91.157921\tBCE:73.8798\tKLD:17.2781\tC_loss:0.0000\n",
      "Train Epoch: 24 [30720/60000 (51%)]\tLoss: 94.289024\tBCE:76.6096\tKLD:17.6794\tC_loss:0.0000\n",
      "Train Epoch: 24 [33280/60000 (56%)]\tLoss: 92.045273\tBCE:74.9533\tKLD:17.0920\tC_loss:0.0000\n",
      "Train Epoch: 24 [35840/60000 (60%)]\tLoss: 92.438240\tBCE:75.3777\tKLD:17.0606\tC_loss:0.0000\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 91.637848\tBCE:74.2646\tKLD:17.3732\tC_loss:0.0000\n",
      "Train Epoch: 24 [40960/60000 (68%)]\tLoss: 94.168533\tBCE:76.7581\tKLD:17.4104\tC_loss:0.0000\n",
      "Train Epoch: 24 [43520/60000 (73%)]\tLoss: 93.991409\tBCE:76.0641\tKLD:17.9273\tC_loss:0.0000\n",
      "Train Epoch: 24 [46080/60000 (77%)]\tLoss: 90.862839\tBCE:73.7557\tKLD:17.1071\tC_loss:0.0000\n",
      "Train Epoch: 24 [48640/60000 (81%)]\tLoss: 89.895447\tBCE:72.3969\tKLD:17.4985\tC_loss:0.0000\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 90.983246\tBCE:73.4704\tKLD:17.5128\tC_loss:0.0000\n",
      "Train Epoch: 24 [53760/60000 (90%)]\tLoss: 90.381210\tBCE:72.9919\tKLD:17.3893\tC_loss:0.0000\n",
      "Train Epoch: 24 [56320/60000 (94%)]\tLoss: 90.054886\tBCE:72.4736\tKLD:17.5813\tC_loss:0.0000\n",
      "Train Epoch: 24 [58880/60000 (98%)]\tLoss: 90.647835\tBCE:73.7502\tKLD:16.8977\tC_loss:0.0000\n",
      "====> Epoch: 24 Average loss: 91.8975\tClassifier Accuracy: 99.7596\n",
      "====> Test set loss: 92.2570\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 95.171082\tBCE:77.5519\tKLD:17.6192\tC_loss:0.0000\n",
      "Train Epoch: 25 [2560/60000 (4%)]\tLoss: 90.104721\tBCE:72.4422\tKLD:17.6625\tC_loss:0.0000\n",
      "Train Epoch: 25 [5120/60000 (9%)]\tLoss: 92.398018\tBCE:74.3515\tKLD:18.0466\tC_loss:0.0000\n",
      "Train Epoch: 25 [7680/60000 (13%)]\tLoss: 91.321442\tBCE:74.0925\tKLD:17.2289\tC_loss:0.0000\n",
      "Train Epoch: 25 [10240/60000 (17%)]\tLoss: 91.013023\tBCE:73.8262\tKLD:17.1868\tC_loss:0.0000\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 91.862541\tBCE:74.6175\tKLD:17.2451\tC_loss:0.0000\n",
      "Train Epoch: 25 [15360/60000 (26%)]\tLoss: 94.525764\tBCE:76.3027\tKLD:18.2231\tC_loss:0.0000\n",
      "Train Epoch: 25 [17920/60000 (30%)]\tLoss: 94.332840\tBCE:76.8225\tKLD:17.5103\tC_loss:0.0000\n",
      "Train Epoch: 25 [20480/60000 (34%)]\tLoss: 87.279228\tBCE:70.5368\tKLD:16.7425\tC_loss:0.0000\n",
      "Train Epoch: 25 [23040/60000 (38%)]\tLoss: 92.957657\tBCE:75.4830\tKLD:17.4746\tC_loss:0.0000\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 90.547501\tBCE:73.2753\tKLD:17.2722\tC_loss:0.0000\n",
      "Train Epoch: 25 [28160/60000 (47%)]\tLoss: 92.780273\tBCE:75.1720\tKLD:17.6083\tC_loss:0.0000\n",
      "Train Epoch: 25 [30720/60000 (51%)]\tLoss: 91.128853\tBCE:73.9803\tKLD:17.1485\tC_loss:0.0000\n",
      "Train Epoch: 25 [33280/60000 (56%)]\tLoss: 91.160019\tBCE:74.0448\tKLD:17.1152\tC_loss:0.0000\n",
      "Train Epoch: 25 [35840/60000 (60%)]\tLoss: 90.198715\tBCE:72.6829\tKLD:17.5158\tC_loss:0.0000\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 92.715027\tBCE:75.8758\tKLD:16.8392\tC_loss:0.0000\n",
      "Train Epoch: 25 [40960/60000 (68%)]\tLoss: 92.825882\tBCE:75.4722\tKLD:17.3537\tC_loss:0.0000\n",
      "Train Epoch: 25 [43520/60000 (73%)]\tLoss: 92.933762\tBCE:75.5032\tKLD:17.4305\tC_loss:0.0000\n",
      "Train Epoch: 25 [46080/60000 (77%)]\tLoss: 94.448883\tBCE:77.4859\tKLD:16.9629\tC_loss:0.0000\n",
      "Train Epoch: 25 [48640/60000 (81%)]\tLoss: 90.854828\tBCE:73.0693\tKLD:17.7855\tC_loss:0.0000\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 95.129929\tBCE:77.6020\tKLD:17.5280\tC_loss:0.0000\n",
      "Train Epoch: 25 [53760/60000 (90%)]\tLoss: 91.730003\tBCE:74.5286\tKLD:17.2014\tC_loss:0.0000\n",
      "Train Epoch: 25 [56320/60000 (94%)]\tLoss: 92.283943\tBCE:75.2557\tKLD:17.0282\tC_loss:0.0000\n",
      "Train Epoch: 25 [58880/60000 (98%)]\tLoss: 93.025581\tBCE:75.6866\tKLD:17.3390\tC_loss:0.0000\n",
      "====> Epoch: 25 Average loss: 91.7440\tClassifier Accuracy: 99.8214\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 92.0570\n",
      "Random number: 4\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 91.937798\tBCE:74.3250\tKLD:17.6127\tC_loss:0.0000\n",
      "Train Epoch: 26 [2560/60000 (4%)]\tLoss: 88.758629\tBCE:71.7499\tKLD:17.0088\tC_loss:0.0000\n",
      "Train Epoch: 26 [5120/60000 (9%)]\tLoss: 92.972183\tBCE:75.4789\tKLD:17.4933\tC_loss:0.0000\n",
      "Train Epoch: 26 [7680/60000 (13%)]\tLoss: 87.517761\tBCE:70.1390\tKLD:17.3788\tC_loss:0.0000\n",
      "Train Epoch: 26 [10240/60000 (17%)]\tLoss: 90.420547\tBCE:73.3523\tKLD:17.0682\tC_loss:0.0000\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 90.824463\tBCE:73.6441\tKLD:17.1804\tC_loss:0.0000\n",
      "Train Epoch: 26 [15360/60000 (26%)]\tLoss: 88.822968\tBCE:71.6159\tKLD:17.2071\tC_loss:0.0000\n",
      "Train Epoch: 26 [17920/60000 (30%)]\tLoss: 93.797386\tBCE:76.0977\tKLD:17.6997\tC_loss:0.0000\n",
      "Train Epoch: 26 [20480/60000 (34%)]\tLoss: 91.062485\tBCE:73.8873\tKLD:17.1752\tC_loss:0.0000\n",
      "Train Epoch: 26 [23040/60000 (38%)]\tLoss: 93.170349\tBCE:76.2356\tKLD:16.9347\tC_loss:0.0000\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 94.863693\tBCE:76.7989\tKLD:18.0648\tC_loss:0.0000\n",
      "Train Epoch: 26 [28160/60000 (47%)]\tLoss: 90.294083\tBCE:73.1863\tKLD:17.1078\tC_loss:0.0000\n",
      "Train Epoch: 26 [30720/60000 (51%)]\tLoss: 88.990753\tBCE:71.6832\tKLD:17.3075\tC_loss:0.0000\n",
      "Train Epoch: 26 [33280/60000 (56%)]\tLoss: 87.577942\tBCE:70.5191\tKLD:17.0589\tC_loss:0.0000\n",
      "Train Epoch: 26 [35840/60000 (60%)]\tLoss: 91.955795\tBCE:73.9681\tKLD:17.9877\tC_loss:0.0000\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 90.900627\tBCE:73.7661\tKLD:17.1345\tC_loss:0.0000\n",
      "Train Epoch: 26 [40960/60000 (68%)]\tLoss: 91.608353\tBCE:74.3700\tKLD:17.2383\tC_loss:0.0000\n",
      "Train Epoch: 26 [43520/60000 (73%)]\tLoss: 92.708305\tBCE:75.5323\tKLD:17.1760\tC_loss:0.0000\n",
      "Train Epoch: 26 [46080/60000 (77%)]\tLoss: 92.495422\tBCE:74.7892\tKLD:17.7063\tC_loss:0.0000\n",
      "Train Epoch: 26 [48640/60000 (81%)]\tLoss: 91.100967\tBCE:74.2086\tKLD:16.8924\tC_loss:0.0000\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 91.350456\tBCE:73.7803\tKLD:17.5702\tC_loss:0.0000\n",
      "Train Epoch: 26 [53760/60000 (90%)]\tLoss: 91.160492\tBCE:74.3904\tKLD:16.7700\tC_loss:0.0000\n",
      "Train Epoch: 26 [56320/60000 (94%)]\tLoss: 94.430634\tBCE:76.7917\tKLD:17.6390\tC_loss:0.0000\n",
      "Train Epoch: 26 [58880/60000 (98%)]\tLoss: 92.920807\tBCE:75.0947\tKLD:17.8261\tC_loss:0.0000\n",
      "====> Epoch: 26 Average loss: 91.5554\tClassifier Accuracy: 99.8431\n",
      "====> Test set loss: 92.3118\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 89.029938\tBCE:72.4369\tKLD:16.5930\tC_loss:0.0000\n",
      "Train Epoch: 27 [2560/60000 (4%)]\tLoss: 89.405762\tBCE:72.1320\tKLD:17.2738\tC_loss:0.0000\n",
      "Train Epoch: 27 [5120/60000 (9%)]\tLoss: 92.133713\tBCE:74.5044\tKLD:17.6293\tC_loss:0.0000\n",
      "Train Epoch: 27 [7680/60000 (13%)]\tLoss: 93.605957\tBCE:76.0360\tKLD:17.5699\tC_loss:0.0000\n",
      "Train Epoch: 27 [10240/60000 (17%)]\tLoss: 93.983841\tBCE:77.0569\tKLD:16.9270\tC_loss:0.0000\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 91.623703\tBCE:74.0442\tKLD:17.5795\tC_loss:0.0000\n",
      "Train Epoch: 27 [15360/60000 (26%)]\tLoss: 93.048065\tBCE:75.4398\tKLD:17.6082\tC_loss:0.0000\n",
      "Train Epoch: 27 [17920/60000 (30%)]\tLoss: 94.830902\tBCE:76.6701\tKLD:18.1608\tC_loss:0.0000\n",
      "Train Epoch: 27 [20480/60000 (34%)]\tLoss: 93.145187\tBCE:75.9819\tKLD:17.1633\tC_loss:0.0000\n",
      "Train Epoch: 27 [23040/60000 (38%)]\tLoss: 93.496536\tBCE:75.6374\tKLD:17.8591\tC_loss:0.0000\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 92.332077\tBCE:74.7313\tKLD:17.6008\tC_loss:0.0000\n",
      "Train Epoch: 27 [28160/60000 (47%)]\tLoss: 91.011826\tBCE:73.8693\tKLD:17.1425\tC_loss:0.0000\n",
      "Train Epoch: 27 [30720/60000 (51%)]\tLoss: 89.590302\tBCE:72.2885\tKLD:17.3018\tC_loss:0.0000\n",
      "Train Epoch: 27 [33280/60000 (56%)]\tLoss: 93.821335\tBCE:76.2997\tKLD:17.5216\tC_loss:0.0000\n",
      "Train Epoch: 27 [35840/60000 (60%)]\tLoss: 94.526718\tBCE:77.0248\tKLD:17.5019\tC_loss:0.0000\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 91.884621\tBCE:73.8504\tKLD:18.0342\tC_loss:0.0000\n",
      "Train Epoch: 27 [40960/60000 (68%)]\tLoss: 90.721008\tBCE:73.2745\tKLD:17.4465\tC_loss:0.0000\n",
      "Train Epoch: 27 [43520/60000 (73%)]\tLoss: 89.914917\tBCE:72.5360\tKLD:17.3789\tC_loss:0.0000\n",
      "Train Epoch: 27 [46080/60000 (77%)]\tLoss: 89.406898\tBCE:71.8353\tKLD:17.5716\tC_loss:0.0000\n",
      "Train Epoch: 27 [48640/60000 (81%)]\tLoss: 93.000702\tBCE:75.1530\tKLD:17.8477\tC_loss:0.0000\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 90.627594\tBCE:73.6605\tKLD:16.9671\tC_loss:0.0000\n",
      "Train Epoch: 27 [53760/60000 (90%)]\tLoss: 92.022789\tBCE:74.8995\tKLD:17.1233\tC_loss:0.0000\n",
      "Train Epoch: 27 [56320/60000 (94%)]\tLoss: 90.166534\tBCE:72.8002\tKLD:17.3663\tC_loss:0.0000\n",
      "Train Epoch: 27 [58880/60000 (98%)]\tLoss: 92.204002\tBCE:74.6911\tKLD:17.5129\tC_loss:0.0000\n",
      "====> Epoch: 27 Average loss: 91.4435\tClassifier Accuracy: 99.8464\n",
      "====> Test set loss: 92.9177\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 91.939835\tBCE:74.9221\tKLD:17.0178\tC_loss:0.0000\n",
      "Train Epoch: 28 [2560/60000 (4%)]\tLoss: 91.470047\tBCE:73.7198\tKLD:17.7502\tC_loss:0.0000\n",
      "Train Epoch: 28 [5120/60000 (9%)]\tLoss: 89.666672\tBCE:72.3357\tKLD:17.3309\tC_loss:0.0000\n",
      "Train Epoch: 28 [7680/60000 (13%)]\tLoss: 88.927269\tBCE:71.9958\tKLD:16.9314\tC_loss:0.0000\n",
      "Train Epoch: 28 [10240/60000 (17%)]\tLoss: 92.887833\tBCE:75.1296\tKLD:17.7582\tC_loss:0.0000\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 92.540237\tBCE:75.3722\tKLD:17.1680\tC_loss:0.0000\n",
      "Train Epoch: 28 [15360/60000 (26%)]\tLoss: 94.901154\tBCE:77.8072\tKLD:17.0940\tC_loss:0.0000\n",
      "Train Epoch: 28 [17920/60000 (30%)]\tLoss: 92.806747\tBCE:74.9365\tKLD:17.8703\tC_loss:0.0000\n",
      "Train Epoch: 28 [20480/60000 (34%)]\tLoss: 92.081375\tBCE:74.5413\tKLD:17.5401\tC_loss:0.0000\n",
      "Train Epoch: 28 [23040/60000 (38%)]\tLoss: 91.279846\tBCE:73.1328\tKLD:18.1470\tC_loss:0.0000\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 93.448471\tBCE:75.7870\tKLD:17.6614\tC_loss:0.0000\n",
      "Train Epoch: 28 [28160/60000 (47%)]\tLoss: 92.231003\tBCE:74.5900\tKLD:17.6410\tC_loss:0.0000\n",
      "Train Epoch: 28 [30720/60000 (51%)]\tLoss: 93.532990\tBCE:75.6579\tKLD:17.8751\tC_loss:0.0000\n",
      "Train Epoch: 28 [33280/60000 (56%)]\tLoss: 92.823074\tBCE:75.3356\tKLD:17.4875\tC_loss:0.0000\n",
      "Train Epoch: 28 [35840/60000 (60%)]\tLoss: 93.652031\tBCE:76.4246\tKLD:17.2274\tC_loss:0.0000\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 91.110100\tBCE:73.8588\tKLD:17.2513\tC_loss:0.0000\n",
      "Train Epoch: 28 [40960/60000 (68%)]\tLoss: 91.458046\tBCE:74.0834\tKLD:17.3747\tC_loss:0.0000\n",
      "Train Epoch: 28 [43520/60000 (73%)]\tLoss: 91.332321\tBCE:74.2290\tKLD:17.1033\tC_loss:0.0000\n",
      "Train Epoch: 28 [46080/60000 (77%)]\tLoss: 94.315750\tBCE:76.8809\tKLD:17.4348\tC_loss:0.0000\n",
      "Train Epoch: 28 [48640/60000 (81%)]\tLoss: 91.569901\tBCE:73.7120\tKLD:17.8578\tC_loss:0.0000\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 88.799957\tBCE:71.5783\tKLD:17.2216\tC_loss:0.0000\n",
      "Train Epoch: 28 [53760/60000 (90%)]\tLoss: 92.082054\tBCE:74.7061\tKLD:17.3759\tC_loss:0.0000\n",
      "Train Epoch: 28 [56320/60000 (94%)]\tLoss: 93.430389\tBCE:75.9903\tKLD:17.4401\tC_loss:0.0000\n",
      "Train Epoch: 28 [58880/60000 (98%)]\tLoss: 89.682755\tBCE:72.5222\tKLD:17.1606\tC_loss:0.0000\n",
      "====> Epoch: 28 Average loss: 91.2911\tClassifier Accuracy: 99.8831\n",
      "====> Test set loss: 93.1162\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 91.691734\tBCE:74.1123\tKLD:17.5795\tC_loss:0.0000\n",
      "Train Epoch: 29 [2560/60000 (4%)]\tLoss: 92.562965\tBCE:75.2400\tKLD:17.3230\tC_loss:0.0000\n",
      "Train Epoch: 29 [5120/60000 (9%)]\tLoss: 92.778152\tBCE:75.0227\tKLD:17.7555\tC_loss:0.0000\n",
      "Train Epoch: 29 [7680/60000 (13%)]\tLoss: 92.158310\tBCE:74.3104\tKLD:17.8479\tC_loss:0.0000\n",
      "Train Epoch: 29 [10240/60000 (17%)]\tLoss: 90.613556\tBCE:73.0663\tKLD:17.5473\tC_loss:0.0000\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 89.382538\tBCE:71.8038\tKLD:17.5788\tC_loss:0.0000\n",
      "Train Epoch: 29 [15360/60000 (26%)]\tLoss: 91.159081\tBCE:73.4829\tKLD:17.6762\tC_loss:0.0000\n",
      "Train Epoch: 29 [17920/60000 (30%)]\tLoss: 88.638252\tBCE:71.6897\tKLD:16.9486\tC_loss:0.0000\n",
      "Train Epoch: 29 [20480/60000 (34%)]\tLoss: 92.350082\tBCE:74.4514\tKLD:17.8987\tC_loss:0.0000\n",
      "Train Epoch: 29 [23040/60000 (38%)]\tLoss: 92.444611\tBCE:74.9336\tKLD:17.5110\tC_loss:0.0000\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 91.099823\tBCE:73.6282\tKLD:17.4716\tC_loss:0.0000\n",
      "Train Epoch: 29 [28160/60000 (47%)]\tLoss: 91.360146\tBCE:74.2580\tKLD:17.1022\tC_loss:0.0000\n",
      "Train Epoch: 29 [30720/60000 (51%)]\tLoss: 90.198586\tBCE:72.6908\tKLD:17.5078\tC_loss:0.0000\n",
      "Train Epoch: 29 [33280/60000 (56%)]\tLoss: 92.729584\tBCE:75.1807\tKLD:17.5489\tC_loss:0.0000\n",
      "Train Epoch: 29 [35840/60000 (60%)]\tLoss: 94.227554\tBCE:76.5687\tKLD:17.6588\tC_loss:0.0000\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 90.206375\tBCE:73.5630\tKLD:16.6433\tC_loss:0.0000\n",
      "Train Epoch: 29 [40960/60000 (68%)]\tLoss: 93.030235\tBCE:74.6651\tKLD:18.3651\tC_loss:0.0000\n",
      "Train Epoch: 29 [43520/60000 (73%)]\tLoss: 90.698624\tBCE:74.0867\tKLD:16.6120\tC_loss:0.0000\n",
      "Train Epoch: 29 [46080/60000 (77%)]\tLoss: 91.417625\tBCE:73.7269\tKLD:17.6907\tC_loss:0.0000\n",
      "Train Epoch: 29 [48640/60000 (81%)]\tLoss: 91.712624\tBCE:74.0916\tKLD:17.6210\tC_loss:0.0000\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 90.991928\tBCE:73.5926\tKLD:17.3993\tC_loss:0.0000\n",
      "Train Epoch: 29 [53760/60000 (90%)]\tLoss: 91.222122\tBCE:74.1032\tKLD:17.1189\tC_loss:0.0000\n",
      "Train Epoch: 29 [56320/60000 (94%)]\tLoss: 88.539886\tBCE:71.3207\tKLD:17.2192\tC_loss:0.0000\n",
      "Train Epoch: 29 [58880/60000 (98%)]\tLoss: 92.831940\tBCE:75.4695\tKLD:17.3624\tC_loss:0.0000\n",
      "====> Epoch: 29 Average loss: 91.2257\tClassifier Accuracy: 99.9282\n",
      "====> Test set loss: 91.8613\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 93.899582\tBCE:76.2230\tKLD:17.6766\tC_loss:0.0000\n",
      "Train Epoch: 30 [2560/60000 (4%)]\tLoss: 90.783440\tBCE:73.1642\tKLD:17.6192\tC_loss:0.0000\n",
      "Train Epoch: 30 [5120/60000 (9%)]\tLoss: 90.612274\tBCE:72.8062\tKLD:17.8061\tC_loss:0.0000\n",
      "Train Epoch: 30 [7680/60000 (13%)]\tLoss: 90.180550\tBCE:72.9714\tKLD:17.2091\tC_loss:0.0000\n",
      "Train Epoch: 30 [10240/60000 (17%)]\tLoss: 90.720245\tBCE:72.8006\tKLD:17.9196\tC_loss:0.0000\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 90.487816\tBCE:73.0570\tKLD:17.4308\tC_loss:0.0000\n",
      "Train Epoch: 30 [15360/60000 (26%)]\tLoss: 92.858772\tBCE:75.2952\tKLD:17.5636\tC_loss:0.0000\n",
      "Train Epoch: 30 [17920/60000 (30%)]\tLoss: 93.279922\tBCE:75.3995\tKLD:17.8804\tC_loss:0.0000\n",
      "Train Epoch: 30 [20480/60000 (34%)]\tLoss: 89.741913\tBCE:72.7522\tKLD:16.9897\tC_loss:0.0000\n",
      "Train Epoch: 30 [23040/60000 (38%)]\tLoss: 89.147659\tBCE:72.1243\tKLD:17.0233\tC_loss:0.0000\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 94.153748\tBCE:76.6880\tKLD:17.4658\tC_loss:0.0000\n",
      "Train Epoch: 30 [28160/60000 (47%)]\tLoss: 91.808861\tBCE:74.7620\tKLD:17.0468\tC_loss:0.0000\n",
      "Train Epoch: 30 [30720/60000 (51%)]\tLoss: 91.639198\tBCE:74.1225\tKLD:17.5167\tC_loss:0.0000\n",
      "Train Epoch: 30 [33280/60000 (56%)]\tLoss: 91.881989\tBCE:75.0757\tKLD:16.8063\tC_loss:0.0000\n",
      "Train Epoch: 30 [35840/60000 (60%)]\tLoss: 92.401543\tBCE:74.8964\tKLD:17.5052\tC_loss:0.0000\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 92.054146\tBCE:74.2015\tKLD:17.8527\tC_loss:0.0000\n",
      "Train Epoch: 30 [40960/60000 (68%)]\tLoss: 93.198364\tBCE:75.7016\tKLD:17.4968\tC_loss:0.0000\n",
      "Train Epoch: 30 [43520/60000 (73%)]\tLoss: 93.067635\tBCE:75.5720\tKLD:17.4956\tC_loss:0.0000\n",
      "Train Epoch: 30 [46080/60000 (77%)]\tLoss: 92.219307\tBCE:74.7973\tKLD:17.4220\tC_loss:0.0000\n",
      "Train Epoch: 30 [48640/60000 (81%)]\tLoss: 92.320824\tBCE:74.4830\tKLD:17.8378\tC_loss:0.0000\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 91.208832\tBCE:74.0336\tKLD:17.1753\tC_loss:0.0000\n",
      "Train Epoch: 30 [53760/60000 (90%)]\tLoss: 90.259293\tBCE:73.3432\tKLD:16.9161\tC_loss:0.0000\n",
      "Train Epoch: 30 [56320/60000 (94%)]\tLoss: 91.549408\tBCE:73.6294\tKLD:17.9200\tC_loss:0.0000\n",
      "Train Epoch: 30 [58880/60000 (98%)]\tLoss: 94.711334\tBCE:76.9478\tKLD:17.7635\tC_loss:0.0000\n",
      "====> Epoch: 30 Average loss: 91.0698\tClassifier Accuracy: 99.9416\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.6605\n",
      "Random number: 4\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 94.281212\tBCE:76.7440\tKLD:17.5372\tC_loss:0.0000\n",
      "Train Epoch: 31 [2560/60000 (4%)]\tLoss: 91.490250\tBCE:73.8486\tKLD:17.6417\tC_loss:0.0000\n",
      "Train Epoch: 31 [5120/60000 (9%)]\tLoss: 93.768738\tBCE:75.9546\tKLD:17.8142\tC_loss:0.0000\n",
      "Train Epoch: 31 [7680/60000 (13%)]\tLoss: 89.092888\tBCE:72.2036\tKLD:16.8893\tC_loss:0.0000\n",
      "Train Epoch: 31 [10240/60000 (17%)]\tLoss: 89.334702\tBCE:71.8295\tKLD:17.5052\tC_loss:0.0000\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 89.703827\tBCE:72.7681\tKLD:16.9357\tC_loss:0.0000\n",
      "Train Epoch: 31 [15360/60000 (26%)]\tLoss: 91.584038\tBCE:73.5507\tKLD:18.0334\tC_loss:0.0000\n",
      "Train Epoch: 31 [17920/60000 (30%)]\tLoss: 89.936890\tBCE:72.5116\tKLD:17.4252\tC_loss:0.0000\n",
      "Train Epoch: 31 [20480/60000 (34%)]\tLoss: 88.613098\tBCE:72.0012\tKLD:16.6119\tC_loss:0.0000\n",
      "Train Epoch: 31 [23040/60000 (38%)]\tLoss: 88.235954\tBCE:71.4644\tKLD:16.7715\tC_loss:0.0000\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 89.927780\tBCE:72.3619\tKLD:17.5659\tC_loss:0.0000\n",
      "Train Epoch: 31 [28160/60000 (47%)]\tLoss: 89.284744\tBCE:72.0243\tKLD:17.2604\tC_loss:0.0000\n",
      "Train Epoch: 31 [30720/60000 (51%)]\tLoss: 90.851822\tBCE:73.3109\tKLD:17.5409\tC_loss:0.0000\n",
      "Train Epoch: 31 [33280/60000 (56%)]\tLoss: 95.421341\tBCE:77.3004\tKLD:18.1209\tC_loss:0.0000\n",
      "Train Epoch: 31 [35840/60000 (60%)]\tLoss: 90.814911\tBCE:73.3837\tKLD:17.4312\tC_loss:0.0000\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 89.433510\tBCE:72.2880\tKLD:17.1455\tC_loss:0.0000\n",
      "Train Epoch: 31 [40960/60000 (68%)]\tLoss: 92.420052\tBCE:74.8627\tKLD:17.5573\tC_loss:0.0000\n",
      "Train Epoch: 31 [43520/60000 (73%)]\tLoss: 89.574944\tBCE:72.0992\tKLD:17.4757\tC_loss:0.0000\n",
      "Train Epoch: 31 [46080/60000 (77%)]\tLoss: 92.204041\tBCE:74.6563\tKLD:17.5477\tC_loss:0.0000\n",
      "Train Epoch: 31 [48640/60000 (81%)]\tLoss: 92.075867\tBCE:74.7901\tKLD:17.2858\tC_loss:0.0000\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 93.522324\tBCE:75.8712\tKLD:17.6511\tC_loss:0.0000\n",
      "Train Epoch: 31 [53760/60000 (90%)]\tLoss: 90.471268\tBCE:73.4338\tKLD:17.0375\tC_loss:0.0000\n",
      "Train Epoch: 31 [56320/60000 (94%)]\tLoss: 92.770317\tBCE:75.5008\tKLD:17.2695\tC_loss:0.0000\n",
      "Train Epoch: 31 [58880/60000 (98%)]\tLoss: 89.143372\tBCE:72.1656\tKLD:16.9777\tC_loss:0.0000\n",
      "====> Epoch: 31 Average loss: 90.9409\tClassifier Accuracy: 99.9583\n",
      "====> Test set loss: 91.9581\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 91.373390\tBCE:74.2461\tKLD:17.1273\tC_loss:0.0000\n",
      "Train Epoch: 32 [2560/60000 (4%)]\tLoss: 92.935822\tBCE:75.2835\tKLD:17.6523\tC_loss:0.0000\n",
      "Train Epoch: 32 [5120/60000 (9%)]\tLoss: 91.196716\tBCE:73.7573\tKLD:17.4394\tC_loss:0.0000\n",
      "Train Epoch: 32 [7680/60000 (13%)]\tLoss: 92.487488\tBCE:74.5715\tKLD:17.9160\tC_loss:0.0000\n",
      "Train Epoch: 32 [10240/60000 (17%)]\tLoss: 92.510841\tBCE:74.8413\tKLD:17.6695\tC_loss:0.0000\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 89.766037\tBCE:72.3618\tKLD:17.4042\tC_loss:0.0000\n",
      "Train Epoch: 32 [15360/60000 (26%)]\tLoss: 95.085785\tBCE:77.2896\tKLD:17.7961\tC_loss:0.0000\n",
      "Train Epoch: 32 [17920/60000 (30%)]\tLoss: 92.223831\tBCE:74.9219\tKLD:17.3020\tC_loss:0.0000\n",
      "Train Epoch: 32 [20480/60000 (34%)]\tLoss: 88.444717\tBCE:71.3977\tKLD:17.0470\tC_loss:0.0000\n",
      "Train Epoch: 32 [23040/60000 (38%)]\tLoss: 90.121262\tBCE:72.9304\tKLD:17.1909\tC_loss:0.0000\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 90.988846\tBCE:73.5555\tKLD:17.4334\tC_loss:0.0000\n",
      "Train Epoch: 32 [28160/60000 (47%)]\tLoss: 91.817513\tBCE:73.7805\tKLD:18.0370\tC_loss:0.0000\n",
      "Train Epoch: 32 [30720/60000 (51%)]\tLoss: 89.246857\tBCE:71.7060\tKLD:17.5408\tC_loss:0.0000\n",
      "Train Epoch: 32 [33280/60000 (56%)]\tLoss: 91.653030\tBCE:73.9879\tKLD:17.6651\tC_loss:0.0000\n",
      "Train Epoch: 32 [35840/60000 (60%)]\tLoss: 89.172226\tBCE:72.2294\tKLD:16.9428\tC_loss:0.0000\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 90.973267\tBCE:73.5442\tKLD:17.4290\tC_loss:0.0000\n",
      "Train Epoch: 32 [40960/60000 (68%)]\tLoss: 91.924728\tBCE:73.9433\tKLD:17.9815\tC_loss:0.0000\n",
      "Train Epoch: 32 [43520/60000 (73%)]\tLoss: 90.334946\tBCE:73.3070\tKLD:17.0279\tC_loss:0.0000\n",
      "Train Epoch: 32 [46080/60000 (77%)]\tLoss: 91.494751\tBCE:73.9750\tKLD:17.5197\tC_loss:0.0000\n",
      "Train Epoch: 32 [48640/60000 (81%)]\tLoss: 88.184357\tBCE:71.1097\tKLD:17.0747\tC_loss:0.0000\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 91.627716\tBCE:74.1351\tKLD:17.4926\tC_loss:0.0000\n",
      "Train Epoch: 32 [53760/60000 (90%)]\tLoss: 88.395233\tBCE:71.0605\tKLD:17.3347\tC_loss:0.0000\n",
      "Train Epoch: 32 [56320/60000 (94%)]\tLoss: 92.471329\tBCE:74.8938\tKLD:17.5775\tC_loss:0.0000\n",
      "Train Epoch: 32 [58880/60000 (98%)]\tLoss: 94.421539\tBCE:76.4493\tKLD:17.9722\tC_loss:0.0000\n",
      "====> Epoch: 32 Average loss: 90.7217\tClassifier Accuracy: 99.9616\n",
      "====> Test set loss: 91.4777\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 88.289642\tBCE:70.8319\tKLD:17.4578\tC_loss:0.0000\n",
      "Train Epoch: 33 [2560/60000 (4%)]\tLoss: 91.458954\tBCE:74.0395\tKLD:17.4195\tC_loss:0.0000\n",
      "Train Epoch: 33 [5120/60000 (9%)]\tLoss: 90.475777\tBCE:73.1006\tKLD:17.3752\tC_loss:0.0000\n",
      "Train Epoch: 33 [7680/60000 (13%)]\tLoss: 91.016655\tBCE:73.2087\tKLD:17.8080\tC_loss:0.0000\n",
      "Train Epoch: 33 [10240/60000 (17%)]\tLoss: 93.600174\tBCE:75.7568\tKLD:17.8434\tC_loss:0.0000\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 89.317017\tBCE:71.6020\tKLD:17.7150\tC_loss:0.0000\n",
      "Train Epoch: 33 [15360/60000 (26%)]\tLoss: 91.147461\tBCE:73.3814\tKLD:17.7660\tC_loss:0.0000\n",
      "Train Epoch: 33 [17920/60000 (30%)]\tLoss: 91.434418\tBCE:74.0931\tKLD:17.3413\tC_loss:0.0000\n",
      "Train Epoch: 33 [20480/60000 (34%)]\tLoss: 90.084557\tBCE:73.2822\tKLD:16.8024\tC_loss:0.0000\n",
      "Train Epoch: 33 [23040/60000 (38%)]\tLoss: 89.602524\tBCE:72.5223\tKLD:17.0802\tC_loss:0.0000\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 90.908585\tBCE:73.3713\tKLD:17.5373\tC_loss:0.0000\n",
      "Train Epoch: 33 [28160/60000 (47%)]\tLoss: 91.861885\tBCE:75.0663\tKLD:16.7956\tC_loss:0.0000\n",
      "Train Epoch: 33 [30720/60000 (51%)]\tLoss: 91.002762\tBCE:73.0016\tKLD:18.0012\tC_loss:0.0000\n",
      "Train Epoch: 33 [33280/60000 (56%)]\tLoss: 91.733505\tBCE:74.1678\tKLD:17.5657\tC_loss:0.0000\n",
      "Train Epoch: 33 [35840/60000 (60%)]\tLoss: 89.058609\tBCE:71.9847\tKLD:17.0739\tC_loss:0.0000\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 90.693375\tBCE:73.6230\tKLD:17.0703\tC_loss:0.0000\n",
      "Train Epoch: 33 [40960/60000 (68%)]\tLoss: 90.185028\tBCE:73.0593\tKLD:17.1257\tC_loss:0.0000\n",
      "Train Epoch: 33 [43520/60000 (73%)]\tLoss: 89.761909\tBCE:72.4903\tKLD:17.2716\tC_loss:0.0000\n",
      "Train Epoch: 33 [46080/60000 (77%)]\tLoss: 90.440872\tBCE:73.1952\tKLD:17.2456\tC_loss:0.0000\n",
      "Train Epoch: 33 [48640/60000 (81%)]\tLoss: 88.155342\tBCE:71.0139\tKLD:17.1414\tC_loss:0.0000\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 90.941132\tBCE:73.5543\tKLD:17.3868\tC_loss:0.0000\n",
      "Train Epoch: 33 [53760/60000 (90%)]\tLoss: 93.111862\tBCE:75.4389\tKLD:17.6729\tC_loss:0.0000\n",
      "Train Epoch: 33 [56320/60000 (94%)]\tLoss: 92.311554\tBCE:74.7668\tKLD:17.5448\tC_loss:0.0000\n",
      "Train Epoch: 33 [58880/60000 (98%)]\tLoss: 91.653732\tBCE:74.1608\tKLD:17.4930\tC_loss:0.0000\n",
      "====> Epoch: 33 Average loss: 90.6531\tClassifier Accuracy: 99.9633\n",
      "====> Test set loss: 91.5402\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 90.927216\tBCE:73.3906\tKLD:17.5366\tC_loss:0.0000\n",
      "Train Epoch: 34 [2560/60000 (4%)]\tLoss: 89.659760\tBCE:72.3465\tKLD:17.3133\tC_loss:0.0000\n",
      "Train Epoch: 34 [5120/60000 (9%)]\tLoss: 90.233788\tBCE:72.8941\tKLD:17.3397\tC_loss:0.0000\n",
      "Train Epoch: 34 [7680/60000 (13%)]\tLoss: 90.065521\tBCE:72.7350\tKLD:17.3305\tC_loss:0.0000\n",
      "Train Epoch: 34 [10240/60000 (17%)]\tLoss: 89.557999\tBCE:72.3304\tKLD:17.2276\tC_loss:0.0000\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 92.193535\tBCE:74.9507\tKLD:17.2428\tC_loss:0.0000\n",
      "Train Epoch: 34 [15360/60000 (26%)]\tLoss: 90.973061\tBCE:73.9052\tKLD:17.0678\tC_loss:0.0000\n",
      "Train Epoch: 34 [17920/60000 (30%)]\tLoss: 91.395844\tBCE:74.0151\tKLD:17.3807\tC_loss:0.0000\n",
      "Train Epoch: 34 [20480/60000 (34%)]\tLoss: 89.765930\tBCE:72.2755\tKLD:17.4904\tC_loss:0.0000\n",
      "Train Epoch: 34 [23040/60000 (38%)]\tLoss: 91.124588\tBCE:73.3080\tKLD:17.8166\tC_loss:0.0000\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 88.903763\tBCE:72.2020\tKLD:16.7018\tC_loss:0.0000\n",
      "Train Epoch: 34 [28160/60000 (47%)]\tLoss: 91.223907\tBCE:74.1953\tKLD:17.0286\tC_loss:0.0000\n",
      "Train Epoch: 34 [30720/60000 (51%)]\tLoss: 92.383240\tBCE:74.5042\tKLD:17.8791\tC_loss:0.0000\n",
      "Train Epoch: 34 [33280/60000 (56%)]\tLoss: 91.508095\tBCE:73.9552\tKLD:17.5529\tC_loss:0.0000\n",
      "Train Epoch: 34 [35840/60000 (60%)]\tLoss: 91.240173\tBCE:73.9243\tKLD:17.3159\tC_loss:0.0000\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 90.634499\tBCE:73.4196\tKLD:17.2149\tC_loss:0.0000\n",
      "Train Epoch: 34 [40960/60000 (68%)]\tLoss: 90.190842\tBCE:72.9771\tKLD:17.2137\tC_loss:0.0000\n",
      "Train Epoch: 34 [43520/60000 (73%)]\tLoss: 90.422531\tBCE:73.1637\tKLD:17.2589\tC_loss:0.0000\n",
      "Train Epoch: 34 [46080/60000 (77%)]\tLoss: 87.313324\tBCE:70.0374\tKLD:17.2759\tC_loss:0.0000\n",
      "Train Epoch: 34 [48640/60000 (81%)]\tLoss: 90.563583\tBCE:73.2892\tKLD:17.2744\tC_loss:0.0000\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 91.642860\tBCE:74.1912\tKLD:17.4517\tC_loss:0.0000\n",
      "Train Epoch: 34 [53760/60000 (90%)]\tLoss: 91.593643\tBCE:74.0711\tKLD:17.5225\tC_loss:0.0000\n",
      "Train Epoch: 34 [56320/60000 (94%)]\tLoss: 92.061050\tBCE:74.8627\tKLD:17.1983\tC_loss:0.0000\n",
      "Train Epoch: 34 [58880/60000 (98%)]\tLoss: 90.684387\tBCE:73.2507\tKLD:17.4337\tC_loss:0.0000\n",
      "====> Epoch: 34 Average loss: 90.5993\tClassifier Accuracy: 99.9783\n",
      "====> Test set loss: 91.4935\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 90.731110\tBCE:73.5728\tKLD:17.1583\tC_loss:0.0000\n",
      "Train Epoch: 35 [2560/60000 (4%)]\tLoss: 89.735138\tBCE:72.4067\tKLD:17.3285\tC_loss:0.0000\n",
      "Train Epoch: 35 [5120/60000 (9%)]\tLoss: 90.160309\tBCE:72.4830\tKLD:17.6773\tC_loss:0.0000\n",
      "Train Epoch: 35 [7680/60000 (13%)]\tLoss: 89.000885\tBCE:72.3659\tKLD:16.6350\tC_loss:0.0000\n",
      "Train Epoch: 35 [10240/60000 (17%)]\tLoss: 90.389999\tBCE:73.1865\tKLD:17.2035\tC_loss:0.0000\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 91.869781\tBCE:74.2408\tKLD:17.6289\tC_loss:0.0000\n",
      "Train Epoch: 35 [15360/60000 (26%)]\tLoss: 89.949181\tBCE:72.5097\tKLD:17.4395\tC_loss:0.0000\n",
      "Train Epoch: 35 [17920/60000 (30%)]\tLoss: 90.909790\tBCE:73.5724\tKLD:17.3374\tC_loss:0.0000\n",
      "Train Epoch: 35 [20480/60000 (34%)]\tLoss: 90.477402\tBCE:73.1647\tKLD:17.3127\tC_loss:0.0000\n",
      "Train Epoch: 35 [23040/60000 (38%)]\tLoss: 87.621696\tBCE:70.3230\tKLD:17.2987\tC_loss:0.0000\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 90.225708\tBCE:72.9208\tKLD:17.3049\tC_loss:0.0000\n",
      "Train Epoch: 35 [28160/60000 (47%)]\tLoss: 89.833931\tBCE:72.7228\tKLD:17.1111\tC_loss:0.0000\n",
      "Train Epoch: 35 [30720/60000 (51%)]\tLoss: 92.918236\tBCE:75.3641\tKLD:17.5542\tC_loss:0.0000\n",
      "Train Epoch: 35 [33280/60000 (56%)]\tLoss: 89.142525\tBCE:71.7767\tKLD:17.3658\tC_loss:0.0000\n",
      "Train Epoch: 35 [35840/60000 (60%)]\tLoss: 88.911545\tBCE:71.3865\tKLD:17.5250\tC_loss:0.0000\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 90.033485\tBCE:72.3989\tKLD:17.6346\tC_loss:0.0000\n",
      "Train Epoch: 35 [40960/60000 (68%)]\tLoss: 92.164757\tBCE:74.4235\tKLD:17.7413\tC_loss:0.0000\n",
      "Train Epoch: 35 [43520/60000 (73%)]\tLoss: 87.655670\tBCE:70.2806\tKLD:17.3751\tC_loss:0.0000\n",
      "Train Epoch: 35 [46080/60000 (77%)]\tLoss: 89.156197\tBCE:71.8542\tKLD:17.3020\tC_loss:0.0000\n",
      "Train Epoch: 35 [48640/60000 (81%)]\tLoss: 90.597984\tBCE:73.3896\tKLD:17.2083\tC_loss:0.0000\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 88.632172\tBCE:71.4854\tKLD:17.1468\tC_loss:0.0000\n",
      "Train Epoch: 35 [53760/60000 (90%)]\tLoss: 90.411713\tBCE:73.5458\tKLD:16.8659\tC_loss:0.0000\n",
      "Train Epoch: 35 [56320/60000 (94%)]\tLoss: 91.181168\tBCE:73.2149\tKLD:17.9663\tC_loss:0.0000\n",
      "Train Epoch: 35 [58880/60000 (98%)]\tLoss: 88.466858\tBCE:71.2600\tKLD:17.2068\tC_loss:0.0000\n",
      "====> Epoch: 35 Average loss: 90.4183\tClassifier Accuracy: 99.9800\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.5481\n",
      "Random number: 1\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 90.730698\tBCE:72.8475\tKLD:17.8832\tC_loss:0.0000\n",
      "Train Epoch: 36 [2560/60000 (4%)]\tLoss: 90.028954\tBCE:72.7254\tKLD:17.3035\tC_loss:0.0000\n",
      "Train Epoch: 36 [5120/60000 (9%)]\tLoss: 93.569794\tBCE:75.9252\tKLD:17.6446\tC_loss:0.0000\n",
      "Train Epoch: 36 [7680/60000 (13%)]\tLoss: 89.035019\tBCE:71.4846\tKLD:17.5504\tC_loss:0.0000\n",
      "Train Epoch: 36 [10240/60000 (17%)]\tLoss: 90.089661\tBCE:72.8808\tKLD:17.2089\tC_loss:0.0000\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 90.785103\tBCE:73.3140\tKLD:17.4711\tC_loss:0.0000\n",
      "Train Epoch: 36 [15360/60000 (26%)]\tLoss: 92.267853\tBCE:74.0361\tKLD:18.2317\tC_loss:0.0000\n",
      "Train Epoch: 36 [17920/60000 (30%)]\tLoss: 90.504990\tBCE:72.6882\tKLD:17.8168\tC_loss:0.0000\n",
      "Train Epoch: 36 [20480/60000 (34%)]\tLoss: 92.238968\tBCE:74.9955\tKLD:17.2435\tC_loss:0.0000\n",
      "Train Epoch: 36 [23040/60000 (38%)]\tLoss: 88.936584\tBCE:71.4700\tKLD:17.4666\tC_loss:0.0000\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 90.314529\tBCE:72.9892\tKLD:17.3253\tC_loss:0.0000\n",
      "Train Epoch: 36 [28160/60000 (47%)]\tLoss: 89.833206\tBCE:72.6154\tKLD:17.2178\tC_loss:0.0000\n",
      "Train Epoch: 36 [30720/60000 (51%)]\tLoss: 89.919632\tBCE:72.2141\tKLD:17.7056\tC_loss:0.0000\n",
      "Train Epoch: 36 [33280/60000 (56%)]\tLoss: 91.188980\tBCE:73.5968\tKLD:17.5922\tC_loss:0.0000\n",
      "Train Epoch: 36 [35840/60000 (60%)]\tLoss: 88.963303\tBCE:71.9622\tKLD:17.0012\tC_loss:0.0000\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 87.895325\tBCE:71.0414\tKLD:16.8540\tC_loss:0.0000\n",
      "Train Epoch: 36 [40960/60000 (68%)]\tLoss: 92.713219\tBCE:75.4529\tKLD:17.2603\tC_loss:0.0000\n",
      "Train Epoch: 36 [43520/60000 (73%)]\tLoss: 88.084007\tBCE:70.3096\tKLD:17.7744\tC_loss:0.0000\n",
      "Train Epoch: 36 [46080/60000 (77%)]\tLoss: 90.244156\tBCE:73.1170\tKLD:17.1271\tC_loss:0.0000\n",
      "Train Epoch: 36 [48640/60000 (81%)]\tLoss: 90.437378\tBCE:72.8091\tKLD:17.6282\tC_loss:0.0000\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 88.085709\tBCE:71.5971\tKLD:16.4886\tC_loss:0.0000\n",
      "Train Epoch: 36 [53760/60000 (90%)]\tLoss: 89.330818\tBCE:72.3392\tKLD:16.9916\tC_loss:0.0000\n",
      "Train Epoch: 36 [56320/60000 (94%)]\tLoss: 92.347618\tBCE:74.7137\tKLD:17.6339\tC_loss:0.0000\n",
      "Train Epoch: 36 [58880/60000 (98%)]\tLoss: 90.543198\tBCE:73.2980\tKLD:17.2452\tC_loss:0.0000\n",
      "====> Epoch: 36 Average loss: 90.3037\tClassifier Accuracy: 99.9716\n",
      "====> Test set loss: 91.1659\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 89.672089\tBCE:72.4438\tKLD:17.2283\tC_loss:0.0000\n",
      "Train Epoch: 37 [2560/60000 (4%)]\tLoss: 92.765892\tBCE:74.8892\tKLD:17.8767\tC_loss:0.0000\n",
      "Train Epoch: 37 [5120/60000 (9%)]\tLoss: 87.992371\tBCE:71.0941\tKLD:16.8983\tC_loss:0.0000\n",
      "Train Epoch: 37 [7680/60000 (13%)]\tLoss: 89.791183\tBCE:72.2486\tKLD:17.5426\tC_loss:0.0000\n",
      "Train Epoch: 37 [10240/60000 (17%)]\tLoss: 89.997902\tBCE:72.7758\tKLD:17.2221\tC_loss:0.0000\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 91.500687\tBCE:74.4160\tKLD:17.0847\tC_loss:0.0000\n",
      "Train Epoch: 37 [15360/60000 (26%)]\tLoss: 88.310295\tBCE:70.8914\tKLD:17.4189\tC_loss:0.0000\n",
      "Train Epoch: 37 [17920/60000 (30%)]\tLoss: 90.159264\tBCE:72.7803\tKLD:17.3790\tC_loss:0.0000\n",
      "Train Epoch: 37 [20480/60000 (34%)]\tLoss: 89.764481\tBCE:72.6527\tKLD:17.1118\tC_loss:0.0000\n",
      "Train Epoch: 37 [23040/60000 (38%)]\tLoss: 92.190552\tBCE:74.2297\tKLD:17.9609\tC_loss:0.0000\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 89.165306\tBCE:71.8063\tKLD:17.3590\tC_loss:0.0000\n",
      "Train Epoch: 37 [28160/60000 (47%)]\tLoss: 89.056488\tBCE:71.2949\tKLD:17.7616\tC_loss:0.0000\n",
      "Train Epoch: 37 [30720/60000 (51%)]\tLoss: 92.597603\tBCE:74.9603\tKLD:17.6373\tC_loss:0.0000\n",
      "Train Epoch: 37 [33280/60000 (56%)]\tLoss: 92.732994\tBCE:75.0553\tKLD:17.6777\tC_loss:0.0000\n",
      "Train Epoch: 37 [35840/60000 (60%)]\tLoss: 87.408051\tBCE:70.2081\tKLD:17.2000\tC_loss:0.0000\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 92.760094\tBCE:75.3673\tKLD:17.3928\tC_loss:0.0000\n",
      "Train Epoch: 37 [40960/60000 (68%)]\tLoss: 92.985962\tBCE:75.8437\tKLD:17.1422\tC_loss:0.0000\n",
      "Train Epoch: 37 [43520/60000 (73%)]\tLoss: 89.273972\tBCE:72.0181\tKLD:17.2558\tC_loss:0.0000\n",
      "Train Epoch: 37 [46080/60000 (77%)]\tLoss: 89.560837\tBCE:72.7422\tKLD:16.8186\tC_loss:0.0000\n",
      "Train Epoch: 37 [48640/60000 (81%)]\tLoss: 91.038010\tBCE:73.4679\tKLD:17.5701\tC_loss:0.0000\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 91.869339\tBCE:74.3360\tKLD:17.5333\tC_loss:0.0000\n",
      "Train Epoch: 37 [53760/60000 (90%)]\tLoss: 91.768456\tBCE:73.8227\tKLD:17.9458\tC_loss:0.0000\n",
      "Train Epoch: 37 [56320/60000 (94%)]\tLoss: 90.073265\tBCE:72.5322\tKLD:17.5411\tC_loss:0.0000\n",
      "Train Epoch: 37 [58880/60000 (98%)]\tLoss: 92.789398\tBCE:75.2934\tKLD:17.4960\tC_loss:0.0000\n",
      "====> Epoch: 37 Average loss: 90.2728\tClassifier Accuracy: 99.9850\n",
      "====> Test set loss: 92.1845\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 91.151100\tBCE:73.7691\tKLD:17.3820\tC_loss:0.0000\n",
      "Train Epoch: 38 [2560/60000 (4%)]\tLoss: 92.546707\tBCE:74.6396\tKLD:17.9071\tC_loss:0.0000\n",
      "Train Epoch: 38 [5120/60000 (9%)]\tLoss: 92.935333\tBCE:75.3686\tKLD:17.5667\tC_loss:0.0000\n",
      "Train Epoch: 38 [7680/60000 (13%)]\tLoss: 90.120392\tBCE:72.5620\tKLD:17.5584\tC_loss:0.0000\n",
      "Train Epoch: 38 [10240/60000 (17%)]\tLoss: 90.173279\tBCE:72.3350\tKLD:17.8383\tC_loss:0.0000\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 92.351028\tBCE:74.7080\tKLD:17.6430\tC_loss:0.0000\n",
      "Train Epoch: 38 [15360/60000 (26%)]\tLoss: 92.296555\tBCE:74.3981\tKLD:17.8985\tC_loss:0.0000\n",
      "Train Epoch: 38 [17920/60000 (30%)]\tLoss: 91.152679\tBCE:73.0771\tKLD:18.0756\tC_loss:0.0000\n",
      "Train Epoch: 38 [20480/60000 (34%)]\tLoss: 90.321693\tBCE:72.6368\tKLD:17.6849\tC_loss:0.0000\n",
      "Train Epoch: 38 [23040/60000 (38%)]\tLoss: 90.811798\tBCE:73.5388\tKLD:17.2730\tC_loss:0.0000\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 90.588882\tBCE:73.5677\tKLD:17.0211\tC_loss:0.0000\n",
      "Train Epoch: 38 [28160/60000 (47%)]\tLoss: 90.130905\tBCE:72.3343\tKLD:17.7966\tC_loss:0.0000\n",
      "Train Epoch: 38 [30720/60000 (51%)]\tLoss: 88.498184\tBCE:71.3346\tKLD:17.1636\tC_loss:0.0000\n",
      "Train Epoch: 38 [33280/60000 (56%)]\tLoss: 93.692474\tBCE:75.8757\tKLD:17.8167\tC_loss:0.0000\n",
      "Train Epoch: 38 [35840/60000 (60%)]\tLoss: 90.771301\tBCE:73.3209\tKLD:17.4504\tC_loss:0.0000\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 89.921234\tBCE:72.2710\tKLD:17.6503\tC_loss:0.0000\n",
      "Train Epoch: 38 [40960/60000 (68%)]\tLoss: 91.066956\tBCE:73.5088\tKLD:17.5582\tC_loss:0.0000\n",
      "Train Epoch: 38 [43520/60000 (73%)]\tLoss: 93.911308\tBCE:76.6920\tKLD:17.2194\tC_loss:0.0000\n",
      "Train Epoch: 38 [46080/60000 (77%)]\tLoss: 90.028145\tBCE:72.6260\tKLD:17.4021\tC_loss:0.0000\n",
      "Train Epoch: 38 [48640/60000 (81%)]\tLoss: 89.325195\tBCE:72.2682\tKLD:17.0570\tC_loss:0.0000\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 87.569504\tBCE:70.7849\tKLD:16.7846\tC_loss:0.0000\n",
      "Train Epoch: 38 [53760/60000 (90%)]\tLoss: 91.672470\tBCE:73.6101\tKLD:18.0624\tC_loss:0.0000\n",
      "Train Epoch: 38 [56320/60000 (94%)]\tLoss: 87.558319\tBCE:70.4280\tKLD:17.1303\tC_loss:0.0000\n",
      "Train Epoch: 38 [58880/60000 (98%)]\tLoss: 91.003830\tBCE:74.1063\tKLD:16.8975\tC_loss:0.0000\n",
      "====> Epoch: 38 Average loss: 90.2636\tClassifier Accuracy: 99.9783\n",
      "====> Test set loss: 91.3526\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 88.855789\tBCE:71.5513\tKLD:17.3045\tC_loss:0.0000\n",
      "Train Epoch: 39 [2560/60000 (4%)]\tLoss: 92.176529\tBCE:73.9357\tKLD:18.2408\tC_loss:0.0000\n",
      "Train Epoch: 39 [5120/60000 (9%)]\tLoss: 90.502701\tBCE:72.8423\tKLD:17.6604\tC_loss:0.0000\n",
      "Train Epoch: 39 [7680/60000 (13%)]\tLoss: 89.975601\tBCE:72.9668\tKLD:17.0088\tC_loss:0.0000\n",
      "Train Epoch: 39 [10240/60000 (17%)]\tLoss: 89.402702\tBCE:72.2502\tKLD:17.1525\tC_loss:0.0000\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 87.003906\tBCE:69.6809\tKLD:17.3230\tC_loss:0.0000\n",
      "Train Epoch: 39 [15360/60000 (26%)]\tLoss: 90.267418\tBCE:72.5608\tKLD:17.7066\tC_loss:0.0000\n",
      "Train Epoch: 39 [17920/60000 (30%)]\tLoss: 90.841042\tBCE:73.1447\tKLD:17.6963\tC_loss:0.0000\n",
      "Train Epoch: 39 [20480/60000 (34%)]\tLoss: 91.583725\tBCE:73.9500\tKLD:17.6337\tC_loss:0.0000\n",
      "Train Epoch: 39 [23040/60000 (38%)]\tLoss: 90.794205\tBCE:73.3360\tKLD:17.4582\tC_loss:0.0000\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 88.875038\tBCE:72.0233\tKLD:16.8517\tC_loss:0.0000\n",
      "Train Epoch: 39 [28160/60000 (47%)]\tLoss: 92.338188\tBCE:74.6122\tKLD:17.7260\tC_loss:0.0000\n",
      "Train Epoch: 39 [30720/60000 (51%)]\tLoss: 90.475601\tBCE:72.8156\tKLD:17.6600\tC_loss:0.0000\n",
      "Train Epoch: 39 [33280/60000 (56%)]\tLoss: 90.991104\tBCE:73.7657\tKLD:17.2254\tC_loss:0.0000\n",
      "Train Epoch: 39 [35840/60000 (60%)]\tLoss: 89.136459\tBCE:71.7017\tKLD:17.4348\tC_loss:0.0000\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 89.872749\tBCE:71.9834\tKLD:17.8894\tC_loss:0.0000\n",
      "Train Epoch: 39 [40960/60000 (68%)]\tLoss: 90.201523\tBCE:72.8573\tKLD:17.3442\tC_loss:0.0000\n",
      "Train Epoch: 39 [43520/60000 (73%)]\tLoss: 91.888504\tBCE:74.1878\tKLD:17.7007\tC_loss:0.0000\n",
      "Train Epoch: 39 [46080/60000 (77%)]\tLoss: 90.262939\tBCE:72.8933\tKLD:17.3696\tC_loss:0.0000\n",
      "Train Epoch: 39 [48640/60000 (81%)]\tLoss: 90.004311\tBCE:72.7629\tKLD:17.2414\tC_loss:0.0000\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 90.908478\tBCE:73.9262\tKLD:16.9823\tC_loss:0.0000\n",
      "Train Epoch: 39 [53760/60000 (90%)]\tLoss: 93.140152\tBCE:75.7866\tKLD:17.3536\tC_loss:0.0000\n",
      "Train Epoch: 39 [56320/60000 (94%)]\tLoss: 88.934906\tBCE:71.1368\tKLD:17.7980\tC_loss:0.0000\n",
      "Train Epoch: 39 [58880/60000 (98%)]\tLoss: 89.669647\tBCE:72.3350\tKLD:17.3347\tC_loss:0.0000\n",
      "====> Epoch: 39 Average loss: 90.1274\tClassifier Accuracy: 99.9583\n",
      "====> Test set loss: 91.5041\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 89.996170\tBCE:72.3757\tKLD:17.6205\tC_loss:0.0000\n",
      "Train Epoch: 40 [2560/60000 (4%)]\tLoss: 91.907028\tBCE:74.7003\tKLD:17.2067\tC_loss:0.0000\n",
      "Train Epoch: 40 [5120/60000 (9%)]\tLoss: 87.577263\tBCE:70.1019\tKLD:17.4754\tC_loss:0.0000\n",
      "Train Epoch: 40 [7680/60000 (13%)]\tLoss: 90.723732\tBCE:72.7269\tKLD:17.9968\tC_loss:0.0000\n",
      "Train Epoch: 40 [10240/60000 (17%)]\tLoss: 88.886612\tBCE:71.3837\tKLD:17.5029\tC_loss:0.0000\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 90.167313\tBCE:72.8515\tKLD:17.3158\tC_loss:0.0000\n",
      "Train Epoch: 40 [15360/60000 (26%)]\tLoss: 90.887177\tBCE:72.8683\tKLD:18.0189\tC_loss:0.0000\n",
      "Train Epoch: 40 [17920/60000 (30%)]\tLoss: 91.006966\tBCE:73.3810\tKLD:17.6260\tC_loss:0.0000\n",
      "Train Epoch: 40 [20480/60000 (34%)]\tLoss: 88.689278\tBCE:71.5465\tKLD:17.1428\tC_loss:0.0000\n",
      "Train Epoch: 40 [23040/60000 (38%)]\tLoss: 91.255165\tBCE:73.3974\tKLD:17.8578\tC_loss:0.0000\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 89.858170\tBCE:72.5723\tKLD:17.2858\tC_loss:0.0000\n",
      "Train Epoch: 40 [28160/60000 (47%)]\tLoss: 91.568962\tBCE:73.8493\tKLD:17.7197\tC_loss:0.0000\n",
      "Train Epoch: 40 [30720/60000 (51%)]\tLoss: 91.030052\tBCE:73.6170\tKLD:17.4130\tC_loss:0.0000\n",
      "Train Epoch: 40 [33280/60000 (56%)]\tLoss: 90.691223\tBCE:72.9434\tKLD:17.7478\tC_loss:0.0000\n",
      "Train Epoch: 40 [35840/60000 (60%)]\tLoss: 91.129730\tBCE:73.1764\tKLD:17.9533\tC_loss:0.0000\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 92.240067\tBCE:74.7217\tKLD:17.5184\tC_loss:0.0000\n",
      "Train Epoch: 40 [40960/60000 (68%)]\tLoss: 89.577255\tBCE:71.9840\tKLD:17.5933\tC_loss:0.0000\n",
      "Train Epoch: 40 [43520/60000 (73%)]\tLoss: 89.749161\tBCE:72.2377\tKLD:17.5114\tC_loss:0.0000\n",
      "Train Epoch: 40 [46080/60000 (77%)]\tLoss: 86.921280\tBCE:69.7927\tKLD:17.1286\tC_loss:0.0000\n",
      "Train Epoch: 40 [48640/60000 (81%)]\tLoss: 90.395309\tBCE:72.7859\tKLD:17.6094\tC_loss:0.0000\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 86.730774\tBCE:70.1274\tKLD:16.6033\tC_loss:0.0000\n",
      "Train Epoch: 40 [53760/60000 (90%)]\tLoss: 90.793159\tBCE:73.5999\tKLD:17.1932\tC_loss:0.0000\n",
      "Train Epoch: 40 [56320/60000 (94%)]\tLoss: 88.779594\tBCE:71.7193\tKLD:17.0603\tC_loss:0.0000\n",
      "Train Epoch: 40 [58880/60000 (98%)]\tLoss: 89.364792\tBCE:72.0040\tKLD:17.3608\tC_loss:0.0000\n",
      "====> Epoch: 40 Average loss: 89.9527\tClassifier Accuracy: 99.9199\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.3232\n",
      "Random number: 0\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 90.361176\tBCE:72.8214\tKLD:17.5397\tC_loss:0.0000\n",
      "Train Epoch: 41 [2560/60000 (4%)]\tLoss: 92.063004\tBCE:74.3560\tKLD:17.7070\tC_loss:0.0000\n",
      "Train Epoch: 41 [5120/60000 (9%)]\tLoss: 91.012650\tBCE:73.4765\tKLD:17.5361\tC_loss:0.0000\n",
      "Train Epoch: 41 [7680/60000 (13%)]\tLoss: 93.085342\tBCE:75.2792\tKLD:17.8061\tC_loss:0.0000\n",
      "Train Epoch: 41 [10240/60000 (17%)]\tLoss: 92.529785\tBCE:74.8664\tKLD:17.6634\tC_loss:0.0000\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 89.839798\tBCE:72.3607\tKLD:17.4791\tC_loss:0.0000\n",
      "Train Epoch: 41 [15360/60000 (26%)]\tLoss: 89.547455\tBCE:72.6940\tKLD:16.8535\tC_loss:0.0000\n",
      "Train Epoch: 41 [17920/60000 (30%)]\tLoss: 94.232704\tBCE:75.7602\tKLD:18.4725\tC_loss:0.0000\n",
      "Train Epoch: 41 [20480/60000 (34%)]\tLoss: 87.216263\tBCE:70.5535\tKLD:16.6628\tC_loss:0.0000\n",
      "Train Epoch: 41 [23040/60000 (38%)]\tLoss: 91.026886\tBCE:73.2225\tKLD:17.8044\tC_loss:0.0000\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 90.648781\tBCE:73.7482\tKLD:16.9006\tC_loss:0.0000\n",
      "Train Epoch: 41 [28160/60000 (47%)]\tLoss: 88.247543\tBCE:70.7070\tKLD:17.5405\tC_loss:0.0000\n",
      "Train Epoch: 41 [30720/60000 (51%)]\tLoss: 90.241615\tBCE:72.6379\tKLD:17.6037\tC_loss:0.0000\n",
      "Train Epoch: 41 [33280/60000 (56%)]\tLoss: 90.605103\tBCE:73.1614\tKLD:17.4437\tC_loss:0.0000\n",
      "Train Epoch: 41 [35840/60000 (60%)]\tLoss: 89.809982\tBCE:71.9592\tKLD:17.8507\tC_loss:0.0000\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 89.851669\tBCE:72.6914\tKLD:17.1603\tC_loss:0.0000\n",
      "Train Epoch: 41 [40960/60000 (68%)]\tLoss: 87.941574\tBCE:70.7488\tKLD:17.1928\tC_loss:0.0000\n",
      "Train Epoch: 41 [43520/60000 (73%)]\tLoss: 87.754822\tBCE:70.7523\tKLD:17.0025\tC_loss:0.0000\n",
      "Train Epoch: 41 [46080/60000 (77%)]\tLoss: 89.886856\tBCE:72.5774\tKLD:17.3094\tC_loss:0.0000\n",
      "Train Epoch: 41 [48640/60000 (81%)]\tLoss: 91.148872\tBCE:73.5089\tKLD:17.6400\tC_loss:0.0000\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 89.956314\tBCE:72.5720\tKLD:17.3843\tC_loss:0.0000\n",
      "Train Epoch: 41 [53760/60000 (90%)]\tLoss: 91.594543\tBCE:73.9330\tKLD:17.6616\tC_loss:0.0000\n",
      "Train Epoch: 41 [56320/60000 (94%)]\tLoss: 88.317535\tBCE:70.8613\tKLD:17.4563\tC_loss:0.0000\n",
      "Train Epoch: 41 [58880/60000 (98%)]\tLoss: 92.302834\tBCE:74.6399\tKLD:17.6629\tC_loss:0.0000\n",
      "====> Epoch: 41 Average loss: 89.8997\tClassifier Accuracy: 99.9332\n",
      "====> Test set loss: 90.9024\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 90.190201\tBCE:72.7467\tKLD:17.4435\tC_loss:0.0000\n",
      "Train Epoch: 42 [2560/60000 (4%)]\tLoss: 88.865013\tBCE:71.4235\tKLD:17.4416\tC_loss:0.0000\n",
      "Train Epoch: 42 [5120/60000 (9%)]\tLoss: 91.454254\tBCE:74.0950\tKLD:17.3592\tC_loss:0.0000\n",
      "Train Epoch: 42 [7680/60000 (13%)]\tLoss: 92.181885\tBCE:74.7451\tKLD:17.4368\tC_loss:0.0000\n",
      "Train Epoch: 42 [10240/60000 (17%)]\tLoss: 91.772339\tBCE:74.1048\tKLD:17.6675\tC_loss:0.0000\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 91.472054\tBCE:73.7506\tKLD:17.7215\tC_loss:0.0000\n",
      "Train Epoch: 42 [15360/60000 (26%)]\tLoss: 91.097008\tBCE:73.6045\tKLD:17.4926\tC_loss:0.0000\n",
      "Train Epoch: 42 [17920/60000 (30%)]\tLoss: 91.871643\tBCE:74.2791\tKLD:17.5925\tC_loss:0.0000\n",
      "Train Epoch: 42 [20480/60000 (34%)]\tLoss: 91.920403\tBCE:74.0152\tKLD:17.9052\tC_loss:0.0000\n",
      "Train Epoch: 42 [23040/60000 (38%)]\tLoss: 92.141418\tBCE:74.6183\tKLD:17.5232\tC_loss:0.0000\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 90.959824\tBCE:73.1785\tKLD:17.7814\tC_loss:0.0000\n",
      "Train Epoch: 42 [28160/60000 (47%)]\tLoss: 89.076424\tBCE:71.5491\tKLD:17.5273\tC_loss:0.0000\n",
      "Train Epoch: 42 [30720/60000 (51%)]\tLoss: 91.913841\tBCE:74.6251\tKLD:17.2888\tC_loss:0.0000\n",
      "Train Epoch: 42 [33280/60000 (56%)]\tLoss: 86.934380\tBCE:69.9347\tKLD:16.9997\tC_loss:0.0000\n",
      "Train Epoch: 42 [35840/60000 (60%)]\tLoss: 88.101624\tBCE:70.3757\tKLD:17.7259\tC_loss:0.0000\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 90.448212\tBCE:72.5388\tKLD:17.9094\tC_loss:0.0000\n",
      "Train Epoch: 42 [40960/60000 (68%)]\tLoss: 91.935295\tBCE:74.5149\tKLD:17.4204\tC_loss:0.0000\n",
      "Train Epoch: 42 [43520/60000 (73%)]\tLoss: 86.688118\tBCE:69.8045\tKLD:16.8836\tC_loss:0.0000\n",
      "Train Epoch: 42 [46080/60000 (77%)]\tLoss: 90.184547\tBCE:72.7566\tKLD:17.4279\tC_loss:0.0000\n",
      "Train Epoch: 42 [48640/60000 (81%)]\tLoss: 88.089691\tBCE:70.9486\tKLD:17.1411\tC_loss:0.0000\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 91.147720\tBCE:73.1095\tKLD:18.0382\tC_loss:0.0000\n",
      "Train Epoch: 42 [53760/60000 (90%)]\tLoss: 89.802711\tBCE:72.2153\tKLD:17.5874\tC_loss:0.0000\n",
      "Train Epoch: 42 [56320/60000 (94%)]\tLoss: 89.284363\tBCE:72.1581\tKLD:17.1263\tC_loss:0.0000\n",
      "Train Epoch: 42 [58880/60000 (98%)]\tLoss: 90.418770\tBCE:72.7609\tKLD:17.6579\tC_loss:0.0000\n",
      "====> Epoch: 42 Average loss: 89.9424\tClassifier Accuracy: 99.9983\n",
      "====> Test set loss: 91.2795\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 89.550964\tBCE:72.2138\tKLD:17.3372\tC_loss:0.0000\n",
      "Train Epoch: 43 [2560/60000 (4%)]\tLoss: 88.752892\tBCE:71.3550\tKLD:17.3978\tC_loss:0.0000\n",
      "Train Epoch: 43 [5120/60000 (9%)]\tLoss: 89.573608\tBCE:71.9882\tKLD:17.5854\tC_loss:0.0000\n",
      "Train Epoch: 43 [7680/60000 (13%)]\tLoss: 87.295456\tBCE:69.9861\tKLD:17.3094\tC_loss:0.0000\n",
      "Train Epoch: 43 [10240/60000 (17%)]\tLoss: 87.569519\tBCE:69.8873\tKLD:17.6822\tC_loss:0.0000\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 89.051895\tBCE:71.9483\tKLD:17.1036\tC_loss:0.0000\n",
      "Train Epoch: 43 [15360/60000 (26%)]\tLoss: 89.605484\tBCE:72.4707\tKLD:17.1348\tC_loss:0.0000\n",
      "Train Epoch: 43 [17920/60000 (30%)]\tLoss: 93.467323\tBCE:76.1727\tKLD:17.2947\tC_loss:0.0000\n",
      "Train Epoch: 43 [20480/60000 (34%)]\tLoss: 86.893921\tBCE:69.6709\tKLD:17.2230\tC_loss:0.0000\n",
      "Train Epoch: 43 [23040/60000 (38%)]\tLoss: 86.765144\tBCE:69.9073\tKLD:16.8578\tC_loss:0.0000\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 87.843613\tBCE:70.8629\tKLD:16.9807\tC_loss:0.0000\n",
      "Train Epoch: 43 [28160/60000 (47%)]\tLoss: 89.141556\tBCE:71.6600\tKLD:17.4815\tC_loss:0.0000\n",
      "Train Epoch: 43 [30720/60000 (51%)]\tLoss: 90.096428\tBCE:72.5559\tKLD:17.5405\tC_loss:0.0000\n",
      "Train Epoch: 43 [33280/60000 (56%)]\tLoss: 90.785027\tBCE:73.5062\tKLD:17.2788\tC_loss:0.0000\n",
      "Train Epoch: 43 [35840/60000 (60%)]\tLoss: 91.228264\tBCE:73.5172\tKLD:17.7110\tC_loss:0.0000\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 91.892677\tBCE:74.2383\tKLD:17.6543\tC_loss:0.0000\n",
      "Train Epoch: 43 [40960/60000 (68%)]\tLoss: 92.296234\tBCE:74.9351\tKLD:17.3611\tC_loss:0.0000\n",
      "Train Epoch: 43 [43520/60000 (73%)]\tLoss: 89.202927\tBCE:71.7664\tKLD:17.4365\tC_loss:0.0000\n",
      "Train Epoch: 43 [46080/60000 (77%)]\tLoss: 87.237358\tBCE:69.9793\tKLD:17.2580\tC_loss:0.0000\n",
      "Train Epoch: 43 [48640/60000 (81%)]\tLoss: 87.988754\tBCE:70.5380\tKLD:17.4508\tC_loss:0.0000\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 92.507309\tBCE:75.0863\tKLD:17.4210\tC_loss:0.0000\n",
      "Train Epoch: 43 [53760/60000 (90%)]\tLoss: 90.741135\tBCE:72.9008\tKLD:17.8403\tC_loss:0.0000\n",
      "Train Epoch: 43 [56320/60000 (94%)]\tLoss: 89.480125\tBCE:72.2092\tKLD:17.2709\tC_loss:0.0000\n",
      "Train Epoch: 43 [58880/60000 (98%)]\tLoss: 92.213623\tBCE:74.7311\tKLD:17.4825\tC_loss:0.0000\n",
      "====> Epoch: 43 Average loss: 89.7743\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.3904\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 93.467789\tBCE:75.8049\tKLD:17.6629\tC_loss:0.0000\n",
      "Train Epoch: 44 [2560/60000 (4%)]\tLoss: 95.019829\tBCE:77.2117\tKLD:17.8081\tC_loss:0.0000\n",
      "Train Epoch: 44 [5120/60000 (9%)]\tLoss: 90.403206\tBCE:73.1812\tKLD:17.2220\tC_loss:0.0000\n",
      "Train Epoch: 44 [7680/60000 (13%)]\tLoss: 89.950249\tBCE:72.2761\tKLD:17.6742\tC_loss:0.0000\n",
      "Train Epoch: 44 [10240/60000 (17%)]\tLoss: 91.863861\tBCE:74.2560\tKLD:17.6079\tC_loss:0.0000\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 89.315109\tBCE:72.1759\tKLD:17.1392\tC_loss:0.0000\n",
      "Train Epoch: 44 [15360/60000 (26%)]\tLoss: 88.950623\tBCE:71.4290\tKLD:17.5216\tC_loss:0.0000\n",
      "Train Epoch: 44 [17920/60000 (30%)]\tLoss: 89.066696\tBCE:71.8619\tKLD:17.2048\tC_loss:0.0000\n",
      "Train Epoch: 44 [20480/60000 (34%)]\tLoss: 87.408150\tBCE:70.0887\tKLD:17.3195\tC_loss:0.0000\n",
      "Train Epoch: 44 [23040/60000 (38%)]\tLoss: 86.169899\tBCE:69.0477\tKLD:17.1222\tC_loss:0.0000\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 91.357269\tBCE:73.7816\tKLD:17.5757\tC_loss:0.0000\n",
      "Train Epoch: 44 [28160/60000 (47%)]\tLoss: 90.346916\tBCE:72.9586\tKLD:17.3884\tC_loss:0.0000\n",
      "Train Epoch: 44 [30720/60000 (51%)]\tLoss: 88.824142\tBCE:71.1929\tKLD:17.6313\tC_loss:0.0000\n",
      "Train Epoch: 44 [33280/60000 (56%)]\tLoss: 90.297295\tBCE:72.7619\tKLD:17.5354\tC_loss:0.0000\n",
      "Train Epoch: 44 [35840/60000 (60%)]\tLoss: 88.162766\tBCE:71.2228\tKLD:16.9400\tC_loss:0.0000\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 91.007843\tBCE:72.8347\tKLD:18.1731\tC_loss:0.0000\n",
      "Train Epoch: 44 [40960/60000 (68%)]\tLoss: 91.263802\tBCE:73.6252\tKLD:17.6386\tC_loss:0.0000\n",
      "Train Epoch: 44 [43520/60000 (73%)]\tLoss: 89.090324\tBCE:71.9706\tKLD:17.1197\tC_loss:0.0000\n",
      "Train Epoch: 44 [46080/60000 (77%)]\tLoss: 91.176041\tBCE:74.0311\tKLD:17.1449\tC_loss:0.0000\n",
      "Train Epoch: 44 [48640/60000 (81%)]\tLoss: 89.545929\tBCE:71.9270\tKLD:17.6190\tC_loss:0.0000\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 89.064049\tBCE:72.1264\tKLD:16.9376\tC_loss:0.0000\n",
      "Train Epoch: 44 [53760/60000 (90%)]\tLoss: 88.826317\tBCE:71.1170\tKLD:17.7093\tC_loss:0.0000\n",
      "Train Epoch: 44 [56320/60000 (94%)]\tLoss: 89.756569\tBCE:72.3999\tKLD:17.3567\tC_loss:0.0000\n",
      "Train Epoch: 44 [58880/60000 (98%)]\tLoss: 91.805168\tBCE:73.9877\tKLD:17.8175\tC_loss:0.0000\n",
      "====> Epoch: 44 Average loss: 89.7112\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.8564\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 86.414330\tBCE:69.1678\tKLD:17.2466\tC_loss:0.0000\n",
      "Train Epoch: 45 [2560/60000 (4%)]\tLoss: 90.993683\tBCE:72.9258\tKLD:18.0679\tC_loss:0.0000\n",
      "Train Epoch: 45 [5120/60000 (9%)]\tLoss: 89.239639\tBCE:71.8679\tKLD:17.3717\tC_loss:0.0000\n",
      "Train Epoch: 45 [7680/60000 (13%)]\tLoss: 87.802597\tBCE:71.0082\tKLD:16.7944\tC_loss:0.0000\n",
      "Train Epoch: 45 [10240/60000 (17%)]\tLoss: 91.701584\tBCE:74.2807\tKLD:17.4209\tC_loss:0.0000\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 87.522125\tBCE:70.5963\tKLD:16.9259\tC_loss:0.0000\n",
      "Train Epoch: 45 [15360/60000 (26%)]\tLoss: 88.124741\tBCE:70.7830\tKLD:17.3417\tC_loss:0.0000\n",
      "Train Epoch: 45 [17920/60000 (30%)]\tLoss: 90.564857\tBCE:73.1412\tKLD:17.4237\tC_loss:0.0000\n",
      "Train Epoch: 45 [20480/60000 (34%)]\tLoss: 89.742043\tBCE:72.1554\tKLD:17.5867\tC_loss:0.0000\n",
      "Train Epoch: 45 [23040/60000 (38%)]\tLoss: 87.264778\tBCE:69.9679\tKLD:17.2968\tC_loss:0.0000\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 89.514999\tBCE:71.8334\tKLD:17.6816\tC_loss:0.0000\n",
      "Train Epoch: 45 [28160/60000 (47%)]\tLoss: 92.386238\tBCE:75.3329\tKLD:17.0533\tC_loss:0.0000\n",
      "Train Epoch: 45 [30720/60000 (51%)]\tLoss: 93.266846\tBCE:74.9147\tKLD:18.3521\tC_loss:0.0000\n",
      "Train Epoch: 45 [33280/60000 (56%)]\tLoss: 88.617233\tBCE:71.4861\tKLD:17.1312\tC_loss:0.0000\n",
      "Train Epoch: 45 [35840/60000 (60%)]\tLoss: 88.124069\tBCE:70.8213\tKLD:17.3028\tC_loss:0.0000\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 88.068321\tBCE:70.6871\tKLD:17.3813\tC_loss:0.0000\n",
      "Train Epoch: 45 [40960/60000 (68%)]\tLoss: 89.889297\tBCE:72.7529\tKLD:17.1364\tC_loss:0.0000\n",
      "Train Epoch: 45 [43520/60000 (73%)]\tLoss: 92.464188\tBCE:74.5011\tKLD:17.9631\tC_loss:0.0000\n",
      "Train Epoch: 45 [46080/60000 (77%)]\tLoss: 90.143524\tBCE:72.6537\tKLD:17.4898\tC_loss:0.0000\n",
      "Train Epoch: 45 [48640/60000 (81%)]\tLoss: 93.409988\tBCE:75.0409\tKLD:18.3691\tC_loss:0.0000\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 91.944466\tBCE:74.0928\tKLD:17.8517\tC_loss:0.0000\n",
      "Train Epoch: 45 [53760/60000 (90%)]\tLoss: 92.418045\tBCE:74.9054\tKLD:17.5127\tC_loss:0.0000\n",
      "Train Epoch: 45 [56320/60000 (94%)]\tLoss: 90.787827\tBCE:73.4686\tKLD:17.3192\tC_loss:0.0000\n",
      "Train Epoch: 45 [58880/60000 (98%)]\tLoss: 89.918129\tBCE:72.2215\tKLD:17.6966\tC_loss:0.0000\n",
      "====> Epoch: 45 Average loss: 89.6597\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 91.6645\n",
      "Random number: 9\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 90.204208\tBCE:73.4787\tKLD:16.7255\tC_loss:0.0000\n",
      "Train Epoch: 46 [2560/60000 (4%)]\tLoss: 88.863533\tBCE:71.4368\tKLD:17.4267\tC_loss:0.0000\n",
      "Train Epoch: 46 [5120/60000 (9%)]\tLoss: 89.088699\tBCE:71.8180\tKLD:17.2707\tC_loss:0.0000\n",
      "Train Epoch: 46 [7680/60000 (13%)]\tLoss: 89.727882\tBCE:71.9672\tKLD:17.7607\tC_loss:0.0000\n",
      "Train Epoch: 46 [10240/60000 (17%)]\tLoss: 90.654388\tBCE:73.0223\tKLD:17.6321\tC_loss:0.0000\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 90.938248\tBCE:73.3377\tKLD:17.6006\tC_loss:0.0000\n",
      "Train Epoch: 46 [15360/60000 (26%)]\tLoss: 88.374855\tBCE:70.7359\tKLD:17.6389\tC_loss:0.0000\n",
      "Train Epoch: 46 [17920/60000 (30%)]\tLoss: 85.502464\tBCE:68.9517\tKLD:16.5508\tC_loss:0.0000\n",
      "Train Epoch: 46 [20480/60000 (34%)]\tLoss: 89.376305\tBCE:71.9515\tKLD:17.4248\tC_loss:0.0000\n",
      "Train Epoch: 46 [23040/60000 (38%)]\tLoss: 92.731781\tBCE:74.9958\tKLD:17.7360\tC_loss:0.0000\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 93.289276\tBCE:75.3781\tKLD:17.9112\tC_loss:0.0000\n",
      "Train Epoch: 46 [28160/60000 (47%)]\tLoss: 89.544037\tBCE:71.9736\tKLD:17.5704\tC_loss:0.0000\n",
      "Train Epoch: 46 [30720/60000 (51%)]\tLoss: 89.756638\tBCE:72.2433\tKLD:17.5133\tC_loss:0.0000\n",
      "Train Epoch: 46 [33280/60000 (56%)]\tLoss: 88.575714\tBCE:71.4051\tKLD:17.1707\tC_loss:0.0000\n",
      "Train Epoch: 46 [35840/60000 (60%)]\tLoss: 91.071640\tBCE:73.3181\tKLD:17.7536\tC_loss:0.0000\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 91.310120\tBCE:73.8934\tKLD:17.4167\tC_loss:0.0000\n",
      "Train Epoch: 46 [40960/60000 (68%)]\tLoss: 89.528328\tBCE:72.5267\tKLD:17.0017\tC_loss:0.0000\n",
      "Train Epoch: 46 [43520/60000 (73%)]\tLoss: 89.338959\tBCE:71.6717\tKLD:17.6673\tC_loss:0.0000\n",
      "Train Epoch: 46 [46080/60000 (77%)]\tLoss: 90.041512\tBCE:72.4430\tKLD:17.5985\tC_loss:0.0000\n",
      "Train Epoch: 46 [48640/60000 (81%)]\tLoss: 91.532509\tBCE:74.0059\tKLD:17.5266\tC_loss:0.0000\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 89.534317\tBCE:71.6588\tKLD:17.8755\tC_loss:0.0000\n",
      "Train Epoch: 46 [53760/60000 (90%)]\tLoss: 91.489044\tBCE:73.7424\tKLD:17.7467\tC_loss:0.0000\n",
      "Train Epoch: 46 [56320/60000 (94%)]\tLoss: 87.331642\tBCE:69.7282\tKLD:17.6034\tC_loss:0.0000\n",
      "Train Epoch: 46 [58880/60000 (98%)]\tLoss: 89.693832\tBCE:72.3369\tKLD:17.3570\tC_loss:0.0000\n",
      "====> Epoch: 46 Average loss: 89.5187\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.6854\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 86.400421\tBCE:69.2774\tKLD:17.1230\tC_loss:0.0000\n",
      "Train Epoch: 47 [2560/60000 (4%)]\tLoss: 87.895164\tBCE:70.3117\tKLD:17.5835\tC_loss:0.0000\n",
      "Train Epoch: 47 [5120/60000 (9%)]\tLoss: 91.971680\tBCE:74.0410\tKLD:17.9307\tC_loss:0.0000\n",
      "Train Epoch: 47 [7680/60000 (13%)]\tLoss: 90.935120\tBCE:72.8090\tKLD:18.1262\tC_loss:0.0000\n",
      "Train Epoch: 47 [10240/60000 (17%)]\tLoss: 89.205498\tBCE:71.8822\tKLD:17.3233\tC_loss:0.0000\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 88.970093\tBCE:71.3892\tKLD:17.5809\tC_loss:0.0000\n",
      "Train Epoch: 47 [15360/60000 (26%)]\tLoss: 86.769402\tBCE:70.0027\tKLD:16.7667\tC_loss:0.0000\n",
      "Train Epoch: 47 [17920/60000 (30%)]\tLoss: 89.915497\tBCE:72.3112\tKLD:17.6043\tC_loss:0.0000\n",
      "Train Epoch: 47 [20480/60000 (34%)]\tLoss: 87.227112\tBCE:70.3440\tKLD:16.8831\tC_loss:0.0000\n",
      "Train Epoch: 47 [23040/60000 (38%)]\tLoss: 90.510834\tBCE:72.3006\tKLD:18.2102\tC_loss:0.0000\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 89.822113\tBCE:72.3439\tKLD:17.4782\tC_loss:0.0000\n",
      "Train Epoch: 47 [28160/60000 (47%)]\tLoss: 90.943115\tBCE:73.7138\tKLD:17.2293\tC_loss:0.0000\n",
      "Train Epoch: 47 [30720/60000 (51%)]\tLoss: 88.941200\tBCE:71.5261\tKLD:17.4151\tC_loss:0.0000\n",
      "Train Epoch: 47 [33280/60000 (56%)]\tLoss: 92.114044\tBCE:74.6816\tKLD:17.4325\tC_loss:0.0000\n",
      "Train Epoch: 47 [35840/60000 (60%)]\tLoss: 88.255936\tBCE:70.6615\tKLD:17.5944\tC_loss:0.0000\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 91.030289\tBCE:72.9238\tKLD:18.1065\tC_loss:0.0000\n",
      "Train Epoch: 47 [40960/60000 (68%)]\tLoss: 91.989243\tBCE:73.8305\tKLD:18.1588\tC_loss:0.0000\n",
      "Train Epoch: 47 [43520/60000 (73%)]\tLoss: 87.548820\tBCE:70.3018\tKLD:17.2470\tC_loss:0.0000\n",
      "Train Epoch: 47 [46080/60000 (77%)]\tLoss: 92.033585\tBCE:73.9634\tKLD:18.0702\tC_loss:0.0000\n",
      "Train Epoch: 47 [48640/60000 (81%)]\tLoss: 91.510635\tBCE:72.9772\tKLD:18.5335\tC_loss:0.0000\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 89.734947\tBCE:72.5956\tKLD:17.1393\tC_loss:0.0000\n",
      "Train Epoch: 47 [53760/60000 (90%)]\tLoss: 91.480850\tBCE:73.5992\tKLD:17.8817\tC_loss:0.0000\n",
      "Train Epoch: 47 [56320/60000 (94%)]\tLoss: 90.700348\tBCE:73.0822\tKLD:17.6181\tC_loss:0.0000\n",
      "Train Epoch: 47 [58880/60000 (98%)]\tLoss: 90.186371\tBCE:72.3585\tKLD:17.8279\tC_loss:0.0000\n",
      "====> Epoch: 47 Average loss: 89.4565\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.9102\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 88.810516\tBCE:71.2001\tKLD:17.6105\tC_loss:0.0000\n",
      "Train Epoch: 48 [2560/60000 (4%)]\tLoss: 90.527924\tBCE:72.9632\tKLD:17.5648\tC_loss:0.0000\n",
      "Train Epoch: 48 [5120/60000 (9%)]\tLoss: 89.495514\tBCE:72.2707\tKLD:17.2249\tC_loss:0.0000\n",
      "Train Epoch: 48 [7680/60000 (13%)]\tLoss: 90.157089\tBCE:72.5625\tKLD:17.5946\tC_loss:0.0000\n",
      "Train Epoch: 48 [10240/60000 (17%)]\tLoss: 92.500092\tBCE:74.3569\tKLD:18.1432\tC_loss:0.0000\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 92.568222\tBCE:74.9019\tKLD:17.6663\tC_loss:0.0000\n",
      "Train Epoch: 48 [15360/60000 (26%)]\tLoss: 87.828110\tBCE:70.4454\tKLD:17.3827\tC_loss:0.0000\n",
      "Train Epoch: 48 [17920/60000 (30%)]\tLoss: 90.045631\tBCE:72.1923\tKLD:17.8533\tC_loss:0.0000\n",
      "Train Epoch: 48 [20480/60000 (34%)]\tLoss: 89.229660\tBCE:71.9832\tKLD:17.2465\tC_loss:0.0000\n",
      "Train Epoch: 48 [23040/60000 (38%)]\tLoss: 88.760254\tBCE:71.7629\tKLD:16.9973\tC_loss:0.0000\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 89.894516\tBCE:72.0975\tKLD:17.7970\tC_loss:0.0000\n",
      "Train Epoch: 48 [28160/60000 (47%)]\tLoss: 90.845505\tBCE:73.4460\tKLD:17.3995\tC_loss:0.0000\n",
      "Train Epoch: 48 [30720/60000 (51%)]\tLoss: 88.721344\tBCE:71.3504\tKLD:17.3710\tC_loss:0.0000\n",
      "Train Epoch: 48 [33280/60000 (56%)]\tLoss: 86.586487\tBCE:68.9559\tKLD:17.6306\tC_loss:0.0000\n",
      "Train Epoch: 48 [35840/60000 (60%)]\tLoss: 88.440086\tBCE:71.1127\tKLD:17.3274\tC_loss:0.0000\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 88.250366\tBCE:70.7938\tKLD:17.4565\tC_loss:0.0000\n",
      "Train Epoch: 48 [40960/60000 (68%)]\tLoss: 92.564980\tBCE:74.5698\tKLD:17.9952\tC_loss:0.0000\n",
      "Train Epoch: 48 [43520/60000 (73%)]\tLoss: 89.653786\tBCE:71.6484\tKLD:18.0054\tC_loss:0.0000\n",
      "Train Epoch: 48 [46080/60000 (77%)]\tLoss: 92.498901\tBCE:75.1742\tKLD:17.3247\tC_loss:0.0000\n",
      "Train Epoch: 48 [48640/60000 (81%)]\tLoss: 87.105461\tBCE:69.8672\tKLD:17.2383\tC_loss:0.0000\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 88.368164\tBCE:71.1806\tKLD:17.1875\tC_loss:0.0000\n",
      "Train Epoch: 48 [53760/60000 (90%)]\tLoss: 88.147148\tBCE:71.1182\tKLD:17.0289\tC_loss:0.0000\n",
      "Train Epoch: 48 [56320/60000 (94%)]\tLoss: 88.638931\tBCE:71.4844\tKLD:17.1545\tC_loss:0.0000\n",
      "Train Epoch: 48 [58880/60000 (98%)]\tLoss: 90.090248\tBCE:72.5518\tKLD:17.5384\tC_loss:0.0000\n",
      "====> Epoch: 48 Average loss: 89.3844\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 90.7277\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 89.809654\tBCE:72.2956\tKLD:17.5140\tC_loss:0.0000\n",
      "Train Epoch: 49 [2560/60000 (4%)]\tLoss: 88.999603\tBCE:71.1426\tKLD:17.8570\tC_loss:0.0000\n",
      "Train Epoch: 49 [5120/60000 (9%)]\tLoss: 87.795135\tBCE:70.5756\tKLD:17.2195\tC_loss:0.0000\n",
      "Train Epoch: 49 [7680/60000 (13%)]\tLoss: 88.572723\tBCE:71.1523\tKLD:17.4204\tC_loss:0.0000\n",
      "Train Epoch: 49 [10240/60000 (17%)]\tLoss: 88.631081\tBCE:70.9757\tKLD:17.6554\tC_loss:0.0000\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 89.289330\tBCE:71.8909\tKLD:17.3984\tC_loss:0.0000\n",
      "Train Epoch: 49 [15360/60000 (26%)]\tLoss: 88.900169\tBCE:71.8203\tKLD:17.0799\tC_loss:0.0000\n",
      "Train Epoch: 49 [17920/60000 (30%)]\tLoss: 90.490112\tBCE:72.5796\tKLD:17.9105\tC_loss:0.0000\n",
      "Train Epoch: 49 [20480/60000 (34%)]\tLoss: 87.581711\tBCE:69.9064\tKLD:17.6753\tC_loss:0.0000\n",
      "Train Epoch: 49 [23040/60000 (38%)]\tLoss: 91.846756\tBCE:74.1508\tKLD:17.6960\tC_loss:0.0000\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 90.044662\tBCE:72.4451\tKLD:17.5996\tC_loss:0.0000\n",
      "Train Epoch: 49 [28160/60000 (47%)]\tLoss: 90.239998\tBCE:72.5198\tKLD:17.7202\tC_loss:0.0000\n",
      "Train Epoch: 49 [30720/60000 (51%)]\tLoss: 90.035812\tBCE:72.6784\tKLD:17.3574\tC_loss:0.0000\n",
      "Train Epoch: 49 [33280/60000 (56%)]\tLoss: 88.523987\tBCE:70.7286\tKLD:17.7954\tC_loss:0.0000\n",
      "Train Epoch: 49 [35840/60000 (60%)]\tLoss: 90.287727\tBCE:72.8193\tKLD:17.4684\tC_loss:0.0000\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 92.508972\tBCE:74.3573\tKLD:18.1516\tC_loss:0.0000\n",
      "Train Epoch: 49 [40960/60000 (68%)]\tLoss: 86.839233\tBCE:69.6965\tKLD:17.1427\tC_loss:0.0000\n",
      "Train Epoch: 49 [43520/60000 (73%)]\tLoss: 89.919853\tBCE:72.7862\tKLD:17.1336\tC_loss:0.0000\n",
      "Train Epoch: 49 [46080/60000 (77%)]\tLoss: 89.630440\tBCE:72.0878\tKLD:17.5427\tC_loss:0.0000\n",
      "Train Epoch: 49 [48640/60000 (81%)]\tLoss: 89.348587\tBCE:71.5235\tKLD:17.8251\tC_loss:0.0000\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 90.955673\tBCE:73.4872\tKLD:17.4684\tC_loss:0.0000\n",
      "Train Epoch: 49 [53760/60000 (90%)]\tLoss: 85.950729\tBCE:69.1766\tKLD:16.7741\tC_loss:0.0000\n",
      "Train Epoch: 49 [56320/60000 (94%)]\tLoss: 90.508141\tBCE:72.6572\tKLD:17.8509\tC_loss:0.0000\n",
      "Train Epoch: 49 [58880/60000 (98%)]\tLoss: 88.879623\tBCE:71.4288\tKLD:17.4508\tC_loss:0.0000\n",
      "====> Epoch: 49 Average loss: 89.3839\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 91.0135\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 88.337845\tBCE:70.9493\tKLD:17.3885\tC_loss:0.0000\n",
      "Train Epoch: 50 [2560/60000 (4%)]\tLoss: 90.064621\tBCE:72.6380\tKLD:17.4266\tC_loss:0.0000\n",
      "Train Epoch: 50 [5120/60000 (9%)]\tLoss: 91.108566\tBCE:73.5557\tKLD:17.5528\tC_loss:0.0000\n",
      "Train Epoch: 50 [7680/60000 (13%)]\tLoss: 91.638611\tBCE:73.9311\tKLD:17.7075\tC_loss:0.0000\n",
      "Train Epoch: 50 [10240/60000 (17%)]\tLoss: 89.778824\tBCE:72.0849\tKLD:17.6939\tC_loss:0.0000\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 88.570000\tBCE:71.1808\tKLD:17.3892\tC_loss:0.0000\n",
      "Train Epoch: 50 [15360/60000 (26%)]\tLoss: 91.228882\tBCE:73.1752\tKLD:18.0537\tC_loss:0.0000\n",
      "Train Epoch: 50 [17920/60000 (30%)]\tLoss: 88.939598\tBCE:71.4113\tKLD:17.5283\tC_loss:0.0000\n",
      "Train Epoch: 50 [20480/60000 (34%)]\tLoss: 89.703430\tBCE:72.5253\tKLD:17.1782\tC_loss:0.0000\n",
      "Train Epoch: 50 [23040/60000 (38%)]\tLoss: 90.307159\tBCE:73.0862\tKLD:17.2210\tC_loss:0.0000\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 89.917801\tBCE:73.1319\tKLD:16.7859\tC_loss:0.0000\n",
      "Train Epoch: 50 [28160/60000 (47%)]\tLoss: 91.659592\tBCE:74.0643\tKLD:17.5953\tC_loss:0.0000\n",
      "Train Epoch: 50 [30720/60000 (51%)]\tLoss: 91.518173\tBCE:73.8284\tKLD:17.6897\tC_loss:0.0000\n",
      "Train Epoch: 50 [33280/60000 (56%)]\tLoss: 90.558182\tBCE:73.1201\tKLD:17.4381\tC_loss:0.0000\n",
      "Train Epoch: 50 [35840/60000 (60%)]\tLoss: 89.843781\tBCE:71.9666\tKLD:17.8772\tC_loss:0.0000\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 92.691971\tBCE:74.8369\tKLD:17.8550\tC_loss:0.0000\n",
      "Train Epoch: 50 [40960/60000 (68%)]\tLoss: 87.674019\tBCE:70.2201\tKLD:17.4539\tC_loss:0.0000\n",
      "Train Epoch: 50 [43520/60000 (73%)]\tLoss: 90.601959\tBCE:72.7950\tKLD:17.8070\tC_loss:0.0000\n",
      "Train Epoch: 50 [46080/60000 (77%)]\tLoss: 89.273392\tBCE:71.6932\tKLD:17.5802\tC_loss:0.0000\n",
      "Train Epoch: 50 [48640/60000 (81%)]\tLoss: 89.752823\tBCE:72.1760\tKLD:17.5768\tC_loss:0.0000\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 90.024536\tBCE:72.4521\tKLD:17.5724\tC_loss:0.0000\n",
      "Train Epoch: 50 [53760/60000 (90%)]\tLoss: 90.198875\tBCE:72.7771\tKLD:17.4217\tC_loss:0.0000\n",
      "Train Epoch: 50 [56320/60000 (94%)]\tLoss: 91.447296\tBCE:73.4292\tKLD:18.0181\tC_loss:0.0000\n",
      "Train Epoch: 50 [58880/60000 (98%)]\tLoss: 86.258118\tBCE:69.6017\tKLD:16.6564\tC_loss:0.0000\n",
      "====> Epoch: 50 Average loss: 89.2830\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 90.5192\n",
      "Random number: 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch_train_loss = list()\n",
    "epoch_train_class_acc = list()\n",
    "epoch_test_loss = list()\n",
    "epoch_test_class_acc = list()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc = train(epoch)\n",
    "    test_loss, test_acc =test(epoch)\n",
    "    epoch_train_loss.append(train_loss)\n",
    "    epoch_train_class_acc.append(train_acc)\n",
    "    epoch_test_loss.append(test_loss)\n",
    "    epoch_test_class_acc.append(test_acc)\n",
    "\n",
    "    # Generate random digits every n epochs\n",
    "    with torch.inference_mode():\n",
    "        if epoch%5==0:\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "        \n",
    "            c = np.zeros(shape=(sample.shape[0],))\n",
    "            rand = np.random.randint(0, 10)\n",
    "            print(f\"Random number: {rand}\")\n",
    "            c[:] = rand\n",
    "            c = torch.FloatTensor(c)\n",
    "            c = c.to(torch.int64)\n",
    "            c = c.to(device)\n",
    "            c = F.one_hot(c, cond_shape)\n",
    "            sample = model.decoder((sample, c)).cpu()\n",
    "            \n",
    "            generated_image = sample[:, 0:sample.shape[1]]\n",
    "            \n",
    "            \n",
    "            save_image(generated_image.view(64, 1, 28, 28),\n",
    "                    'results/sample_' + str(epoch) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    sample = torch.randn(64, 20).to(device)\n",
    "        \n",
    "    c = np.zeros(shape=(sample.shape[0],))\n",
    "    num = i\n",
    "    c[:] = num\n",
    "    c = torch.FloatTensor(c)\n",
    "    c = c.to(torch.int64)\n",
    "    c = c.to(device)\n",
    "    c = F.one_hot(c, cond_shape)\n",
    "    sample = model.decoder((sample, c)).cpu()\n",
    "\n",
    "    generated_image = sample[:, 0:sample.shape[1]]\n",
    "\n",
    "\n",
    "    save_image(generated_image.view(64, 1, 28, 28),\n",
    "            'results/generated_conv_' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the digit \"4\" from gaussian noise\n",
      "Classifier says the below image is a 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1fe3a67eb0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "i = random.randint(0, 9)\n",
    "sample = torch.randn(1, 20).to(device)\n",
    "c = np.zeros(shape=(sample.shape[0],))\n",
    "num = i\n",
    "print(f\"Generating the digit \\\"{i}\\\" from gaussian noise\")\n",
    "c[:] = num\n",
    "c = torch.FloatTensor(c)\n",
    "c = c.to(torch.int64)\n",
    "c = c.to(device)\n",
    "c = F.one_hot(c, cond_shape)\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    sample = model.decoder((sample, c))\n",
    "    sample = sample.reshape([1, 1, 28, 28])\n",
    "    c_out = model.classifier(sample)\n",
    "\n",
    "c_out = torch.argmax(c_out).item()\n",
    "print(f\"Classifier says the below image is a {c_out}\")\n",
    "plt.imshow(sample[0].cpu().squeeze())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_train_class_acc)\n",
    "plt.plot(epoch_test_class_acc)\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_train_loss)\n",
    "plt.plot(epoch_test_loss)\n",
    "plt.title('Total Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the digit \"8\" from gaussian noise\n",
      "Classifier says the below image is a 6\n",
      "Iterations till mismatch: 55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1fe398c670>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "flag=0\n",
    "count=0\n",
    "while flag==0:\n",
    "    count+=1\n",
    "    i = random.randint(0, 9)\n",
    "    sample = torch.randn(1, 20).to(device)\n",
    "    c = np.zeros(shape=(sample.shape[0],))\n",
    "    num = i\n",
    "    # print(f\"Generating the digit \\\"{i}\\\" from gaussian noise\")\n",
    "    c[:] = num\n",
    "    c = torch.FloatTensor(c)\n",
    "    c = c.to(torch.int64)\n",
    "    c = c.to(device)\n",
    "    c = F.one_hot(c, cond_shape)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        sample = model.decoder((sample, c))\n",
    "        sample = sample.reshape([1, 1, 28, 28])\n",
    "        c_out = model.classifier(sample)\n",
    "        c_out = torch.argmax(c_out).item()\n",
    "    if c_out!=i:\n",
    "        flag=1\n",
    "\n",
    "print(f\"Generating the digit \\\"{i}\\\" from gaussian noise\")\n",
    "# c_out = torch.argmax(c_out).item()\n",
    "print(f\"Classifier says the below image is a {c_out}\")\n",
    "print(f\"Iterations till mismatch: {count}\")\n",
    "plt.imshow(sample[0].cpu().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
