{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fea5887e050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.distributions import Independent, Normal\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure as ssim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "PRINT_REQ= False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "BATCH_SIZE=256\n",
    "EPOCHS=50\n",
    "\n",
    "cond_shape=10\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size = 128\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_data = datasets.MNIST(root=\"../data\", train=True, \n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(root=\"../data\", train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=1)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True, num_workers=1)\n",
    "\n",
    "flat_img=torch.flatten(train_data[0][0])\n",
    "flat_shape=list(flat_img.shape)\n",
    "flat_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_debug(data):\n",
    "    if PRINT_REQ:\n",
    "        print(data)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_x, dim_y, dim_z):\n",
    "\n",
    "        super().__init__()\n",
    "         # Encoder layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=11, out_channels=32, kernel_size=5, stride=1,padding='same')\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=2,padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1,padding='same')\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, stride=2,padding=0)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=80, kernel_size=7, stride=1,padding='valid')\n",
    "        self.lin1 = nn.Linear(in_features=80, out_features=20)\n",
    "        self.lin2 = nn.Linear(in_features=80, out_features=20)\n",
    "\n",
    "        # reparameterization\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = inputs[0].to(device)\n",
    "        y = inputs[1].to(device)\n",
    "        \n",
    "        # y = F.one_hot(y, 10).to(device)\n",
    "        y = y.view(-1, 10, 1, 1).to(device)\n",
    "        \n",
    "        ones = torch.ones(x.size()[0], \n",
    "                            10,\n",
    "                            x.size()[2], \n",
    "                            x.size()[3], \n",
    "                            dtype=x.dtype).to(device)\n",
    "        y = ones * y\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        \n",
    "        print_debug(f\"input shape: {x.shape}\")\n",
    "        x = F.relu(self.conv1(x))\n",
    "        print_debug(x.shape)\n",
    "        # 32, 28, 28\n",
    "        x = F.pad(x, (0,3,0,3))\n",
    "        print_debug(x.shape)\n",
    "        # 32, 31, 31\n",
    "        x = F.relu(self.conv2(x))\n",
    "        print_debug(x.shape)\n",
    "        # 32, 14, 14\n",
    "        x = F.relu(self.conv3(x))\n",
    "        print_debug(x.shape)\n",
    "        # 64, 14, 14\n",
    "        x = F.pad(x, (0,3,0,3))\n",
    "        print_debug(x.shape)\n",
    "        # 64, 17, 17\n",
    "        x = F.relu(self.conv4(x))\n",
    "        print_debug(x.shape)\n",
    "        # 64, 7, 7\n",
    "        x = F.relu(self.conv5(x))\n",
    "        print_debug(x.shape)\n",
    "        # 80, 1, 1\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "        print_debug(f\"After flatten shape: {x.shape}\")\n",
    "        # 80\n",
    "        # print_debug(f\"Concatenating {x.shape} with {y.shape}\")\n",
    "        # concat = torch.cat([x, y], dim=-1)\n",
    "        # print_debug(f\"After concatenation shape: {concat.shape}\")\n",
    "        # 90\n",
    "        # loc=torch.zeros(mu_logvar.shape)\n",
    "        # scale=torch.ones(mu_logvar.shape)\n",
    "        # diagn = Independent(Normal(loc, scale), 1)\n",
    "        mu = self.lin1(x)\n",
    "        print_debug(f\"mu shape: {mu.shape}\")\n",
    "        # 20\n",
    "        logvar = self.lin2(x)\n",
    "        print_debug(f\"logvar shape: {logvar.shape}\")\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        print_debug(f\"Returning shape {z.shape}\")\n",
    "        return  mu, logvar, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim_y, dim_z):\n",
    "        super().__init__()\n",
    "        self.dim_z = dim_z\n",
    "        self.dim_y = dim_y\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=30, out_channels=64, kernel_size=7, stride=1, padding=0) # valid means no pad\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=5, stride=1, padding=2)\n",
    "        self.deconv3 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=5, stride=2, padding=2, output_padding=1) # pad operation added in forward\n",
    "        self.deconv4 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.deconv5 = nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=5, stride=2, padding=2, output_padding=1) # pad operation added in forward\n",
    "        self.deconv6 = nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=5, stride=1,padding='same')\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs[0].to(device)#.unsqueeze(dim=0)\n",
    "        y = inputs[1].to(device)\n",
    "        print_debug(f\"latent space shape: {x.shape}, labels shape: {y.shape}\")\n",
    "        x = torch.cat([x, y], dim=1)\n",
    "        x = torch.reshape(x, (-1, self.dim_z+self.dim_y, 1, 1))\n",
    "        print_debug(f\"After concatenation shape: {x.shape}\")\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        print_debug(f\"ConvTrans1 output shape: {x.shape}\")\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        print_debug(f\"ConvTrans2 output shape: {x.shape}\")\n",
    "        x = F.pad(x, (0,0,0,0))\n",
    "        x = F.relu(self.deconv3(x))\n",
    "        print_debug(f\"ConvTrans3 output shape: {x.shape}\")\n",
    "        x = F.relu(self.deconv4(x))\n",
    "        print_debug(f\"ConvTrans4 output shape: {x.shape}\")\n",
    "        # x = F.pad(x, (0,3,0,3))\n",
    "        x = F.relu(self.deconv5(x))\n",
    "        print_debug(f\"ConvTrans5 output shape: {x.shape}\")\n",
    "        x = F.relu(self.deconv6(x))\n",
    "        print_debug(f\"ConvTrans6 output shape: {x.shape}\")\n",
    "        x = torch.sigmoid(self.conv(x))\n",
    "        print_debug(f\"Conv output shape: {x.shape}\")\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "        print_debug(f\"After flatten shape: {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, dim_x, dim_y, dim_z):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=flat_shape[0], out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=dim_y),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        #Encoder \n",
    "        self.encoder = Encoder(dim_x=dim_x, dim_y=dim_y, dim_z=dim_z)\n",
    "\n",
    "        #Decoder\n",
    "        self.decoder = Decoder(dim_y=dim_y, dim_z=dim_z)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x, y = inputs      \n",
    "        x = x.to(device)\n",
    "        y = F.one_hot(y, 10).to(device)  \n",
    "        print_debug(f\"Inputs shape: {x.shape} and labels: {y.shape}\")\n",
    "        c_out = self.classifier(x)\n",
    "        mu, logvar, z = self.encoder((x,y))\n",
    "        out = self.decoder((z, y))\n",
    "        print_debug(f\"decoder output shape is: {out.shape}\")\n",
    "        return mu, logvar, out, c_out\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CVAE(dim_x=(28, 28, 1), dim_y=10, dim_z=20).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_fn(recon, x, mu, logvar, c_out, y_onehot):\n",
    "    y_onehot1 = y_onehot.type(torch.FloatTensor).to(device)\n",
    "    # print(c_out.shape, y_onehot.shape, c_out.dtype, y_onehot.dtype)\n",
    "    classif_loss = torch.nn.BCELoss()(c_out, y_onehot1)\n",
    "    BCE = F.binary_cross_entropy(recon, x, reduction='sum')        \n",
    "    KLD = -0.5*torch.sum(1+logvar-mu.pow(2)-logvar.exp())\n",
    "    return classif_loss+BCE+KLD, classif_loss, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one iteration to validate output shapes with PRINT_DEBUG = True\n",
    "for i, (x, y) in enumerate(train_dataloader):    \n",
    "    model((x,y))\n",
    "    # x = x.to(device)\n",
    "    # print(f\"ysghape is {y.shape}\")\n",
    "    # y = F.one_hot(y, 10).to(device)\n",
    "    # y = y.view(-1, 10, 1, 1).to(device)\n",
    "    \n",
    "    # ones = torch.ones(x.size()[0], \n",
    "    #                     10,\n",
    "    #                     x.size()[2], \n",
    "    #                     x.size()[3], \n",
    "    #                     dtype=x.dtype).to(device)\n",
    "    # y = ones * y\n",
    "    # print(ones.shape, y.shape)\n",
    "    # x = torch.cat((x, y), dim=1)\n",
    "    # print(x.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Truth labels for predictions.\n",
    "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
    "\n",
    "    Returns:\n",
    "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
    "    \"\"\"\n",
    "    \n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    classif_accuracy = 0\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X = X.to(device) #[64, 1, 28, 28]\n",
    "        y = y.to(device)\n",
    "        \n",
    "        \n",
    "        # 1. Forward pass\n",
    "        mu, logvar, recon_batch, c_out = model((X, y))\n",
    "        # print(f\"---------------{torch.argmax(c_out, dim=1).shape}\")\n",
    "        # print(f\"---------------{y.shape}\")\n",
    "        flat_data = X.view(-1, flat_shape[0]).to(device)                            \n",
    "        y_onehot = F.one_hot(y, cond_shape).to(device)\n",
    "        inp = torch.cat((flat_data, y_onehot), 1)\n",
    "        \n",
    "        # 2. Calculate loss\n",
    "        loss, C_loss, BCE, KLD = loss_fn(recon_batch, flat_data, mu, logvar, c_out, y_onehot)\n",
    "        train_loss += loss.item()\n",
    "        classif_accuracy += accuracy_fn(y, torch.argmax(c_out, dim=1))\n",
    "        \n",
    "        \n",
    "\n",
    "        # 3. Zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Backprop\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Step\n",
    "        optimizer.step()\n",
    "    \n",
    "        if batch % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tBCE:{:.4f}\\tKLD:{:.4f}\\tC_loss:{:.4f}'.format(\n",
    "                epoch,\n",
    "                batch * len(X),\n",
    "                len(train_dataloader.dataset),\n",
    "                100. * batch / len(train_dataloader),\n",
    "                loss.item() / len(X), BCE.item() / len(X), KLD.item() / len(X), C_loss.item() / len(X)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}\\tClassifier Accuracy: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_dataloader.dataset), classif_accuracy/len(train_dataloader)))\n",
    "    return train_loss/len(train_dataloader.dataset), classif_accuracy/len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    #Sets the module in evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    classif_accuracy = 0\n",
    "    with torch.inference_mode():\n",
    "        for i, (X, y) in enumerate(test_dataloader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # 1. Forward pass\n",
    "            mu, logvar, recon_batch, c_out = model((X, y))\n",
    "            \n",
    "            flat_data = X.view(-1, flat_shape[0]).to(device)\n",
    "            y_onehot = F.one_hot(y, cond_shape).to(device)\n",
    "            inp = torch.cat((flat_data, y_onehot), 1)\n",
    "\n",
    "            # 2. Loss\n",
    "            tot_loss, C_loss, BCE, KLD = loss_fn(recon_batch, flat_data, mu, logvar, c_out, y_onehot)\n",
    "            test_loss += tot_loss.item()\n",
    "            classif_accuracy += accuracy_fn(y, torch.argmax(c_out, dim=1))\n",
    "\n",
    "            # 3. Save images\n",
    "            if epoch%5==0 and i == 0:\n",
    "                n = min(X.size(0), 8)\n",
    "                recon_image = recon_batch[:, 0:recon_batch.shape[1]]\n",
    "                print(recon_image.shape)\n",
    "                recon_image = recon_image.view(BATCH_SIZE, 1, 28,28)\n",
    "                print('---',recon_image.shape)\n",
    "                comparison = torch.cat([X[:n],\n",
    "                                      recon_image.view(BATCH_SIZE, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss, classif_accuracy/len(test_dataloader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.261414\tBCE:550.1899\tKLD:0.0702\tC_loss:0.0013\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 406.930420\tBCE:406.8771\tKLD:0.0523\tC_loss:0.0010\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 351.715302\tBCE:351.6536\tKLD:0.0610\tC_loss:0.0007\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 286.468445\tBCE:286.3947\tKLD:0.0733\tC_loss:0.0005\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 232.389832\tBCE:232.2981\tKLD:0.0913\tC_loss:0.0004\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 210.967834\tBCE:210.8722\tKLD:0.0953\tC_loss:0.0003\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 200.741821\tBCE:200.6570\tKLD:0.0846\tC_loss:0.0003\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 194.749878\tBCE:194.6738\tKLD:0.0758\tC_loss:0.0003\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 192.209625\tBCE:192.1398\tKLD:0.0697\tC_loss:0.0002\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 191.264679\tBCE:191.1985\tKLD:0.0659\tC_loss:0.0003\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 183.513474\tBCE:183.4503\tKLD:0.0629\tC_loss:0.0002\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 182.473602\tBCE:182.4135\tKLD:0.0598\tC_loss:0.0003\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 178.745941\tBCE:178.6900\tKLD:0.0557\tC_loss:0.0002\n",
      "Train Epoch: 1 [33280/60000 (56%)]\tLoss: 181.946182\tBCE:181.8946\tKLD:0.0514\tC_loss:0.0002\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 181.268906\tBCE:181.2205\tKLD:0.0481\tC_loss:0.0003\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 184.837967\tBCE:184.7924\tKLD:0.0454\tC_loss:0.0002\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 183.157104\tBCE:183.1145\tKLD:0.0424\tC_loss:0.0002\n",
      "Train Epoch: 1 [43520/60000 (73%)]\tLoss: 181.247437\tBCE:181.2069\tKLD:0.0404\tC_loss:0.0002\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 175.880844\tBCE:175.8418\tKLD:0.0389\tC_loss:0.0002\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 185.089096\tBCE:185.0511\tKLD:0.0378\tC_loss:0.0002\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 173.680145\tBCE:173.6421\tKLD:0.0378\tC_loss:0.0002\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 180.271057\tBCE:180.2328\tKLD:0.0381\tC_loss:0.0002\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 174.786575\tBCE:174.7478\tKLD:0.0386\tC_loss:0.0002\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 173.043427\tBCE:173.0038\tKLD:0.0394\tC_loss:0.0002\n",
      "====> Epoch: 1 Average loss: 215.6889\tClassifier Accuracy: 86.6403\n",
      "====> Test set loss: 174.9493\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 175.082153\tBCE:175.0420\tKLD:0.0399\tC_loss:0.0002\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 176.383682\tBCE:176.3409\tKLD:0.0426\tC_loss:0.0002\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 176.318604\tBCE:176.2748\tKLD:0.0436\tC_loss:0.0001\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 172.212723\tBCE:172.1673\tKLD:0.0453\tC_loss:0.0001\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 177.061676\tBCE:177.0149\tKLD:0.0466\tC_loss:0.0002\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 175.257370\tBCE:175.2107\tKLD:0.0465\tC_loss:0.0002\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 174.066971\tBCE:174.0225\tKLD:0.0443\tC_loss:0.0002\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 174.945114\tBCE:174.9001\tKLD:0.0448\tC_loss:0.0002\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 178.924316\tBCE:178.8810\tKLD:0.0432\tC_loss:0.0002\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 179.634216\tBCE:179.5899\tKLD:0.0442\tC_loss:0.0002\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 175.752670\tBCE:175.7079\tKLD:0.0446\tC_loss:0.0002\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 173.705429\tBCE:173.6608\tKLD:0.0445\tC_loss:0.0002\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 176.224396\tBCE:176.1807\tKLD:0.0436\tC_loss:0.0001\n",
      "Train Epoch: 2 [33280/60000 (56%)]\tLoss: 178.963257\tBCE:178.9227\tKLD:0.0403\tC_loss:0.0002\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 175.240402\tBCE:175.2010\tKLD:0.0393\tC_loss:0.0001\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 173.755844\tBCE:173.7169\tKLD:0.0388\tC_loss:0.0001\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 177.250351\tBCE:177.2124\tKLD:0.0378\tC_loss:0.0002\n",
      "Train Epoch: 2 [43520/60000 (73%)]\tLoss: 167.965881\tBCE:167.9296\tKLD:0.0362\tC_loss:0.0001\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 174.630325\tBCE:174.5950\tKLD:0.0352\tC_loss:0.0002\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 173.685989\tBCE:173.6512\tKLD:0.0346\tC_loss:0.0001\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 171.417587\tBCE:171.3838\tKLD:0.0337\tC_loss:0.0001\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 166.958649\tBCE:166.9263\tKLD:0.0322\tC_loss:0.0001\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 172.957031\tBCE:172.9262\tKLD:0.0307\tC_loss:0.0001\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 171.373337\tBCE:171.3430\tKLD:0.0302\tC_loss:0.0001\n",
      "====> Epoch: 2 Average loss: 175.2216\tClassifier Accuracy: 93.4412\n",
      "====> Test set loss: 172.7827\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 172.169342\tBCE:172.1394\tKLD:0.0299\tC_loss:0.0001\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 175.878906\tBCE:175.8492\tKLD:0.0296\tC_loss:0.0001\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 175.549835\tBCE:175.5214\tKLD:0.0283\tC_loss:0.0001\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 176.923721\tBCE:176.8961\tKLD:0.0275\tC_loss:0.0001\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 180.544968\tBCE:180.5175\tKLD:0.0273\tC_loss:0.0002\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 174.418823\tBCE:174.3924\tKLD:0.0262\tC_loss:0.0002\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 178.050858\tBCE:178.0259\tKLD:0.0249\tC_loss:0.0001\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 175.179611\tBCE:175.1562\tKLD:0.0234\tC_loss:0.0001\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 176.438309\tBCE:176.4151\tKLD:0.0231\tC_loss:0.0001\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 172.685715\tBCE:172.6620\tKLD:0.0236\tC_loss:0.0001\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 179.213699\tBCE:179.1909\tKLD:0.0226\tC_loss:0.0001\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 172.922623\tBCE:172.9005\tKLD:0.0220\tC_loss:0.0001\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 175.744995\tBCE:175.7237\tKLD:0.0212\tC_loss:0.0001\n",
      "Train Epoch: 3 [33280/60000 (56%)]\tLoss: 173.021713\tBCE:173.0001\tKLD:0.0215\tC_loss:0.0001\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 173.522598\tBCE:173.5014\tKLD:0.0211\tC_loss:0.0001\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 175.180252\tBCE:175.1588\tKLD:0.0214\tC_loss:0.0001\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 173.840027\tBCE:173.8191\tKLD:0.0208\tC_loss:0.0001\n",
      "Train Epoch: 3 [43520/60000 (73%)]\tLoss: 173.798126\tBCE:173.7776\tKLD:0.0204\tC_loss:0.0001\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 175.113876\tBCE:175.0944\tKLD:0.0194\tC_loss:0.0001\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 181.901489\tBCE:181.8832\tKLD:0.0182\tC_loss:0.0001\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 173.705872\tBCE:173.6880\tKLD:0.0178\tC_loss:0.0001\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 170.395721\tBCE:170.3782\tKLD:0.0174\tC_loss:0.0001\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 169.291397\tBCE:169.2740\tKLD:0.0172\tC_loss:0.0001\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 171.217896\tBCE:171.2014\tKLD:0.0164\tC_loss:0.0001\n",
      "====> Epoch: 3 Average loss: 174.1773\tClassifier Accuracy: 94.9870\n",
      "====> Test set loss: 172.6860\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 175.184875\tBCE:175.1689\tKLD:0.0159\tC_loss:0.0001\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 177.523178\tBCE:177.5077\tKLD:0.0154\tC_loss:0.0001\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 177.490570\tBCE:177.4753\tKLD:0.0151\tC_loss:0.0001\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 174.764633\tBCE:174.7500\tKLD:0.0146\tC_loss:0.0001\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 176.811493\tBCE:176.7972\tKLD:0.0142\tC_loss:0.0001\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 173.743073\tBCE:173.7285\tKLD:0.0144\tC_loss:0.0001\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 172.664536\tBCE:172.6499\tKLD:0.0145\tC_loss:0.0001\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 174.528778\tBCE:174.5140\tKLD:0.0147\tC_loss:0.0001\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 172.286057\tBCE:172.2718\tKLD:0.0142\tC_loss:0.0001\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 173.763611\tBCE:173.7494\tKLD:0.0141\tC_loss:0.0001\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 171.350876\tBCE:171.3371\tKLD:0.0137\tC_loss:0.0001\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 171.625687\tBCE:171.6120\tKLD:0.0136\tC_loss:0.0001\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 169.228302\tBCE:169.2154\tKLD:0.0128\tC_loss:0.0001\n",
      "Train Epoch: 4 [33280/60000 (56%)]\tLoss: 171.716202\tBCE:171.7038\tKLD:0.0123\tC_loss:0.0001\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 177.166733\tBCE:177.1545\tKLD:0.0121\tC_loss:0.0001\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 170.572739\tBCE:170.5611\tKLD:0.0115\tC_loss:0.0001\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 172.686081\tBCE:172.6744\tKLD:0.0116\tC_loss:0.0001\n",
      "Train Epoch: 4 [43520/60000 (73%)]\tLoss: 172.930969\tBCE:172.9193\tKLD:0.0116\tC_loss:0.0001\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 175.614182\tBCE:175.6025\tKLD:0.0116\tC_loss:0.0001\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 178.292953\tBCE:178.2818\tKLD:0.0111\tC_loss:0.0001\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 167.092392\tBCE:167.0817\tKLD:0.0107\tC_loss:0.0001\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 172.079681\tBCE:172.0690\tKLD:0.0106\tC_loss:0.0001\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 173.612152\tBCE:173.6016\tKLD:0.0105\tC_loss:0.0001\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 172.765839\tBCE:172.7551\tKLD:0.0106\tC_loss:0.0001\n",
      "====> Epoch: 4 Average loss: 173.7970\tClassifier Accuracy: 95.8634\n",
      "====> Test set loss: 172.1693\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 176.217056\tBCE:176.2066\tKLD:0.0104\tC_loss:0.0001\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 176.076614\tBCE:176.0665\tKLD:0.0100\tC_loss:0.0001\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 170.603455\tBCE:170.5937\tKLD:0.0097\tC_loss:0.0001\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 171.580704\tBCE:171.5708\tKLD:0.0099\tC_loss:0.0001\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 171.130676\tBCE:171.1208\tKLD:0.0097\tC_loss:0.0001\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 176.727417\tBCE:176.7181\tKLD:0.0093\tC_loss:0.0001\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 172.204575\tBCE:172.1954\tKLD:0.0091\tC_loss:0.0000\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 173.833633\tBCE:173.8241\tKLD:0.0095\tC_loss:0.0001\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 177.078384\tBCE:177.0686\tKLD:0.0097\tC_loss:0.0001\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 180.371414\tBCE:180.3612\tKLD:0.0101\tC_loss:0.0001\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 171.009811\tBCE:170.9998\tKLD:0.0099\tC_loss:0.0001\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 177.340897\tBCE:177.3315\tKLD:0.0093\tC_loss:0.0001\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 169.811066\tBCE:169.8021\tKLD:0.0089\tC_loss:0.0001\n",
      "Train Epoch: 5 [33280/60000 (56%)]\tLoss: 173.922318\tBCE:173.9133\tKLD:0.0090\tC_loss:0.0001\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 171.468597\tBCE:171.4593\tKLD:0.0092\tC_loss:0.0001\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 175.872269\tBCE:175.8632\tKLD:0.0090\tC_loss:0.0001\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 176.826416\tBCE:176.8177\tKLD:0.0086\tC_loss:0.0001\n",
      "Train Epoch: 5 [43520/60000 (73%)]\tLoss: 179.085052\tBCE:179.0764\tKLD:0.0086\tC_loss:0.0001\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 172.110168\tBCE:172.1019\tKLD:0.0082\tC_loss:0.0001\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 177.166199\tBCE:177.1581\tKLD:0.0080\tC_loss:0.0001\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 179.935181\tBCE:179.9271\tKLD:0.0080\tC_loss:0.0001\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 165.921829\tBCE:165.9140\tKLD:0.0078\tC_loss:0.0001\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 173.372650\tBCE:173.3649\tKLD:0.0076\tC_loss:0.0001\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 175.042267\tBCE:175.0347\tKLD:0.0075\tC_loss:0.0001\n",
      "====> Epoch: 5 Average loss: 173.7399\tClassifier Accuracy: 96.5278\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 172.3044\n",
      "Random number: 2\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 169.989944\tBCE:169.9826\tKLD:0.0073\tC_loss:0.0001\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 175.447052\tBCE:175.4395\tKLD:0.0075\tC_loss:0.0001\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 167.772644\tBCE:167.7652\tKLD:0.0074\tC_loss:0.0001\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 170.878799\tBCE:170.8717\tKLD:0.0070\tC_loss:0.0001\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 175.294418\tBCE:175.2875\tKLD:0.0069\tC_loss:0.0000\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 177.428101\tBCE:177.4212\tKLD:0.0069\tC_loss:0.0001\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 173.184113\tBCE:173.1772\tKLD:0.0069\tC_loss:0.0001\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 172.090210\tBCE:172.0832\tKLD:0.0069\tC_loss:0.0001\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 173.584274\tBCE:173.5772\tKLD:0.0070\tC_loss:0.0001\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 171.856674\tBCE:171.8497\tKLD:0.0069\tC_loss:0.0001\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 174.822708\tBCE:174.8161\tKLD:0.0066\tC_loss:0.0001\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 171.859360\tBCE:171.8532\tKLD:0.0060\tC_loss:0.0001\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 177.952194\tBCE:177.9461\tKLD:0.0060\tC_loss:0.0001\n",
      "Train Epoch: 6 [33280/60000 (56%)]\tLoss: 176.301682\tBCE:176.2957\tKLD:0.0059\tC_loss:0.0001\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 173.492996\tBCE:173.4872\tKLD:0.0057\tC_loss:0.0001\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 173.124588\tBCE:173.1189\tKLD:0.0057\tC_loss:0.0001\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 172.080154\tBCE:172.0743\tKLD:0.0057\tC_loss:0.0001\n",
      "Train Epoch: 6 [43520/60000 (73%)]\tLoss: 174.813431\tBCE:174.8076\tKLD:0.0058\tC_loss:0.0001\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 175.288864\tBCE:175.2829\tKLD:0.0058\tC_loss:0.0001\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 171.261978\tBCE:171.2563\tKLD:0.0056\tC_loss:0.0000\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 173.337143\tBCE:173.3318\tKLD:0.0053\tC_loss:0.0001\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 169.121643\tBCE:169.1161\tKLD:0.0055\tC_loss:0.0001\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 171.314316\tBCE:171.3086\tKLD:0.0056\tC_loss:0.0001\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 181.689514\tBCE:181.6839\tKLD:0.0055\tC_loss:0.0001\n",
      "====> Epoch: 6 Average loss: 173.4914\tClassifier Accuracy: 97.0837\n",
      "====> Test set loss: 172.9062\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 172.263809\tBCE:172.2584\tKLD:0.0054\tC_loss:0.0001\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 168.465912\tBCE:168.4606\tKLD:0.0052\tC_loss:0.0001\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 169.692871\tBCE:169.6875\tKLD:0.0053\tC_loss:0.0000\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 169.794647\tBCE:169.7894\tKLD:0.0052\tC_loss:0.0001\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 169.791031\tBCE:169.7858\tKLD:0.0051\tC_loss:0.0001\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 173.568680\tBCE:173.5634\tKLD:0.0053\tC_loss:0.0000\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 176.522232\tBCE:176.5169\tKLD:0.0053\tC_loss:0.0000\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 174.810547\tBCE:174.8053\tKLD:0.0052\tC_loss:0.0001\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 172.828537\tBCE:172.8233\tKLD:0.0051\tC_loss:0.0001\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 177.680161\tBCE:177.6750\tKLD:0.0051\tC_loss:0.0001\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 175.028564\tBCE:175.0235\tKLD:0.0050\tC_loss:0.0000\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 173.500198\tBCE:173.4953\tKLD:0.0048\tC_loss:0.0001\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 173.236786\tBCE:173.2319\tKLD:0.0048\tC_loss:0.0001\n",
      "Train Epoch: 7 [33280/60000 (56%)]\tLoss: 175.096054\tBCE:175.0914\tKLD:0.0046\tC_loss:0.0001\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 173.286377\tBCE:173.2817\tKLD:0.0046\tC_loss:0.0001\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 176.714737\tBCE:176.7097\tKLD:0.0050\tC_loss:0.0001\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 178.051025\tBCE:178.0460\tKLD:0.0050\tC_loss:0.0001\n",
      "Train Epoch: 7 [43520/60000 (73%)]\tLoss: 179.544037\tBCE:179.5392\tKLD:0.0048\tC_loss:0.0001\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 171.954681\tBCE:171.9497\tKLD:0.0049\tC_loss:0.0001\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 175.255615\tBCE:175.2507\tKLD:0.0048\tC_loss:0.0001\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 176.321442\tBCE:176.3165\tKLD:0.0049\tC_loss:0.0001\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 171.155731\tBCE:171.1509\tKLD:0.0048\tC_loss:0.0000\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 172.954437\tBCE:172.9497\tKLD:0.0047\tC_loss:0.0001\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 177.878876\tBCE:177.8745\tKLD:0.0044\tC_loss:0.0000\n",
      "====> Epoch: 7 Average loss: 173.4181\tClassifier Accuracy: 97.5060\n",
      "====> Test set loss: 172.0799\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 172.845322\tBCE:172.8410\tKLD:0.0042\tC_loss:0.0000\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 170.696503\tBCE:170.6922\tKLD:0.0043\tC_loss:0.0000\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 172.889893\tBCE:172.8854\tKLD:0.0044\tC_loss:0.0001\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 177.172562\tBCE:177.1680\tKLD:0.0045\tC_loss:0.0000\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 174.974899\tBCE:174.9706\tKLD:0.0042\tC_loss:0.0001\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 174.264923\tBCE:174.2608\tKLD:0.0041\tC_loss:0.0001\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 172.759430\tBCE:172.7554\tKLD:0.0040\tC_loss:0.0001\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 174.449631\tBCE:174.4456\tKLD:0.0040\tC_loss:0.0001\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 179.695740\tBCE:179.6917\tKLD:0.0040\tC_loss:0.0001\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 174.255417\tBCE:174.2512\tKLD:0.0042\tC_loss:0.0001\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 177.303696\tBCE:177.2995\tKLD:0.0042\tC_loss:0.0000\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 179.711258\tBCE:179.7073\tKLD:0.0039\tC_loss:0.0001\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 169.348831\tBCE:169.3450\tKLD:0.0038\tC_loss:0.0000\n",
      "Train Epoch: 8 [33280/60000 (56%)]\tLoss: 177.182312\tBCE:177.1782\tKLD:0.0041\tC_loss:0.0001\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 174.940979\tBCE:174.9368\tKLD:0.0041\tC_loss:0.0001\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 175.275665\tBCE:175.2716\tKLD:0.0040\tC_loss:0.0000\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 171.518677\tBCE:171.5148\tKLD:0.0038\tC_loss:0.0000\n",
      "Train Epoch: 8 [43520/60000 (73%)]\tLoss: 173.799515\tBCE:173.7957\tKLD:0.0037\tC_loss:0.0001\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 177.991501\tBCE:177.9876\tKLD:0.0039\tC_loss:0.0000\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 179.999649\tBCE:179.9960\tKLD:0.0036\tC_loss:0.0000\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 176.088654\tBCE:176.0853\tKLD:0.0033\tC_loss:0.0000\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 172.498535\tBCE:172.4953\tKLD:0.0032\tC_loss:0.0000\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 170.978958\tBCE:170.9754\tKLD:0.0034\tC_loss:0.0001\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 171.634186\tBCE:171.6307\tKLD:0.0034\tC_loss:0.0000\n",
      "====> Epoch: 8 Average loss: 173.3762\tClassifier Accuracy: 97.7865\n",
      "====> Test set loss: 171.9952\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 175.316422\tBCE:175.3130\tKLD:0.0033\tC_loss:0.0001\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 173.299210\tBCE:173.2961\tKLD:0.0031\tC_loss:0.0000\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 174.712585\tBCE:174.7095\tKLD:0.0031\tC_loss:0.0000\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 179.270859\tBCE:179.2679\tKLD:0.0029\tC_loss:0.0000\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 169.373230\tBCE:169.3704\tKLD:0.0028\tC_loss:0.0000\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 172.097855\tBCE:172.0948\tKLD:0.0030\tC_loss:0.0000\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 172.608582\tBCE:172.6056\tKLD:0.0030\tC_loss:0.0000\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 175.647736\tBCE:175.6445\tKLD:0.0031\tC_loss:0.0001\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 174.564606\tBCE:174.5616\tKLD:0.0030\tC_loss:0.0000\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 169.112610\tBCE:169.1096\tKLD:0.0030\tC_loss:0.0000\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 170.893433\tBCE:170.8904\tKLD:0.0030\tC_loss:0.0001\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 178.696518\tBCE:178.6937\tKLD:0.0028\tC_loss:0.0001\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 173.939484\tBCE:173.9367\tKLD:0.0028\tC_loss:0.0000\n",
      "Train Epoch: 9 [33280/60000 (56%)]\tLoss: 170.435181\tBCE:170.4320\tKLD:0.0031\tC_loss:0.0000\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 176.105331\tBCE:176.1021\tKLD:0.0032\tC_loss:0.0000\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 177.027298\tBCE:177.0238\tKLD:0.0034\tC_loss:0.0001\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 171.631775\tBCE:171.6285\tKLD:0.0032\tC_loss:0.0000\n",
      "Train Epoch: 9 [43520/60000 (73%)]\tLoss: 176.463135\tBCE:176.4600\tKLD:0.0031\tC_loss:0.0001\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 169.310318\tBCE:169.3073\tKLD:0.0030\tC_loss:0.0000\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 169.710236\tBCE:169.7072\tKLD:0.0030\tC_loss:0.0000\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 171.768616\tBCE:171.7657\tKLD:0.0029\tC_loss:0.0000\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 167.789444\tBCE:167.7865\tKLD:0.0029\tC_loss:0.0000\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 176.651077\tBCE:176.6480\tKLD:0.0030\tC_loss:0.0000\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 174.570175\tBCE:174.5673\tKLD:0.0029\tC_loss:0.0000\n",
      "====> Epoch: 9 Average loss: 173.3315\tClassifier Accuracy: 98.0753\n",
      "====> Test set loss: 171.6651\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 178.183090\tBCE:178.1802\tKLD:0.0028\tC_loss:0.0001\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 170.542419\tBCE:170.5396\tKLD:0.0028\tC_loss:0.0000\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 175.891693\tBCE:175.8887\tKLD:0.0030\tC_loss:0.0000\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 177.739441\tBCE:177.7366\tKLD:0.0027\tC_loss:0.0001\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 168.777237\tBCE:168.7746\tKLD:0.0026\tC_loss:0.0000\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 169.701859\tBCE:169.6991\tKLD:0.0027\tC_loss:0.0000\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 172.436844\tBCE:172.4340\tKLD:0.0028\tC_loss:0.0000\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 174.188538\tBCE:174.1856\tKLD:0.0029\tC_loss:0.0000\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 176.076828\tBCE:176.0741\tKLD:0.0027\tC_loss:0.0000\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 169.119812\tBCE:169.1173\tKLD:0.0025\tC_loss:0.0000\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 171.112717\tBCE:171.1104\tKLD:0.0023\tC_loss:0.0000\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 178.216949\tBCE:178.2144\tKLD:0.0026\tC_loss:0.0000\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 171.912308\tBCE:171.9095\tKLD:0.0027\tC_loss:0.0000\n",
      "Train Epoch: 10 [33280/60000 (56%)]\tLoss: 172.498550\tBCE:172.4960\tKLD:0.0025\tC_loss:0.0001\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 174.973877\tBCE:174.9714\tKLD:0.0024\tC_loss:0.0000\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 167.191772\tBCE:167.1893\tKLD:0.0025\tC_loss:0.0000\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 180.589554\tBCE:180.5871\tKLD:0.0024\tC_loss:0.0001\n",
      "Train Epoch: 10 [43520/60000 (73%)]\tLoss: 173.894821\tBCE:173.8922\tKLD:0.0026\tC_loss:0.0000\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 174.086960\tBCE:174.0841\tKLD:0.0028\tC_loss:0.0001\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 178.960754\tBCE:178.9579\tKLD:0.0028\tC_loss:0.0000\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 176.660675\tBCE:176.6580\tKLD:0.0026\tC_loss:0.0001\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 177.618439\tBCE:177.6160\tKLD:0.0024\tC_loss:0.0000\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 170.007706\tBCE:170.0053\tKLD:0.0024\tC_loss:0.0000\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 173.569885\tBCE:173.5676\tKLD:0.0023\tC_loss:0.0000\n",
      "====> Epoch: 10 Average loss: 173.2954\tClassifier Accuracy: 98.3156\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 172.2122\n",
      "Random number: 1\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 175.580276\tBCE:175.5780\tKLD:0.0022\tC_loss:0.0000\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 178.517792\tBCE:178.5157\tKLD:0.0020\tC_loss:0.0000\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 169.383499\tBCE:169.3813\tKLD:0.0022\tC_loss:0.0000\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 172.394302\tBCE:172.3919\tKLD:0.0024\tC_loss:0.0000\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 171.098480\tBCE:171.0960\tKLD:0.0024\tC_loss:0.0000\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 170.680801\tBCE:170.6783\tKLD:0.0024\tC_loss:0.0000\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 177.174545\tBCE:177.1720\tKLD:0.0025\tC_loss:0.0000\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 173.589905\tBCE:173.5874\tKLD:0.0025\tC_loss:0.0000\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 173.390900\tBCE:173.3884\tKLD:0.0025\tC_loss:0.0001\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 175.959610\tBCE:175.9571\tKLD:0.0025\tC_loss:0.0000\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 172.113434\tBCE:172.1109\tKLD:0.0025\tC_loss:0.0001\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 178.767181\tBCE:178.7644\tKLD:0.0027\tC_loss:0.0001\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 176.020233\tBCE:176.0175\tKLD:0.0027\tC_loss:0.0000\n",
      "Train Epoch: 11 [33280/60000 (56%)]\tLoss: 173.141190\tBCE:173.1387\tKLD:0.0024\tC_loss:0.0000\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 171.975174\tBCE:171.9729\tKLD:0.0022\tC_loss:0.0000\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 175.185364\tBCE:175.1831\tKLD:0.0022\tC_loss:0.0000\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 177.845367\tBCE:177.8432\tKLD:0.0022\tC_loss:0.0000\n",
      "Train Epoch: 11 [43520/60000 (73%)]\tLoss: 177.033859\tBCE:177.0314\tKLD:0.0024\tC_loss:0.0000\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 178.627609\tBCE:178.6252\tKLD:0.0023\tC_loss:0.0001\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 170.680817\tBCE:170.6786\tKLD:0.0021\tC_loss:0.0000\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 177.214523\tBCE:177.2124\tKLD:0.0021\tC_loss:0.0000\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 176.282745\tBCE:176.2806\tKLD:0.0021\tC_loss:0.0001\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 174.182663\tBCE:174.1806\tKLD:0.0021\tC_loss:0.0000\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 170.417145\tBCE:170.4149\tKLD:0.0022\tC_loss:0.0000\n",
      "====> Epoch: 11 Average loss: 173.3243\tClassifier Accuracy: 98.4909\n",
      "====> Test set loss: 171.8851\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 176.141983\tBCE:176.1397\tKLD:0.0023\tC_loss:0.0000\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 169.590607\tBCE:169.5884\tKLD:0.0022\tC_loss:0.0000\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 172.435928\tBCE:172.4338\tKLD:0.0021\tC_loss:0.0000\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 172.752838\tBCE:172.7508\tKLD:0.0020\tC_loss:0.0000\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 172.638107\tBCE:172.6361\tKLD:0.0020\tC_loss:0.0000\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 175.514297\tBCE:175.5124\tKLD:0.0019\tC_loss:0.0000\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 167.258652\tBCE:167.2568\tKLD:0.0018\tC_loss:0.0000\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 165.824829\tBCE:165.8230\tKLD:0.0018\tC_loss:0.0000\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 172.618454\tBCE:172.6166\tKLD:0.0018\tC_loss:0.0000\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 174.381271\tBCE:174.3794\tKLD:0.0018\tC_loss:0.0000\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 167.622040\tBCE:167.6201\tKLD:0.0020\tC_loss:0.0000\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 174.280838\tBCE:174.2790\tKLD:0.0018\tC_loss:0.0000\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 177.231888\tBCE:177.2301\tKLD:0.0018\tC_loss:0.0000\n",
      "Train Epoch: 12 [33280/60000 (56%)]\tLoss: 174.896820\tBCE:174.8949\tKLD:0.0019\tC_loss:0.0000\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 170.578659\tBCE:170.5767\tKLD:0.0020\tC_loss:0.0000\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 170.940948\tBCE:170.9389\tKLD:0.0020\tC_loss:0.0000\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 170.395157\tBCE:170.3930\tKLD:0.0021\tC_loss:0.0000\n",
      "Train Epoch: 12 [43520/60000 (73%)]\tLoss: 174.622025\tBCE:174.6199\tKLD:0.0021\tC_loss:0.0000\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 169.451553\tBCE:169.4497\tKLD:0.0019\tC_loss:0.0000\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 172.800003\tBCE:172.7981\tKLD:0.0019\tC_loss:0.0000\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 170.003357\tBCE:170.0014\tKLD:0.0019\tC_loss:0.0000\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 170.676208\tBCE:170.6744\tKLD:0.0018\tC_loss:0.0000\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 171.451004\tBCE:171.4495\tKLD:0.0015\tC_loss:0.0000\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 172.425262\tBCE:172.4238\tKLD:0.0014\tC_loss:0.0000\n",
      "====> Epoch: 12 Average loss: 173.2087\tClassifier Accuracy: 98.7046\n",
      "====> Test set loss: 171.8293\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 176.115829\tBCE:176.1144\tKLD:0.0015\tC_loss:0.0000\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 172.802429\tBCE:172.8009\tKLD:0.0015\tC_loss:0.0000\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 174.231644\tBCE:174.2301\tKLD:0.0015\tC_loss:0.0000\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 173.116241\tBCE:173.1146\tKLD:0.0016\tC_loss:0.0000\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 172.802734\tBCE:172.8011\tKLD:0.0016\tC_loss:0.0000\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 171.658554\tBCE:171.6568\tKLD:0.0017\tC_loss:0.0000\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 169.506577\tBCE:169.5046\tKLD:0.0019\tC_loss:0.0000\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 175.629150\tBCE:175.6271\tKLD:0.0021\tC_loss:0.0000\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 173.854355\tBCE:173.8523\tKLD:0.0021\tC_loss:0.0000\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 171.378967\tBCE:171.3771\tKLD:0.0018\tC_loss:0.0000\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 171.353241\tBCE:171.3517\tKLD:0.0016\tC_loss:0.0000\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 176.499496\tBCE:176.4979\tKLD:0.0016\tC_loss:0.0000\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 173.392639\tBCE:173.3910\tKLD:0.0016\tC_loss:0.0000\n",
      "Train Epoch: 13 [33280/60000 (56%)]\tLoss: 175.926910\tBCE:175.9252\tKLD:0.0017\tC_loss:0.0000\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 174.520142\tBCE:174.5184\tKLD:0.0017\tC_loss:0.0000\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 178.787048\tBCE:178.7855\tKLD:0.0016\tC_loss:0.0000\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 172.644104\tBCE:172.6427\tKLD:0.0014\tC_loss:0.0000\n",
      "Train Epoch: 13 [43520/60000 (73%)]\tLoss: 176.105881\tBCE:176.1045\tKLD:0.0014\tC_loss:0.0000\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 176.778519\tBCE:176.7770\tKLD:0.0015\tC_loss:0.0000\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 175.502533\tBCE:175.5011\tKLD:0.0014\tC_loss:0.0000\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 174.509750\tBCE:174.5084\tKLD:0.0013\tC_loss:0.0000\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 169.088654\tBCE:169.0873\tKLD:0.0013\tC_loss:0.0000\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 176.703766\tBCE:176.7022\tKLD:0.0015\tC_loss:0.0000\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 172.795746\tBCE:172.7941\tKLD:0.0017\tC_loss:0.0000\n",
      "====> Epoch: 13 Average loss: 173.1749\tClassifier Accuracy: 98.8932\n",
      "====> Test set loss: 171.7304\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 171.280289\tBCE:171.2785\tKLD:0.0017\tC_loss:0.0000\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 172.043930\tBCE:172.0421\tKLD:0.0018\tC_loss:0.0000\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 171.202637\tBCE:171.2009\tKLD:0.0017\tC_loss:0.0001\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 176.743011\tBCE:176.7413\tKLD:0.0017\tC_loss:0.0000\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 171.766876\tBCE:171.7653\tKLD:0.0015\tC_loss:0.0000\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 176.868958\tBCE:176.8674\tKLD:0.0015\tC_loss:0.0000\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 172.563904\tBCE:172.5623\tKLD:0.0015\tC_loss:0.0000\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 171.274216\tBCE:171.2728\tKLD:0.0014\tC_loss:0.0000\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 169.349426\tBCE:169.3480\tKLD:0.0015\tC_loss:0.0000\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 174.931580\tBCE:174.9299\tKLD:0.0016\tC_loss:0.0000\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 171.085159\tBCE:171.0836\tKLD:0.0015\tC_loss:0.0000\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 177.442734\tBCE:177.4414\tKLD:0.0013\tC_loss:0.0000\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 173.297745\tBCE:173.2966\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 14 [33280/60000 (56%)]\tLoss: 175.139542\tBCE:175.1385\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 174.484634\tBCE:174.4834\tKLD:0.0012\tC_loss:0.0000\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 176.914978\tBCE:176.9136\tKLD:0.0013\tC_loss:0.0000\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 177.237640\tBCE:177.2363\tKLD:0.0013\tC_loss:0.0000\n",
      "Train Epoch: 14 [43520/60000 (73%)]\tLoss: 170.570908\tBCE:170.5696\tKLD:0.0013\tC_loss:0.0000\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 178.721512\tBCE:178.7202\tKLD:0.0013\tC_loss:0.0000\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 174.485901\tBCE:174.4846\tKLD:0.0013\tC_loss:0.0000\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 176.870056\tBCE:176.8688\tKLD:0.0012\tC_loss:0.0000\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 178.735596\tBCE:178.7344\tKLD:0.0012\tC_loss:0.0000\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 172.061279\tBCE:172.0600\tKLD:0.0012\tC_loss:0.0000\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 179.534637\tBCE:179.5334\tKLD:0.0012\tC_loss:0.0000\n",
      "====> Epoch: 14 Average loss: 173.1474\tClassifier Accuracy: 99.0268\n",
      "====> Test set loss: 171.6640\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 175.175735\tBCE:175.1745\tKLD:0.0012\tC_loss:0.0000\n",
      "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 176.188980\tBCE:176.1879\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 172.011658\tBCE:172.0105\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 173.370895\tBCE:173.3695\tKLD:0.0014\tC_loss:0.0000\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 175.236832\tBCE:175.2354\tKLD:0.0015\tC_loss:0.0000\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 171.937347\tBCE:171.9359\tKLD:0.0014\tC_loss:0.0000\n",
      "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 178.028793\tBCE:178.0275\tKLD:0.0013\tC_loss:0.0000\n",
      "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 169.129196\tBCE:169.1278\tKLD:0.0013\tC_loss:0.0000\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 175.527878\tBCE:175.5265\tKLD:0.0014\tC_loss:0.0000\n",
      "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 170.144531\tBCE:170.1431\tKLD:0.0014\tC_loss:0.0000\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 175.250610\tBCE:175.2493\tKLD:0.0012\tC_loss:0.0000\n",
      "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 177.361237\tBCE:177.3601\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 171.539139\tBCE:171.5381\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 15 [33280/60000 (56%)]\tLoss: 178.192596\tBCE:178.1916\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 171.682037\tBCE:171.6809\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 174.866028\tBCE:174.8647\tKLD:0.0013\tC_loss:0.0000\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 169.552200\tBCE:169.5509\tKLD:0.0013\tC_loss:0.0000\n",
      "Train Epoch: 15 [43520/60000 (73%)]\tLoss: 175.446808\tBCE:175.4456\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 175.754974\tBCE:175.7538\tKLD:0.0012\tC_loss:0.0000\n",
      "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 173.019257\tBCE:173.0181\tKLD:0.0012\tC_loss:0.0000\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 169.263168\tBCE:169.2620\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 15 [53760/60000 (90%)]\tLoss: 174.968246\tBCE:174.9671\tKLD:0.0012\tC_loss:0.0000\n",
      "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 170.870468\tBCE:170.8692\tKLD:0.0013\tC_loss:0.0000\n",
      "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 174.972092\tBCE:174.9707\tKLD:0.0013\tC_loss:0.0000\n",
      "====> Epoch: 15 Average loss: 173.1751\tClassifier Accuracy: 99.1169\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 171.6046\n",
      "Random number: 5\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 174.460815\tBCE:174.4595\tKLD:0.0013\tC_loss:0.0000\n",
      "Train Epoch: 16 [2560/60000 (4%)]\tLoss: 177.464401\tBCE:177.4633\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 16 [5120/60000 (9%)]\tLoss: 173.756165\tBCE:173.7551\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 16 [7680/60000 (13%)]\tLoss: 174.634491\tBCE:174.6334\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 171.322922\tBCE:171.3218\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 175.282654\tBCE:175.2816\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 16 [15360/60000 (26%)]\tLoss: 174.556183\tBCE:174.5551\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 16 [17920/60000 (30%)]\tLoss: 174.879044\tBCE:174.8779\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 170.732513\tBCE:170.7314\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 16 [23040/60000 (38%)]\tLoss: 168.033157\tBCE:168.0320\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 177.785187\tBCE:177.7840\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 16 [28160/60000 (47%)]\tLoss: 174.838516\tBCE:174.8374\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 179.128647\tBCE:179.1277\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 16 [33280/60000 (56%)]\tLoss: 174.398010\tBCE:174.3971\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 16 [35840/60000 (60%)]\tLoss: 175.017105\tBCE:175.0161\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 177.549026\tBCE:177.5480\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 169.515396\tBCE:169.5143\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 16 [43520/60000 (73%)]\tLoss: 166.998184\tBCE:166.9970\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 16 [46080/60000 (77%)]\tLoss: 174.475143\tBCE:174.4741\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 16 [48640/60000 (81%)]\tLoss: 179.245453\tBCE:179.2445\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 178.003128\tBCE:178.0023\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 16 [53760/60000 (90%)]\tLoss: 178.835831\tBCE:178.8349\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 16 [56320/60000 (94%)]\tLoss: 172.882126\tBCE:172.8813\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 16 [58880/60000 (98%)]\tLoss: 172.691483\tBCE:172.6905\tKLD:0.0009\tC_loss:0.0000\n",
      "====> Epoch: 16 Average loss: 173.1303\tClassifier Accuracy: 99.2455\n",
      "====> Test set loss: 172.1312\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 174.091583\tBCE:174.0906\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 17 [2560/60000 (4%)]\tLoss: 171.305008\tBCE:171.3039\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 17 [5120/60000 (9%)]\tLoss: 174.246902\tBCE:174.2458\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 17 [7680/60000 (13%)]\tLoss: 175.614746\tBCE:175.6137\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 175.457001\tBCE:175.4560\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 175.123215\tBCE:175.1222\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 17 [15360/60000 (26%)]\tLoss: 175.445328\tBCE:175.4444\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 17 [17920/60000 (30%)]\tLoss: 168.479996\tBCE:168.4790\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 176.355209\tBCE:176.3542\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 17 [23040/60000 (38%)]\tLoss: 171.113190\tBCE:171.1121\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 167.257370\tBCE:167.2563\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 17 [28160/60000 (47%)]\tLoss: 174.428299\tBCE:174.4274\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 168.759079\tBCE:168.7582\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 17 [33280/60000 (56%)]\tLoss: 174.948624\tBCE:174.9476\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 17 [35840/60000 (60%)]\tLoss: 172.360901\tBCE:172.3599\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 172.604065\tBCE:172.6030\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 174.230621\tBCE:174.2294\tKLD:0.0012\tC_loss:0.0000\n",
      "Train Epoch: 17 [43520/60000 (73%)]\tLoss: 173.290131\tBCE:173.2888\tKLD:0.0013\tC_loss:0.0000\n",
      "Train Epoch: 17 [46080/60000 (77%)]\tLoss: 179.115265\tBCE:179.1141\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 17 [48640/60000 (81%)]\tLoss: 175.387955\tBCE:175.3869\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 175.960037\tBCE:175.9590\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 17 [53760/60000 (90%)]\tLoss: 170.487823\tBCE:170.4867\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 17 [56320/60000 (94%)]\tLoss: 171.846909\tBCE:171.8459\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 17 [58880/60000 (98%)]\tLoss: 174.141739\tBCE:174.1408\tKLD:0.0009\tC_loss:0.0000\n",
      "====> Epoch: 17 Average loss: 173.1167\tClassifier Accuracy: 99.3289\n",
      "====> Test set loss: 171.8001\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 171.048615\tBCE:171.0477\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 18 [2560/60000 (4%)]\tLoss: 171.761536\tBCE:171.7607\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 18 [5120/60000 (9%)]\tLoss: 170.416733\tBCE:170.4160\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 18 [7680/60000 (13%)]\tLoss: 170.499023\tBCE:170.4981\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 173.726624\tBCE:173.7257\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 171.521118\tBCE:171.5200\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 18 [15360/60000 (26%)]\tLoss: 169.195206\tBCE:169.1940\tKLD:0.0012\tC_loss:0.0000\n",
      "Train Epoch: 18 [17920/60000 (30%)]\tLoss: 172.962067\tBCE:172.9610\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 171.468842\tBCE:171.4679\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 18 [23040/60000 (38%)]\tLoss: 170.296524\tBCE:170.2957\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 176.527176\tBCE:176.5264\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 18 [28160/60000 (47%)]\tLoss: 175.478241\tBCE:175.4774\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 176.214813\tBCE:176.2139\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 18 [33280/60000 (56%)]\tLoss: 174.956116\tBCE:174.9552\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 18 [35840/60000 (60%)]\tLoss: 171.913086\tBCE:171.9123\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 169.745270\tBCE:169.7444\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 172.325775\tBCE:172.3249\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 18 [43520/60000 (73%)]\tLoss: 178.208618\tBCE:178.2077\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 18 [46080/60000 (77%)]\tLoss: 174.306107\tBCE:174.3052\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 18 [48640/60000 (81%)]\tLoss: 173.986710\tBCE:173.9859\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 167.289185\tBCE:167.2884\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 18 [53760/60000 (90%)]\tLoss: 175.881073\tBCE:175.8803\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 18 [56320/60000 (94%)]\tLoss: 174.908829\tBCE:174.9081\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 18 [58880/60000 (98%)]\tLoss: 178.063293\tBCE:178.0627\tKLD:0.0006\tC_loss:0.0000\n",
      "====> Epoch: 18 Average loss: 173.0564\tClassifier Accuracy: 99.4207\n",
      "====> Test set loss: 171.7274\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 176.003052\tBCE:176.0025\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 19 [2560/60000 (4%)]\tLoss: 172.988144\tBCE:172.9876\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 19 [5120/60000 (9%)]\tLoss: 176.475342\tBCE:176.4746\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 19 [7680/60000 (13%)]\tLoss: 174.080978\tBCE:174.0800\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 174.394318\tBCE:174.3932\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 172.143356\tBCE:172.1422\tKLD:0.0011\tC_loss:0.0000\n",
      "Train Epoch: 19 [15360/60000 (26%)]\tLoss: 171.087112\tBCE:171.0861\tKLD:0.0010\tC_loss:0.0000\n",
      "Train Epoch: 19 [17920/60000 (30%)]\tLoss: 172.940308\tBCE:172.9394\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 168.452087\tBCE:168.4512\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 19 [23040/60000 (38%)]\tLoss: 177.416000\tBCE:177.4151\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 169.291367\tBCE:169.2906\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 19 [28160/60000 (47%)]\tLoss: 176.087448\tBCE:176.0868\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 171.193207\tBCE:171.1925\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 19 [33280/60000 (56%)]\tLoss: 174.382980\tBCE:174.3822\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 19 [35840/60000 (60%)]\tLoss: 172.793457\tBCE:172.7927\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 170.667511\tBCE:170.6669\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 173.595749\tBCE:173.5950\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 19 [43520/60000 (73%)]\tLoss: 174.872833\tBCE:174.8721\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 19 [46080/60000 (77%)]\tLoss: 172.133453\tBCE:172.1327\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 19 [48640/60000 (81%)]\tLoss: 169.045578\tBCE:169.0449\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 172.005981\tBCE:172.0053\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 19 [53760/60000 (90%)]\tLoss: 173.823761\tBCE:173.8231\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 19 [56320/60000 (94%)]\tLoss: 178.997131\tBCE:178.9966\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 19 [58880/60000 (98%)]\tLoss: 177.447998\tBCE:177.4473\tKLD:0.0007\tC_loss:0.0000\n",
      "====> Epoch: 19 Average loss: 173.0870\tClassifier Accuracy: 99.5209\n",
      "====> Test set loss: 171.7560\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 179.041473\tBCE:179.0407\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 20 [2560/60000 (4%)]\tLoss: 167.254044\tBCE:167.2532\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 20 [5120/60000 (9%)]\tLoss: 174.179031\tBCE:174.1782\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 20 [7680/60000 (13%)]\tLoss: 175.724777\tBCE:175.7241\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 166.754837\tBCE:166.7542\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 175.296265\tBCE:175.2955\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 20 [15360/60000 (26%)]\tLoss: 178.387741\tBCE:178.3870\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 20 [17920/60000 (30%)]\tLoss: 171.401901\tBCE:171.4010\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 173.321518\tBCE:173.3206\tKLD:0.0009\tC_loss:0.0000\n",
      "Train Epoch: 20 [23040/60000 (38%)]\tLoss: 174.716675\tBCE:174.7159\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 171.430283\tBCE:171.4295\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 20 [28160/60000 (47%)]\tLoss: 175.775070\tBCE:175.7744\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 166.643585\tBCE:166.6429\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 20 [33280/60000 (56%)]\tLoss: 165.747498\tBCE:165.7468\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 20 [35840/60000 (60%)]\tLoss: 171.151520\tBCE:171.1508\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 173.599365\tBCE:173.5988\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 179.847656\tBCE:179.8470\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 20 [43520/60000 (73%)]\tLoss: 170.451599\tBCE:170.4509\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 20 [46080/60000 (77%)]\tLoss: 173.390762\tBCE:173.3901\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 20 [48640/60000 (81%)]\tLoss: 170.280365\tBCE:170.2798\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 174.654144\tBCE:174.6536\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 20 [53760/60000 (90%)]\tLoss: 176.615143\tBCE:176.6147\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 20 [56320/60000 (94%)]\tLoss: 177.111282\tBCE:177.1107\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 20 [58880/60000 (98%)]\tLoss: 172.082352\tBCE:172.0817\tKLD:0.0007\tC_loss:0.0000\n",
      "====> Epoch: 20 Average loss: 173.0964\tClassifier Accuracy: 99.5459\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 171.7086\n",
      "Random number: 1\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 172.241501\tBCE:172.2408\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 21 [2560/60000 (4%)]\tLoss: 174.170059\tBCE:174.1694\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 21 [5120/60000 (9%)]\tLoss: 173.796356\tBCE:173.7957\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 21 [7680/60000 (13%)]\tLoss: 172.319321\tBCE:172.3186\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 21 [10240/60000 (17%)]\tLoss: 171.957199\tBCE:171.9564\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 178.721115\tBCE:178.7204\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 21 [15360/60000 (26%)]\tLoss: 174.252075\tBCE:174.2514\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 21 [17920/60000 (30%)]\tLoss: 168.682892\tBCE:168.6822\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 21 [20480/60000 (34%)]\tLoss: 169.902924\tBCE:169.9021\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 21 [23040/60000 (38%)]\tLoss: 170.006012\tBCE:170.0052\tKLD:0.0008\tC_loss:0.0000\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 176.060623\tBCE:176.0599\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 21 [28160/60000 (47%)]\tLoss: 175.482925\tBCE:175.4822\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 21 [30720/60000 (51%)]\tLoss: 170.798523\tBCE:170.7979\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 21 [33280/60000 (56%)]\tLoss: 175.982117\tBCE:175.9816\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 21 [35840/60000 (60%)]\tLoss: 170.663406\tBCE:170.6629\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 171.081833\tBCE:171.0813\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 21 [40960/60000 (68%)]\tLoss: 173.707062\tBCE:173.7065\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 21 [43520/60000 (73%)]\tLoss: 171.770416\tBCE:171.7699\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 21 [46080/60000 (77%)]\tLoss: 180.332962\tBCE:180.3324\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 21 [48640/60000 (81%)]\tLoss: 174.604507\tBCE:174.6039\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 175.405441\tBCE:175.4048\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 21 [53760/60000 (90%)]\tLoss: 171.955261\tBCE:171.9547\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 21 [56320/60000 (94%)]\tLoss: 177.594315\tBCE:177.5936\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 21 [58880/60000 (98%)]\tLoss: 170.731140\tBCE:170.7305\tKLD:0.0006\tC_loss:0.0000\n",
      "====> Epoch: 21 Average loss: 173.0505\tClassifier Accuracy: 99.6461\n",
      "====> Test set loss: 171.6861\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 168.245224\tBCE:168.2446\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 22 [2560/60000 (4%)]\tLoss: 175.552475\tBCE:175.5519\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 22 [5120/60000 (9%)]\tLoss: 177.447357\tBCE:177.4468\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 22 [7680/60000 (13%)]\tLoss: 176.603622\tBCE:176.6030\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 22 [10240/60000 (17%)]\tLoss: 170.641296\tBCE:170.6406\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 174.535339\tBCE:174.5347\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 22 [15360/60000 (26%)]\tLoss: 173.571609\tBCE:173.5709\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 22 [17920/60000 (30%)]\tLoss: 168.039261\tBCE:168.0386\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 22 [20480/60000 (34%)]\tLoss: 177.099701\tBCE:177.0990\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 22 [23040/60000 (38%)]\tLoss: 171.683548\tBCE:171.6830\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 172.518738\tBCE:172.5181\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 22 [28160/60000 (47%)]\tLoss: 170.402115\tBCE:170.4015\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 22 [30720/60000 (51%)]\tLoss: 171.434250\tBCE:171.4336\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 22 [33280/60000 (56%)]\tLoss: 176.321411\tBCE:176.3209\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 22 [35840/60000 (60%)]\tLoss: 170.900482\tBCE:170.8999\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 171.447235\tBCE:171.4467\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 22 [40960/60000 (68%)]\tLoss: 176.483871\tBCE:176.4833\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 22 [43520/60000 (73%)]\tLoss: 171.363297\tBCE:171.3627\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 22 [46080/60000 (77%)]\tLoss: 176.234329\tBCE:176.2337\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 22 [48640/60000 (81%)]\tLoss: 174.311630\tBCE:174.3111\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 169.753494\tBCE:169.7529\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 22 [53760/60000 (90%)]\tLoss: 173.745361\tBCE:173.7448\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 22 [56320/60000 (94%)]\tLoss: 174.181610\tBCE:174.1811\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 22 [58880/60000 (98%)]\tLoss: 170.641830\tBCE:170.6413\tKLD:0.0005\tC_loss:0.0000\n",
      "====> Epoch: 22 Average loss: 173.0604\tClassifier Accuracy: 99.6895\n",
      "====> Test set loss: 171.7333\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 172.848862\tBCE:172.8484\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 23 [2560/60000 (4%)]\tLoss: 171.267639\tBCE:171.2671\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 23 [5120/60000 (9%)]\tLoss: 175.300018\tBCE:175.2995\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 23 [7680/60000 (13%)]\tLoss: 173.300476\tBCE:173.2999\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 23 [10240/60000 (17%)]\tLoss: 174.503113\tBCE:174.5027\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 171.578690\tBCE:171.5783\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 23 [15360/60000 (26%)]\tLoss: 168.882126\tBCE:168.8817\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 23 [17920/60000 (30%)]\tLoss: 179.912033\tBCE:179.9116\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 23 [20480/60000 (34%)]\tLoss: 173.804352\tBCE:173.8038\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 23 [23040/60000 (38%)]\tLoss: 177.402802\tBCE:177.4023\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 175.369629\tBCE:175.3690\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 23 [28160/60000 (47%)]\tLoss: 173.860809\tBCE:173.8601\tKLD:0.0007\tC_loss:0.0000\n",
      "Train Epoch: 23 [30720/60000 (51%)]\tLoss: 178.436142\tBCE:178.4356\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 23 [33280/60000 (56%)]\tLoss: 172.937042\tBCE:172.9366\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 23 [35840/60000 (60%)]\tLoss: 176.926514\tBCE:176.9260\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 169.752106\tBCE:169.7515\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 23 [40960/60000 (68%)]\tLoss: 168.339508\tBCE:168.3390\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 23 [43520/60000 (73%)]\tLoss: 174.023041\tBCE:174.0225\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 23 [46080/60000 (77%)]\tLoss: 169.097946\tBCE:169.0974\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 23 [48640/60000 (81%)]\tLoss: 175.879700\tBCE:175.8793\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 173.610962\tBCE:173.6106\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 23 [53760/60000 (90%)]\tLoss: 170.843338\tBCE:170.8430\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 23 [56320/60000 (94%)]\tLoss: 172.790985\tBCE:172.7906\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 23 [58880/60000 (98%)]\tLoss: 173.211502\tBCE:173.2111\tKLD:0.0004\tC_loss:0.0000\n",
      "====> Epoch: 23 Average loss: 173.0047\tClassifier Accuracy: 99.7346\n",
      "====> Test set loss: 171.8159\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 175.110748\tBCE:175.1103\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 24 [2560/60000 (4%)]\tLoss: 173.063248\tBCE:173.0628\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 24 [5120/60000 (9%)]\tLoss: 165.592331\tBCE:165.5919\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 24 [7680/60000 (13%)]\tLoss: 175.124039\tBCE:175.1235\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 24 [10240/60000 (17%)]\tLoss: 164.952667\tBCE:164.9522\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 181.968857\tBCE:181.9684\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 24 [15360/60000 (26%)]\tLoss: 176.342072\tBCE:176.3415\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 24 [17920/60000 (30%)]\tLoss: 172.267273\tBCE:172.2668\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 24 [20480/60000 (34%)]\tLoss: 177.581146\tBCE:177.5806\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 24 [23040/60000 (38%)]\tLoss: 174.719452\tBCE:174.7189\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 175.776932\tBCE:175.7763\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 24 [28160/60000 (47%)]\tLoss: 172.546387\tBCE:172.5459\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 24 [30720/60000 (51%)]\tLoss: 174.513016\tBCE:174.5126\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 24 [33280/60000 (56%)]\tLoss: 170.923843\tBCE:170.9233\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 24 [35840/60000 (60%)]\tLoss: 174.104935\tBCE:174.1044\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 172.569458\tBCE:172.5689\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 24 [40960/60000 (68%)]\tLoss: 175.821106\tBCE:175.8206\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 24 [43520/60000 (73%)]\tLoss: 173.693588\tBCE:173.6932\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 24 [46080/60000 (77%)]\tLoss: 171.532288\tBCE:171.5319\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 24 [48640/60000 (81%)]\tLoss: 170.701736\tBCE:170.7013\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 172.173096\tBCE:172.1725\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 24 [53760/60000 (90%)]\tLoss: 169.183075\tBCE:169.1825\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 24 [56320/60000 (94%)]\tLoss: 170.057175\tBCE:170.0566\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 24 [58880/60000 (98%)]\tLoss: 172.175125\tBCE:172.1746\tKLD:0.0006\tC_loss:0.0000\n",
      "====> Epoch: 24 Average loss: 173.0205\tClassifier Accuracy: 99.7596\n",
      "====> Test set loss: 171.4677\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 177.830261\tBCE:177.8297\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 25 [2560/60000 (4%)]\tLoss: 174.023575\tBCE:174.0232\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 25 [5120/60000 (9%)]\tLoss: 173.564041\tBCE:173.5637\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 25 [7680/60000 (13%)]\tLoss: 171.248566\tBCE:171.2481\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 25 [10240/60000 (17%)]\tLoss: 176.965561\tBCE:176.9651\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 172.502289\tBCE:172.5018\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 25 [15360/60000 (26%)]\tLoss: 175.349945\tBCE:175.3495\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 25 [17920/60000 (30%)]\tLoss: 174.906296\tBCE:174.9059\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 25 [20480/60000 (34%)]\tLoss: 165.124954\tBCE:165.1245\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 25 [23040/60000 (38%)]\tLoss: 176.776276\tBCE:176.7758\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 173.010895\tBCE:173.0104\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 25 [28160/60000 (47%)]\tLoss: 176.864685\tBCE:176.8642\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 25 [30720/60000 (51%)]\tLoss: 173.643539\tBCE:173.6430\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 25 [33280/60000 (56%)]\tLoss: 173.583221\tBCE:173.5827\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 25 [35840/60000 (60%)]\tLoss: 172.221237\tBCE:172.2207\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 172.960907\tBCE:172.9604\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 25 [40960/60000 (68%)]\tLoss: 172.190445\tBCE:172.1900\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 25 [43520/60000 (73%)]\tLoss: 175.357086\tBCE:175.3567\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 25 [46080/60000 (77%)]\tLoss: 171.898849\tBCE:171.8985\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 25 [48640/60000 (81%)]\tLoss: 172.760468\tBCE:172.7600\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 179.292969\tBCE:179.2925\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 25 [53760/60000 (90%)]\tLoss: 175.950134\tBCE:175.9496\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 25 [56320/60000 (94%)]\tLoss: 169.046692\tBCE:169.0462\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 25 [58880/60000 (98%)]\tLoss: 172.741653\tBCE:172.7412\tKLD:0.0004\tC_loss:0.0000\n",
      "====> Epoch: 25 Average loss: 172.9916\tClassifier Accuracy: 99.8214\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 171.8310\n",
      "Random number: 4\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 174.200562\tBCE:174.2001\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 26 [2560/60000 (4%)]\tLoss: 169.479141\tBCE:169.4787\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 26 [5120/60000 (9%)]\tLoss: 176.094116\tBCE:176.0938\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 26 [7680/60000 (13%)]\tLoss: 166.751068\tBCE:166.7507\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 26 [10240/60000 (17%)]\tLoss: 171.323761\tBCE:171.3234\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 172.149490\tBCE:172.1491\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 26 [15360/60000 (26%)]\tLoss: 169.232147\tBCE:169.2318\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 26 [17920/60000 (30%)]\tLoss: 176.886673\tBCE:176.8863\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 26 [20480/60000 (34%)]\tLoss: 173.357666\tBCE:173.3573\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 26 [23040/60000 (38%)]\tLoss: 177.391144\tBCE:177.3905\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 178.573166\tBCE:178.5726\tKLD:0.0006\tC_loss:0.0000\n",
      "Train Epoch: 26 [28160/60000 (47%)]\tLoss: 173.493866\tBCE:173.4934\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 26 [30720/60000 (51%)]\tLoss: 171.347198\tBCE:171.3468\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 26 [33280/60000 (56%)]\tLoss: 171.571533\tBCE:171.5712\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 26 [35840/60000 (60%)]\tLoss: 173.676544\tBCE:173.6761\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 169.896225\tBCE:169.8959\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 26 [40960/60000 (68%)]\tLoss: 172.978088\tBCE:172.9777\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 26 [43520/60000 (73%)]\tLoss: 172.907486\tBCE:172.9071\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 26 [46080/60000 (77%)]\tLoss: 174.680084\tBCE:174.6797\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 26 [48640/60000 (81%)]\tLoss: 173.631165\tBCE:173.6309\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 173.935989\tBCE:173.9357\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 26 [53760/60000 (90%)]\tLoss: 171.829453\tBCE:171.8292\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 26 [56320/60000 (94%)]\tLoss: 177.804642\tBCE:177.8043\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 26 [58880/60000 (98%)]\tLoss: 176.473953\tBCE:176.4737\tKLD:0.0002\tC_loss:0.0000\n",
      "====> Epoch: 26 Average loss: 172.9756\tClassifier Accuracy: 99.8431\n",
      "====> Test set loss: 171.5373\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 168.441910\tBCE:168.4417\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 27 [2560/60000 (4%)]\tLoss: 170.010941\tBCE:170.0107\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 27 [5120/60000 (9%)]\tLoss: 175.903336\tBCE:175.9030\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 27 [7680/60000 (13%)]\tLoss: 175.333939\tBCE:175.3335\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 27 [10240/60000 (17%)]\tLoss: 174.062698\tBCE:174.0622\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 172.207367\tBCE:172.2069\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 27 [15360/60000 (26%)]\tLoss: 175.956635\tBCE:175.9562\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 27 [17920/60000 (30%)]\tLoss: 180.609329\tBCE:180.6090\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 27 [20480/60000 (34%)]\tLoss: 175.033173\tBCE:175.0329\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 27 [23040/60000 (38%)]\tLoss: 175.637909\tBCE:175.6376\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 175.635345\tBCE:175.6349\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 27 [28160/60000 (47%)]\tLoss: 173.503784\tBCE:173.5034\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 27 [30720/60000 (51%)]\tLoss: 170.952362\tBCE:170.9520\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 27 [33280/60000 (56%)]\tLoss: 180.020111\tBCE:180.0198\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 27 [35840/60000 (60%)]\tLoss: 177.809647\tBCE:177.8093\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 175.321671\tBCE:175.3214\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 27 [40960/60000 (68%)]\tLoss: 174.724030\tBCE:174.7237\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 27 [43520/60000 (73%)]\tLoss: 173.075089\tBCE:173.0747\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 27 [46080/60000 (77%)]\tLoss: 169.625305\tBCE:169.6248\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 27 [48640/60000 (81%)]\tLoss: 174.939194\tBCE:174.9387\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 171.662201\tBCE:171.6618\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 27 [53760/60000 (90%)]\tLoss: 175.757889\tBCE:175.7575\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 27 [56320/60000 (94%)]\tLoss: 173.272675\tBCE:173.2724\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 27 [58880/60000 (98%)]\tLoss: 172.913879\tBCE:172.9135\tKLD:0.0004\tC_loss:0.0000\n",
      "====> Epoch: 27 Average loss: 172.9830\tClassifier Accuracy: 99.8464\n",
      "====> Test set loss: 171.5016\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 173.483276\tBCE:173.4829\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 28 [2560/60000 (4%)]\tLoss: 172.101929\tBCE:172.1015\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 28 [5120/60000 (9%)]\tLoss: 173.385040\tBCE:173.3846\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 28 [7680/60000 (13%)]\tLoss: 166.633926\tBCE:166.6336\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 28 [10240/60000 (17%)]\tLoss: 173.663147\tBCE:173.6627\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 172.303284\tBCE:172.3028\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 28 [15360/60000 (26%)]\tLoss: 173.390060\tBCE:173.3897\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 28 [17920/60000 (30%)]\tLoss: 177.424225\tBCE:177.4239\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 28 [20480/60000 (34%)]\tLoss: 176.718338\tBCE:176.7181\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 28 [23040/60000 (38%)]\tLoss: 175.806320\tBCE:175.8061\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 179.489639\tBCE:179.4894\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 28 [28160/60000 (47%)]\tLoss: 175.083466\tBCE:175.0832\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 28 [30720/60000 (51%)]\tLoss: 177.246597\tBCE:177.2463\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 28 [33280/60000 (56%)]\tLoss: 172.697739\tBCE:172.6974\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 28 [35840/60000 (60%)]\tLoss: 177.364716\tBCE:177.3643\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 173.022171\tBCE:173.0219\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 28 [40960/60000 (68%)]\tLoss: 171.141006\tBCE:171.1407\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 28 [43520/60000 (73%)]\tLoss: 171.965134\tBCE:171.9648\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 28 [46080/60000 (77%)]\tLoss: 178.491348\tBCE:178.4910\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 28 [48640/60000 (81%)]\tLoss: 177.264877\tBCE:177.2645\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 167.972870\tBCE:167.9724\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 28 [53760/60000 (90%)]\tLoss: 176.943283\tBCE:176.9429\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 28 [56320/60000 (94%)]\tLoss: 174.976135\tBCE:174.9756\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 28 [58880/60000 (98%)]\tLoss: 171.165634\tBCE:171.1652\tKLD:0.0004\tC_loss:0.0000\n",
      "====> Epoch: 28 Average loss: 172.9909\tClassifier Accuracy: 99.8831\n",
      "====> Test set loss: 171.6144\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 170.566391\tBCE:170.5660\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 29 [2560/60000 (4%)]\tLoss: 173.302689\tBCE:173.3023\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 29 [5120/60000 (9%)]\tLoss: 175.829727\tBCE:175.8294\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 29 [7680/60000 (13%)]\tLoss: 172.842178\tBCE:172.8419\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 29 [10240/60000 (17%)]\tLoss: 172.923859\tBCE:172.9235\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 172.814468\tBCE:172.8142\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 29 [15360/60000 (26%)]\tLoss: 172.678238\tBCE:172.6780\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 29 [17920/60000 (30%)]\tLoss: 170.475403\tBCE:170.4750\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 29 [20480/60000 (34%)]\tLoss: 172.737350\tBCE:172.7369\tKLD:0.0005\tC_loss:0.0000\n",
      "Train Epoch: 29 [23040/60000 (38%)]\tLoss: 174.291412\tBCE:174.2910\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 170.407516\tBCE:170.4071\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 29 [28160/60000 (47%)]\tLoss: 176.122345\tBCE:176.1220\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 29 [30720/60000 (51%)]\tLoss: 167.623657\tBCE:167.6234\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 29 [33280/60000 (56%)]\tLoss: 173.544144\tBCE:173.5438\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 29 [35840/60000 (60%)]\tLoss: 176.389252\tBCE:176.3889\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 169.725342\tBCE:169.7251\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 29 [40960/60000 (68%)]\tLoss: 174.522232\tBCE:174.5220\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 29 [43520/60000 (73%)]\tLoss: 172.342850\tBCE:172.3426\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 29 [46080/60000 (77%)]\tLoss: 175.095963\tBCE:175.0957\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 29 [48640/60000 (81%)]\tLoss: 173.037659\tBCE:173.0374\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 173.339706\tBCE:173.3394\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 29 [53760/60000 (90%)]\tLoss: 172.990723\tBCE:172.9904\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 29 [56320/60000 (94%)]\tLoss: 167.216934\tBCE:167.2167\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 29 [58880/60000 (98%)]\tLoss: 174.926590\tBCE:174.9263\tKLD:0.0002\tC_loss:0.0000\n",
      "====> Epoch: 29 Average loss: 172.9665\tClassifier Accuracy: 99.9282\n",
      "====> Test set loss: 171.5720\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 176.061295\tBCE:176.0611\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 30 [2560/60000 (4%)]\tLoss: 175.825836\tBCE:175.8256\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 30 [5120/60000 (9%)]\tLoss: 172.961517\tBCE:172.9612\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 30 [7680/60000 (13%)]\tLoss: 173.892593\tBCE:173.8923\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 30 [10240/60000 (17%)]\tLoss: 176.465790\tBCE:176.4655\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 171.879318\tBCE:171.8791\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 30 [15360/60000 (26%)]\tLoss: 173.794495\tBCE:173.7942\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 30 [17920/60000 (30%)]\tLoss: 174.335526\tBCE:174.3352\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 30 [20480/60000 (34%)]\tLoss: 169.053543\tBCE:169.0533\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 30 [23040/60000 (38%)]\tLoss: 170.311386\tBCE:170.3111\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 171.988434\tBCE:171.9882\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 30 [28160/60000 (47%)]\tLoss: 174.088348\tBCE:174.0879\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 30 [30720/60000 (51%)]\tLoss: 173.618790\tBCE:173.6185\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 30 [33280/60000 (56%)]\tLoss: 174.402557\tBCE:174.4023\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 30 [35840/60000 (60%)]\tLoss: 176.987869\tBCE:176.9876\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 179.264877\tBCE:179.2645\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 30 [40960/60000 (68%)]\tLoss: 175.874603\tBCE:175.8743\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 30 [43520/60000 (73%)]\tLoss: 177.942825\tBCE:177.9425\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 30 [46080/60000 (77%)]\tLoss: 176.559952\tBCE:176.5596\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 30 [48640/60000 (81%)]\tLoss: 176.564041\tBCE:176.5637\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 176.372360\tBCE:176.3720\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 30 [53760/60000 (90%)]\tLoss: 172.754623\tBCE:172.7543\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 30 [56320/60000 (94%)]\tLoss: 175.080444\tBCE:175.0800\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 30 [58880/60000 (98%)]\tLoss: 177.247910\tBCE:177.2474\tKLD:0.0005\tC_loss:0.0000\n",
      "====> Epoch: 30 Average loss: 172.9531\tClassifier Accuracy: 99.9416\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 171.6401\n",
      "Random number: 7\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 175.604675\tBCE:175.6042\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 31 [2560/60000 (4%)]\tLoss: 172.365295\tBCE:172.3650\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 31 [5120/60000 (9%)]\tLoss: 175.906998\tBCE:175.9067\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 31 [7680/60000 (13%)]\tLoss: 167.678116\tBCE:167.6778\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 31 [10240/60000 (17%)]\tLoss: 172.777359\tBCE:172.7770\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 172.777084\tBCE:172.7768\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 31 [15360/60000 (26%)]\tLoss: 173.840073\tBCE:173.8398\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 31 [17920/60000 (30%)]\tLoss: 172.029816\tBCE:172.0295\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 31 [20480/60000 (34%)]\tLoss: 167.235016\tBCE:167.2347\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 31 [23040/60000 (38%)]\tLoss: 166.314240\tBCE:166.3139\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 171.687332\tBCE:171.6870\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 31 [28160/60000 (47%)]\tLoss: 171.657425\tBCE:171.6571\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 31 [30720/60000 (51%)]\tLoss: 176.641983\tBCE:176.6417\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 31 [33280/60000 (56%)]\tLoss: 178.869690\tBCE:178.8694\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 31 [35840/60000 (60%)]\tLoss: 173.129684\tBCE:173.1294\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 170.614029\tBCE:170.6138\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 31 [40960/60000 (68%)]\tLoss: 175.665543\tBCE:175.6653\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 31 [43520/60000 (73%)]\tLoss: 174.171555\tBCE:174.1713\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 31 [46080/60000 (77%)]\tLoss: 175.333542\tBCE:175.3333\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 31 [48640/60000 (81%)]\tLoss: 171.415817\tBCE:171.4155\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 175.329666\tBCE:175.3293\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 31 [53760/60000 (90%)]\tLoss: 172.578293\tBCE:172.5779\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 31 [56320/60000 (94%)]\tLoss: 178.002762\tBCE:178.0024\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 31 [58880/60000 (98%)]\tLoss: 169.505981\tBCE:169.5056\tKLD:0.0004\tC_loss:0.0000\n",
      "====> Epoch: 31 Average loss: 172.9550\tClassifier Accuracy: 99.9583\n",
      "====> Test set loss: 171.7710\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 175.729263\tBCE:175.7289\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 32 [2560/60000 (4%)]\tLoss: 178.914536\tBCE:178.9143\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 32 [5120/60000 (9%)]\tLoss: 174.008911\tBCE:174.0087\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 32 [7680/60000 (13%)]\tLoss: 177.427673\tBCE:177.4275\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 32 [10240/60000 (17%)]\tLoss: 177.328278\tBCE:177.3281\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 171.364700\tBCE:171.3645\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 32 [15360/60000 (26%)]\tLoss: 178.409943\tBCE:178.4098\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 32 [17920/60000 (30%)]\tLoss: 170.291595\tBCE:170.2914\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 32 [20480/60000 (34%)]\tLoss: 172.580368\tBCE:172.5802\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 32 [23040/60000 (38%)]\tLoss: 173.451904\tBCE:173.4516\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 174.148926\tBCE:174.1486\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 32 [28160/60000 (47%)]\tLoss: 174.609802\tBCE:174.6094\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 32 [30720/60000 (51%)]\tLoss: 175.464096\tBCE:175.4638\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 32 [33280/60000 (56%)]\tLoss: 172.047012\tBCE:172.0467\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 32 [35840/60000 (60%)]\tLoss: 172.065994\tBCE:172.0657\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 170.196152\tBCE:170.1959\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 32 [40960/60000 (68%)]\tLoss: 174.202194\tBCE:174.2020\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 32 [43520/60000 (73%)]\tLoss: 171.444855\tBCE:171.4447\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 32 [46080/60000 (77%)]\tLoss: 173.080093\tBCE:173.0798\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 32 [48640/60000 (81%)]\tLoss: 166.082916\tBCE:166.0827\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 174.230972\tBCE:174.2308\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 32 [53760/60000 (90%)]\tLoss: 171.176682\tBCE:171.1765\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 32 [56320/60000 (94%)]\tLoss: 176.069366\tBCE:176.0691\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 32 [58880/60000 (98%)]\tLoss: 175.525391\tBCE:175.5251\tKLD:0.0002\tC_loss:0.0000\n",
      "====> Epoch: 32 Average loss: 172.9174\tClassifier Accuracy: 99.9616\n",
      "====> Test set loss: 171.7857\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 171.593445\tBCE:171.5932\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 33 [2560/60000 (4%)]\tLoss: 173.804047\tBCE:173.8038\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 33 [5120/60000 (9%)]\tLoss: 173.394028\tBCE:173.3938\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 33 [7680/60000 (13%)]\tLoss: 174.183502\tBCE:174.1832\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 33 [10240/60000 (17%)]\tLoss: 182.553635\tBCE:182.5534\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 169.992126\tBCE:169.9919\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 33 [15360/60000 (26%)]\tLoss: 176.351257\tBCE:176.3509\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 33 [17920/60000 (30%)]\tLoss: 172.769882\tBCE:172.7695\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 33 [20480/60000 (34%)]\tLoss: 169.689941\tBCE:169.6896\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 33 [23040/60000 (38%)]\tLoss: 169.683426\tBCE:169.6832\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 172.087830\tBCE:172.0876\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 33 [28160/60000 (47%)]\tLoss: 173.540634\tBCE:173.5404\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 33 [30720/60000 (51%)]\tLoss: 172.604782\tBCE:172.6046\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 33 [33280/60000 (56%)]\tLoss: 173.420380\tBCE:173.4202\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 33 [35840/60000 (60%)]\tLoss: 168.552216\tBCE:168.5519\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 173.946243\tBCE:173.9460\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 33 [40960/60000 (68%)]\tLoss: 173.724686\tBCE:173.7244\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 33 [43520/60000 (73%)]\tLoss: 171.861099\tBCE:171.8608\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 33 [46080/60000 (77%)]\tLoss: 173.381332\tBCE:173.3810\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 33 [48640/60000 (81%)]\tLoss: 166.547638\tBCE:166.5473\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 173.463135\tBCE:173.4629\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 33 [53760/60000 (90%)]\tLoss: 176.500504\tBCE:176.5003\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 33 [56320/60000 (94%)]\tLoss: 177.563324\tBCE:177.5631\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 33 [58880/60000 (98%)]\tLoss: 171.592377\tBCE:171.5921\tKLD:0.0003\tC_loss:0.0000\n",
      "====> Epoch: 33 Average loss: 172.9249\tClassifier Accuracy: 99.9633\n",
      "====> Test set loss: 171.5943\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 174.158554\tBCE:174.1583\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 34 [2560/60000 (4%)]\tLoss: 171.678909\tBCE:171.6787\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 34 [5120/60000 (9%)]\tLoss: 174.688293\tBCE:174.6881\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 34 [7680/60000 (13%)]\tLoss: 174.952728\tBCE:174.9526\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 34 [10240/60000 (17%)]\tLoss: 170.428986\tBCE:170.4288\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 170.602066\tBCE:170.6019\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 34 [15360/60000 (26%)]\tLoss: 170.428177\tBCE:170.4279\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 34 [17920/60000 (30%)]\tLoss: 172.606613\tBCE:172.6063\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 34 [20480/60000 (34%)]\tLoss: 172.053314\tBCE:172.0530\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 34 [23040/60000 (38%)]\tLoss: 174.842300\tBCE:174.8420\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 168.635452\tBCE:168.6351\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 34 [28160/60000 (47%)]\tLoss: 172.465958\tBCE:172.4657\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 34 [30720/60000 (51%)]\tLoss: 179.577362\tBCE:179.5771\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 34 [33280/60000 (56%)]\tLoss: 172.935272\tBCE:172.9351\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 34 [35840/60000 (60%)]\tLoss: 175.244675\tBCE:175.2445\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 172.936661\tBCE:172.9365\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 34 [40960/60000 (68%)]\tLoss: 173.561615\tBCE:173.5614\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 34 [43520/60000 (73%)]\tLoss: 171.069641\tBCE:171.0694\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 34 [46080/60000 (77%)]\tLoss: 170.195694\tBCE:170.1954\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 34 [48640/60000 (81%)]\tLoss: 172.293961\tBCE:172.2937\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 174.166397\tBCE:174.1662\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 34 [53760/60000 (90%)]\tLoss: 175.137070\tBCE:175.1368\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 34 [56320/60000 (94%)]\tLoss: 174.442947\tBCE:174.4427\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 34 [58880/60000 (98%)]\tLoss: 173.270813\tBCE:173.2706\tKLD:0.0002\tC_loss:0.0000\n",
      "====> Epoch: 34 Average loss: 172.9038\tClassifier Accuracy: 99.9783\n",
      "====> Test set loss: 171.7203\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 172.994797\tBCE:172.9945\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 35 [2560/60000 (4%)]\tLoss: 172.483261\tBCE:172.4830\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 35 [5120/60000 (9%)]\tLoss: 174.364197\tBCE:174.3639\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 35 [7680/60000 (13%)]\tLoss: 167.487122\tBCE:167.4868\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 35 [10240/60000 (17%)]\tLoss: 174.210190\tBCE:174.2099\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 178.876312\tBCE:178.8761\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 35 [15360/60000 (26%)]\tLoss: 173.229614\tBCE:173.2293\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 35 [17920/60000 (30%)]\tLoss: 171.876617\tBCE:171.8763\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 35 [20480/60000 (34%)]\tLoss: 171.739517\tBCE:171.7392\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 35 [23040/60000 (38%)]\tLoss: 167.851715\tBCE:167.8515\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 172.498016\tBCE:172.4978\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 35 [28160/60000 (47%)]\tLoss: 173.049210\tBCE:173.0491\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 35 [30720/60000 (51%)]\tLoss: 175.380157\tBCE:175.3800\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 35 [33280/60000 (56%)]\tLoss: 171.054855\tBCE:171.0546\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 35 [35840/60000 (60%)]\tLoss: 169.581528\tBCE:169.5811\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 175.421753\tBCE:175.4213\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 35 [40960/60000 (68%)]\tLoss: 175.101395\tBCE:175.1011\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 35 [43520/60000 (73%)]\tLoss: 171.153915\tBCE:171.1537\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 35 [46080/60000 (77%)]\tLoss: 171.337387\tBCE:171.3371\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 35 [48640/60000 (81%)]\tLoss: 169.817886\tBCE:169.8176\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 167.097931\tBCE:167.0976\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 35 [53760/60000 (90%)]\tLoss: 169.925385\tBCE:169.9251\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 35 [56320/60000 (94%)]\tLoss: 174.179825\tBCE:174.1796\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 35 [58880/60000 (98%)]\tLoss: 169.686050\tBCE:169.6859\tKLD:0.0001\tC_loss:0.0000\n",
      "====> Epoch: 35 Average loss: 172.9025\tClassifier Accuracy: 99.9800\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 171.6017\n",
      "Random number: 9\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 175.077774\tBCE:175.0776\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 36 [2560/60000 (4%)]\tLoss: 170.598938\tBCE:170.5987\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 36 [5120/60000 (9%)]\tLoss: 173.928528\tBCE:173.9283\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 36 [7680/60000 (13%)]\tLoss: 170.027756\tBCE:170.0274\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 36 [10240/60000 (17%)]\tLoss: 175.619873\tBCE:175.6195\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 169.925095\tBCE:169.9247\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 36 [15360/60000 (26%)]\tLoss: 177.467499\tBCE:177.4672\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 36 [17920/60000 (30%)]\tLoss: 172.008575\tBCE:172.0083\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 36 [20480/60000 (34%)]\tLoss: 177.798431\tBCE:177.7982\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 36 [23040/60000 (38%)]\tLoss: 171.663513\tBCE:171.6633\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 171.119293\tBCE:171.1190\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 36 [28160/60000 (47%)]\tLoss: 172.847656\tBCE:172.8474\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 36 [30720/60000 (51%)]\tLoss: 175.329330\tBCE:175.3289\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 36 [33280/60000 (56%)]\tLoss: 175.789581\tBCE:175.7892\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 36 [35840/60000 (60%)]\tLoss: 167.161346\tBCE:167.1611\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 167.058792\tBCE:167.0586\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 36 [40960/60000 (68%)]\tLoss: 177.314911\tBCE:177.3147\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 36 [43520/60000 (73%)]\tLoss: 171.848022\tBCE:171.8477\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 36 [46080/60000 (77%)]\tLoss: 173.985550\tBCE:173.9853\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 36 [48640/60000 (81%)]\tLoss: 172.658478\tBCE:172.6583\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 167.370132\tBCE:167.3699\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 36 [53760/60000 (90%)]\tLoss: 169.965820\tBCE:169.9656\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 36 [56320/60000 (94%)]\tLoss: 176.465363\tBCE:176.4652\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 36 [58880/60000 (98%)]\tLoss: 173.519119\tBCE:173.5190\tKLD:0.0002\tC_loss:0.0000\n",
      "====> Epoch: 36 Average loss: 172.9018\tClassifier Accuracy: 99.9716\n",
      "====> Test set loss: 171.5347\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 171.062119\tBCE:171.0620\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 37 [2560/60000 (4%)]\tLoss: 174.583389\tBCE:174.5833\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 37 [5120/60000 (9%)]\tLoss: 166.644547\tBCE:166.6443\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 37 [7680/60000 (13%)]\tLoss: 171.207077\tBCE:171.2068\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 37 [10240/60000 (17%)]\tLoss: 172.073914\tBCE:172.0737\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 173.748337\tBCE:173.7481\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 37 [15360/60000 (26%)]\tLoss: 172.205475\tBCE:172.2053\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 37 [17920/60000 (30%)]\tLoss: 171.871689\tBCE:171.8716\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 37 [20480/60000 (34%)]\tLoss: 169.876633\tBCE:169.8765\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 37 [23040/60000 (38%)]\tLoss: 176.665390\tBCE:176.6652\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 173.455750\tBCE:173.4555\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 37 [28160/60000 (47%)]\tLoss: 174.417160\tBCE:174.4170\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 37 [30720/60000 (51%)]\tLoss: 176.151505\tBCE:176.1514\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 37 [33280/60000 (56%)]\tLoss: 178.379959\tBCE:178.3798\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 37 [35840/60000 (60%)]\tLoss: 164.769867\tBCE:164.7697\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 177.833542\tBCE:177.8333\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 37 [40960/60000 (68%)]\tLoss: 173.967010\tBCE:173.9667\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 37 [43520/60000 (73%)]\tLoss: 168.893494\tBCE:168.8932\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 37 [46080/60000 (77%)]\tLoss: 171.579407\tBCE:171.5792\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 37 [48640/60000 (81%)]\tLoss: 173.904434\tBCE:173.9042\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 178.651611\tBCE:178.6514\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 37 [53760/60000 (90%)]\tLoss: 177.185989\tBCE:177.1858\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 37 [56320/60000 (94%)]\tLoss: 173.759277\tBCE:173.7589\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 37 [58880/60000 (98%)]\tLoss: 176.427765\tBCE:176.4276\tKLD:0.0002\tC_loss:0.0000\n",
      "====> Epoch: 37 Average loss: 172.9271\tClassifier Accuracy: 99.9850\n",
      "====> Test set loss: 171.6124\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 172.016388\tBCE:172.0162\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 38 [2560/60000 (4%)]\tLoss: 176.931351\tBCE:176.9311\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 38 [5120/60000 (9%)]\tLoss: 174.882263\tBCE:174.8820\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 38 [7680/60000 (13%)]\tLoss: 177.715775\tBCE:177.7156\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 38 [10240/60000 (17%)]\tLoss: 176.317581\tBCE:176.3174\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 177.280640\tBCE:177.2805\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 38 [15360/60000 (26%)]\tLoss: 177.157135\tBCE:177.1569\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 38 [17920/60000 (30%)]\tLoss: 176.393341\tBCE:176.3930\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 38 [20480/60000 (34%)]\tLoss: 170.128952\tBCE:170.1287\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 38 [23040/60000 (38%)]\tLoss: 170.286011\tBCE:170.2859\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 170.932556\tBCE:170.9324\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 38 [28160/60000 (47%)]\tLoss: 176.566757\tBCE:176.5666\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 38 [30720/60000 (51%)]\tLoss: 171.493011\tBCE:171.4929\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 38 [33280/60000 (56%)]\tLoss: 178.542664\tBCE:178.5424\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 38 [35840/60000 (60%)]\tLoss: 173.878815\tBCE:173.8785\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 172.561935\tBCE:172.5616\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 38 [40960/60000 (68%)]\tLoss: 173.620392\tBCE:173.6201\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 38 [43520/60000 (73%)]\tLoss: 179.040924\tBCE:179.0406\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 38 [46080/60000 (77%)]\tLoss: 172.658798\tBCE:172.6586\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 38 [48640/60000 (81%)]\tLoss: 168.830154\tBCE:168.8299\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 166.586899\tBCE:166.5867\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 38 [53760/60000 (90%)]\tLoss: 176.924500\tBCE:176.9243\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 38 [56320/60000 (94%)]\tLoss: 169.522797\tBCE:169.5225\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 38 [58880/60000 (98%)]\tLoss: 172.841919\tBCE:172.8417\tKLD:0.0002\tC_loss:0.0000\n",
      "====> Epoch: 38 Average loss: 172.9156\tClassifier Accuracy: 99.9783\n",
      "====> Test set loss: 171.5408\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 171.471481\tBCE:171.4713\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 39 [2560/60000 (4%)]\tLoss: 175.913681\tBCE:175.9135\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 39 [5120/60000 (9%)]\tLoss: 173.851471\tBCE:173.8513\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 39 [7680/60000 (13%)]\tLoss: 170.674881\tBCE:170.6746\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 39 [10240/60000 (17%)]\tLoss: 171.832657\tBCE:171.8324\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 166.005554\tBCE:166.0054\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 39 [15360/60000 (26%)]\tLoss: 173.132355\tBCE:173.1322\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 39 [17920/60000 (30%)]\tLoss: 177.843430\tBCE:177.8432\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 39 [20480/60000 (34%)]\tLoss: 175.606598\tBCE:175.6063\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 39 [23040/60000 (38%)]\tLoss: 173.163727\tBCE:173.1635\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 170.637100\tBCE:170.6369\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 39 [28160/60000 (47%)]\tLoss: 175.855713\tBCE:175.8556\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 39 [30720/60000 (51%)]\tLoss: 173.455505\tBCE:173.4554\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 39 [33280/60000 (56%)]\tLoss: 173.418884\tBCE:173.4187\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 39 [35840/60000 (60%)]\tLoss: 171.046371\tBCE:171.0462\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 175.091888\tBCE:175.0917\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 39 [40960/60000 (68%)]\tLoss: 171.166840\tBCE:171.1666\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 39 [43520/60000 (73%)]\tLoss: 173.068497\tBCE:173.0682\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 39 [46080/60000 (77%)]\tLoss: 176.159210\tBCE:176.1590\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 39 [48640/60000 (81%)]\tLoss: 169.909317\tBCE:169.9090\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 173.230103\tBCE:173.2298\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 39 [53760/60000 (90%)]\tLoss: 174.454239\tBCE:174.4540\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 39 [56320/60000 (94%)]\tLoss: 173.360657\tBCE:173.3604\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 39 [58880/60000 (98%)]\tLoss: 172.837280\tBCE:172.8371\tKLD:0.0001\tC_loss:0.0000\n",
      "====> Epoch: 39 Average loss: 172.9120\tClassifier Accuracy: 99.9583\n",
      "====> Test set loss: 171.5814\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 176.048279\tBCE:176.0482\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 40 [2560/60000 (4%)]\tLoss: 174.498734\tBCE:174.4986\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 40 [5120/60000 (9%)]\tLoss: 170.346375\tBCE:170.3463\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 40 [7680/60000 (13%)]\tLoss: 174.181213\tBCE:174.1810\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 40 [10240/60000 (17%)]\tLoss: 171.598633\tBCE:171.5984\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 175.914886\tBCE:175.9147\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 40 [15360/60000 (26%)]\tLoss: 175.014221\tBCE:175.0140\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 40 [17920/60000 (30%)]\tLoss: 175.902420\tBCE:175.9022\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 40 [20480/60000 (34%)]\tLoss: 172.302246\tBCE:172.3019\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 40 [23040/60000 (38%)]\tLoss: 172.835831\tBCE:172.8356\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 169.742569\tBCE:169.7423\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 40 [28160/60000 (47%)]\tLoss: 176.122589\tBCE:176.1223\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 40 [30720/60000 (51%)]\tLoss: 174.571365\tBCE:174.5711\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 40 [33280/60000 (56%)]\tLoss: 176.458847\tBCE:176.4587\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 40 [35840/60000 (60%)]\tLoss: 173.607971\tBCE:173.6078\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 176.619522\tBCE:176.6194\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 40 [40960/60000 (68%)]\tLoss: 174.597336\tBCE:174.5972\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 40 [43520/60000 (73%)]\tLoss: 170.491852\tBCE:170.4917\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 40 [46080/60000 (77%)]\tLoss: 168.821762\tBCE:168.8216\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 40 [48640/60000 (81%)]\tLoss: 172.780289\tBCE:172.7801\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 164.762558\tBCE:164.7623\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 40 [53760/60000 (90%)]\tLoss: 172.441940\tBCE:172.4417\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 40 [56320/60000 (94%)]\tLoss: 171.697784\tBCE:171.6975\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 40 [58880/60000 (98%)]\tLoss: 177.125397\tBCE:177.1252\tKLD:0.0002\tC_loss:0.0000\n",
      "====> Epoch: 40 Average loss: 172.9158\tClassifier Accuracy: 99.9199\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 171.5935\n",
      "Random number: 1\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 172.114258\tBCE:172.1141\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 41 [2560/60000 (4%)]\tLoss: 173.506805\tBCE:173.5067\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 41 [5120/60000 (9%)]\tLoss: 173.184875\tBCE:173.1848\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 41 [7680/60000 (13%)]\tLoss: 177.096786\tBCE:177.0967\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 41 [10240/60000 (17%)]\tLoss: 170.701065\tBCE:170.7009\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 173.129135\tBCE:173.1290\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 41 [15360/60000 (26%)]\tLoss: 172.299744\tBCE:172.2996\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 41 [17920/60000 (30%)]\tLoss: 181.800385\tBCE:181.8002\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 41 [20480/60000 (34%)]\tLoss: 168.153656\tBCE:168.1534\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 41 [23040/60000 (38%)]\tLoss: 173.076660\tBCE:173.0765\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 172.860489\tBCE:172.8602\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 41 [28160/60000 (47%)]\tLoss: 174.947083\tBCE:174.9468\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 41 [30720/60000 (51%)]\tLoss: 172.805893\tBCE:172.8057\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 41 [33280/60000 (56%)]\tLoss: 174.472412\tBCE:174.4723\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 41 [35840/60000 (60%)]\tLoss: 171.937912\tBCE:171.9378\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 171.235931\tBCE:171.2357\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 41 [40960/60000 (68%)]\tLoss: 168.677597\tBCE:168.6773\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 41 [43520/60000 (73%)]\tLoss: 167.657257\tBCE:167.6570\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 41 [46080/60000 (77%)]\tLoss: 177.849472\tBCE:177.8493\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 41 [48640/60000 (81%)]\tLoss: 173.793488\tBCE:173.7933\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 172.338165\tBCE:172.3380\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 41 [53760/60000 (90%)]\tLoss: 174.752014\tBCE:174.7518\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 41 [56320/60000 (94%)]\tLoss: 169.757492\tBCE:169.7573\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 41 [58880/60000 (98%)]\tLoss: 181.683182\tBCE:181.6830\tKLD:0.0002\tC_loss:0.0000\n",
      "====> Epoch: 41 Average loss: 172.8734\tClassifier Accuracy: 99.9332\n",
      "====> Test set loss: 171.5114\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 174.391617\tBCE:174.3914\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 42 [2560/60000 (4%)]\tLoss: 172.582306\tBCE:172.5821\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 42 [5120/60000 (9%)]\tLoss: 172.808746\tBCE:172.8085\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 42 [7680/60000 (13%)]\tLoss: 173.959396\tBCE:173.9592\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 42 [10240/60000 (17%)]\tLoss: 174.886703\tBCE:174.8864\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 176.812073\tBCE:176.8118\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 42 [15360/60000 (26%)]\tLoss: 172.100204\tBCE:172.1000\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 42 [17920/60000 (30%)]\tLoss: 176.878525\tBCE:176.8784\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 42 [20480/60000 (34%)]\tLoss: 179.426697\tBCE:179.4266\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 42 [23040/60000 (38%)]\tLoss: 177.615417\tBCE:177.6153\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 176.075302\tBCE:176.0751\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 42 [28160/60000 (47%)]\tLoss: 169.117142\tBCE:169.1169\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 42 [30720/60000 (51%)]\tLoss: 173.114197\tBCE:173.1140\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 42 [33280/60000 (56%)]\tLoss: 167.083572\tBCE:167.0834\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 42 [35840/60000 (60%)]\tLoss: 170.688034\tBCE:170.6879\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 174.955109\tBCE:174.9550\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 42 [40960/60000 (68%)]\tLoss: 174.541901\tBCE:174.5418\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 42 [43520/60000 (73%)]\tLoss: 168.075989\tBCE:168.0758\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 42 [46080/60000 (77%)]\tLoss: 171.118988\tBCE:171.1187\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 42 [48640/60000 (81%)]\tLoss: 168.304535\tBCE:168.3042\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 176.071854\tBCE:176.0716\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 42 [53760/60000 (90%)]\tLoss: 173.651459\tBCE:173.6513\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 42 [56320/60000 (94%)]\tLoss: 169.683243\tBCE:169.6831\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 42 [58880/60000 (98%)]\tLoss: 171.467300\tBCE:171.4670\tKLD:0.0003\tC_loss:0.0000\n",
      "====> Epoch: 42 Average loss: 172.8813\tClassifier Accuracy: 99.9983\n",
      "====> Test set loss: 171.5734\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 175.473526\tBCE:175.4733\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 43 [2560/60000 (4%)]\tLoss: 174.292206\tBCE:174.2919\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 43 [5120/60000 (9%)]\tLoss: 175.216476\tBCE:175.2164\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 43 [7680/60000 (13%)]\tLoss: 173.775818\tBCE:173.7757\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 43 [10240/60000 (17%)]\tLoss: 171.648758\tBCE:171.6487\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 170.251816\tBCE:170.2517\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 43 [15360/60000 (26%)]\tLoss: 174.960678\tBCE:174.9604\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 43 [17920/60000 (30%)]\tLoss: 175.813110\tBCE:175.8128\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 43 [20480/60000 (34%)]\tLoss: 168.559052\tBCE:168.5588\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 43 [23040/60000 (38%)]\tLoss: 165.093842\tBCE:165.0936\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 168.333282\tBCE:168.3331\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 43 [28160/60000 (47%)]\tLoss: 172.957565\tBCE:172.9573\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 43 [30720/60000 (51%)]\tLoss: 171.475510\tBCE:171.4753\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 43 [33280/60000 (56%)]\tLoss: 172.551514\tBCE:172.5513\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 43 [35840/60000 (60%)]\tLoss: 174.787079\tBCE:174.7868\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 176.298965\tBCE:176.2988\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 43 [40960/60000 (68%)]\tLoss: 172.209915\tBCE:172.2097\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 43 [43520/60000 (73%)]\tLoss: 174.418030\tBCE:174.4179\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 43 [46080/60000 (77%)]\tLoss: 169.816788\tBCE:169.8166\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 43 [48640/60000 (81%)]\tLoss: 172.132263\tBCE:172.1320\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 175.982361\tBCE:175.9823\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 43 [53760/60000 (90%)]\tLoss: 175.894363\tBCE:175.8942\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 43 [56320/60000 (94%)]\tLoss: 167.974701\tBCE:167.9745\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 43 [58880/60000 (98%)]\tLoss: 175.351898\tBCE:175.3517\tKLD:0.0002\tC_loss:0.0000\n",
      "====> Epoch: 43 Average loss: 172.8804\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 171.5503\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 174.217743\tBCE:174.2175\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 44 [2560/60000 (4%)]\tLoss: 181.032791\tBCE:181.0326\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 44 [5120/60000 (9%)]\tLoss: 171.383728\tBCE:171.3835\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 44 [7680/60000 (13%)]\tLoss: 173.475861\tBCE:173.4757\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 44 [10240/60000 (17%)]\tLoss: 175.659821\tBCE:175.6597\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 174.898270\tBCE:174.8981\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 44 [15360/60000 (26%)]\tLoss: 171.311935\tBCE:171.3118\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 44 [17920/60000 (30%)]\tLoss: 174.605133\tBCE:174.6050\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 44 [20480/60000 (34%)]\tLoss: 170.868698\tBCE:170.8685\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 44 [23040/60000 (38%)]\tLoss: 168.955231\tBCE:168.9550\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 176.721085\tBCE:176.7209\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 44 [28160/60000 (47%)]\tLoss: 175.993271\tBCE:175.9931\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 44 [30720/60000 (51%)]\tLoss: 171.620255\tBCE:171.6202\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 44 [33280/60000 (56%)]\tLoss: 173.475723\tBCE:173.4756\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 44 [35840/60000 (60%)]\tLoss: 168.390030\tBCE:168.3898\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 174.893463\tBCE:174.8933\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 44 [40960/60000 (68%)]\tLoss: 173.046539\tBCE:173.0464\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 44 [43520/60000 (73%)]\tLoss: 170.691925\tBCE:170.6917\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 44 [46080/60000 (77%)]\tLoss: 171.622086\tBCE:171.6218\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 44 [48640/60000 (81%)]\tLoss: 170.059799\tBCE:170.0596\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 170.841888\tBCE:170.8417\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 44 [53760/60000 (90%)]\tLoss: 168.675018\tBCE:168.6748\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 44 [56320/60000 (94%)]\tLoss: 169.530090\tBCE:169.5298\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 44 [58880/60000 (98%)]\tLoss: 174.406418\tBCE:174.4062\tKLD:0.0002\tC_loss:0.0000\n",
      "====> Epoch: 44 Average loss: 172.9033\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 171.7935\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 169.664307\tBCE:169.6641\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 45 [2560/60000 (4%)]\tLoss: 176.325882\tBCE:176.3256\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 45 [5120/60000 (9%)]\tLoss: 168.914749\tBCE:168.9145\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 45 [7680/60000 (13%)]\tLoss: 168.033875\tBCE:168.0337\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 45 [10240/60000 (17%)]\tLoss: 176.362091\tBCE:176.3619\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 170.731247\tBCE:170.7310\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 45 [15360/60000 (26%)]\tLoss: 170.930511\tBCE:170.9303\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 45 [17920/60000 (30%)]\tLoss: 174.618851\tBCE:174.6185\tKLD:0.0004\tC_loss:0.0000\n",
      "Train Epoch: 45 [20480/60000 (34%)]\tLoss: 177.914047\tBCE:177.9138\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 45 [23040/60000 (38%)]\tLoss: 168.700851\tBCE:168.7007\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 173.339432\tBCE:173.3393\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 45 [28160/60000 (47%)]\tLoss: 173.999664\tBCE:173.9995\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 45 [30720/60000 (51%)]\tLoss: 177.019424\tBCE:177.0193\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 45 [33280/60000 (56%)]\tLoss: 168.772934\tBCE:168.7728\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 45 [35840/60000 (60%)]\tLoss: 172.339157\tBCE:172.3390\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 167.645264\tBCE:167.6451\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 45 [40960/60000 (68%)]\tLoss: 175.592758\tBCE:175.5926\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 45 [43520/60000 (73%)]\tLoss: 179.493149\tBCE:179.4930\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 45 [46080/60000 (77%)]\tLoss: 173.383804\tBCE:173.3837\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 45 [48640/60000 (81%)]\tLoss: 179.411102\tBCE:179.4109\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 176.164215\tBCE:176.1640\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 45 [53760/60000 (90%)]\tLoss: 180.148941\tBCE:180.1487\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 45 [56320/60000 (94%)]\tLoss: 173.828537\tBCE:173.8283\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 45 [58880/60000 (98%)]\tLoss: 173.141861\tBCE:173.1417\tKLD:0.0002\tC_loss:0.0000\n",
      "====> Epoch: 45 Average loss: 172.9081\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 171.4654\n",
      "Random number: 0\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 168.829834\tBCE:168.8297\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 46 [2560/60000 (4%)]\tLoss: 172.128510\tBCE:172.1284\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 46 [5120/60000 (9%)]\tLoss: 173.255463\tBCE:173.2553\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 46 [7680/60000 (13%)]\tLoss: 174.777069\tBCE:174.7769\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 46 [10240/60000 (17%)]\tLoss: 175.563934\tBCE:175.5638\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 172.239777\tBCE:172.2396\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 46 [15360/60000 (26%)]\tLoss: 172.368637\tBCE:172.3685\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 46 [17920/60000 (30%)]\tLoss: 166.249146\tBCE:166.2490\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 46 [20480/60000 (34%)]\tLoss: 172.819748\tBCE:172.8196\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 46 [23040/60000 (38%)]\tLoss: 174.695984\tBCE:174.6958\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 178.282120\tBCE:178.2818\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 46 [28160/60000 (47%)]\tLoss: 178.079895\tBCE:178.0797\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 46 [30720/60000 (51%)]\tLoss: 169.640564\tBCE:169.6404\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 46 [33280/60000 (56%)]\tLoss: 170.679352\tBCE:170.6792\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 46 [35840/60000 (60%)]\tLoss: 175.523819\tBCE:175.5237\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 175.469177\tBCE:175.4691\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 46 [40960/60000 (68%)]\tLoss: 170.825592\tBCE:170.8255\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 46 [43520/60000 (73%)]\tLoss: 172.469162\tBCE:172.4690\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 46 [46080/60000 (77%)]\tLoss: 175.797485\tBCE:175.7973\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 46 [48640/60000 (81%)]\tLoss: 176.966522\tBCE:176.9664\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 175.793182\tBCE:175.7930\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 46 [53760/60000 (90%)]\tLoss: 176.967834\tBCE:176.9676\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 46 [56320/60000 (94%)]\tLoss: 174.109253\tBCE:174.1091\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 46 [58880/60000 (98%)]\tLoss: 169.300400\tBCE:169.3002\tKLD:0.0002\tC_loss:0.0000\n",
      "====> Epoch: 46 Average loss: 172.8541\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 171.5236\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 165.127274\tBCE:165.1271\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 47 [2560/60000 (4%)]\tLoss: 171.084473\tBCE:171.0843\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 47 [5120/60000 (9%)]\tLoss: 172.921143\tBCE:172.9210\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 47 [7680/60000 (13%)]\tLoss: 173.195724\tBCE:173.1956\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 47 [10240/60000 (17%)]\tLoss: 169.788315\tBCE:169.7882\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 172.979584\tBCE:172.9793\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 47 [15360/60000 (26%)]\tLoss: 168.478104\tBCE:168.4780\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 47 [17920/60000 (30%)]\tLoss: 175.488281\tBCE:175.4881\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 47 [20480/60000 (34%)]\tLoss: 170.632477\tBCE:170.6324\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 47 [23040/60000 (38%)]\tLoss: 178.679306\tBCE:178.6790\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 174.535736\tBCE:174.5354\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 47 [28160/60000 (47%)]\tLoss: 173.881821\tBCE:173.8815\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 47 [30720/60000 (51%)]\tLoss: 174.476227\tBCE:174.4761\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 47 [33280/60000 (56%)]\tLoss: 178.058517\tBCE:178.0584\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 47 [35840/60000 (60%)]\tLoss: 167.898499\tBCE:167.8983\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 177.153671\tBCE:177.1535\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 47 [40960/60000 (68%)]\tLoss: 177.403214\tBCE:177.4031\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 47 [43520/60000 (73%)]\tLoss: 172.371307\tBCE:172.3711\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 47 [46080/60000 (77%)]\tLoss: 173.823868\tBCE:173.8236\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 47 [48640/60000 (81%)]\tLoss: 177.198761\tBCE:177.1985\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 174.120361\tBCE:174.1201\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 47 [53760/60000 (90%)]\tLoss: 175.944305\tBCE:175.9441\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 47 [56320/60000 (94%)]\tLoss: 174.298386\tBCE:174.2983\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 47 [58880/60000 (98%)]\tLoss: 173.868134\tBCE:173.8680\tKLD:0.0001\tC_loss:0.0000\n",
      "====> Epoch: 47 Average loss: 172.8622\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 171.4324\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 172.753769\tBCE:172.7536\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 48 [2560/60000 (4%)]\tLoss: 172.794876\tBCE:172.7947\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 48 [5120/60000 (9%)]\tLoss: 172.474335\tBCE:172.4742\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 48 [7680/60000 (13%)]\tLoss: 173.651520\tBCE:173.6514\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 48 [10240/60000 (17%)]\tLoss: 177.654663\tBCE:177.6546\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 177.728531\tBCE:177.7284\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 48 [15360/60000 (26%)]\tLoss: 168.849045\tBCE:168.8489\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 48 [17920/60000 (30%)]\tLoss: 173.083221\tBCE:173.0831\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 48 [20480/60000 (34%)]\tLoss: 175.839813\tBCE:175.8396\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 48 [23040/60000 (38%)]\tLoss: 168.206696\tBCE:168.2065\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 175.967468\tBCE:175.9673\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 48 [28160/60000 (47%)]\tLoss: 176.885788\tBCE:176.8856\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 48 [30720/60000 (51%)]\tLoss: 175.335190\tBCE:175.3350\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 48 [33280/60000 (56%)]\tLoss: 169.459457\tBCE:169.4591\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 48 [35840/60000 (60%)]\tLoss: 172.344208\tBCE:172.3439\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 170.767090\tBCE:170.7669\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 48 [40960/60000 (68%)]\tLoss: 177.648178\tBCE:177.6480\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 48 [43520/60000 (73%)]\tLoss: 175.539886\tBCE:175.5397\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 48 [46080/60000 (77%)]\tLoss: 176.305710\tBCE:176.3056\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 48 [48640/60000 (81%)]\tLoss: 170.290802\tBCE:170.2907\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 174.276306\tBCE:174.2762\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 48 [53760/60000 (90%)]\tLoss: 167.088303\tBCE:167.0880\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 48 [56320/60000 (94%)]\tLoss: 170.499023\tBCE:170.4988\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 48 [58880/60000 (98%)]\tLoss: 172.308426\tBCE:172.3083\tKLD:0.0002\tC_loss:0.0000\n",
      "====> Epoch: 48 Average loss: 172.8620\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 171.6416\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 175.823578\tBCE:175.8234\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 49 [2560/60000 (4%)]\tLoss: 171.754089\tBCE:171.7539\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 49 [5120/60000 (9%)]\tLoss: 170.552002\tBCE:170.5519\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 49 [7680/60000 (13%)]\tLoss: 169.992538\tBCE:169.9924\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 49 [10240/60000 (17%)]\tLoss: 173.479248\tBCE:173.4789\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 173.257278\tBCE:173.2570\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 49 [15360/60000 (26%)]\tLoss: 172.988434\tBCE:172.9883\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 49 [17920/60000 (30%)]\tLoss: 173.417252\tBCE:173.4171\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 49 [20480/60000 (34%)]\tLoss: 172.277710\tBCE:172.2776\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 49 [23040/60000 (38%)]\tLoss: 176.575638\tBCE:176.5755\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 172.493347\tBCE:172.4932\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 49 [28160/60000 (47%)]\tLoss: 173.453110\tBCE:173.4529\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 49 [30720/60000 (51%)]\tLoss: 175.134628\tBCE:175.1344\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 49 [33280/60000 (56%)]\tLoss: 172.508820\tBCE:172.5087\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 49 [35840/60000 (60%)]\tLoss: 177.001678\tBCE:177.0015\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 179.330429\tBCE:179.3303\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 49 [40960/60000 (68%)]\tLoss: 170.489136\tBCE:170.4890\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 49 [43520/60000 (73%)]\tLoss: 174.635056\tBCE:174.6349\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 49 [46080/60000 (77%)]\tLoss: 176.798538\tBCE:176.7984\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 49 [48640/60000 (81%)]\tLoss: 174.906570\tBCE:174.9064\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 177.854645\tBCE:177.8544\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 49 [53760/60000 (90%)]\tLoss: 168.970749\tBCE:168.9706\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 49 [56320/60000 (94%)]\tLoss: 174.832520\tBCE:174.8323\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 49 [58880/60000 (98%)]\tLoss: 171.612045\tBCE:171.6119\tKLD:0.0002\tC_loss:0.0000\n",
      "====> Epoch: 49 Average loss: 172.8294\tClassifier Accuracy: 100.0000\n",
      "====> Test set loss: 171.5349\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 174.337982\tBCE:174.3378\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 50 [2560/60000 (4%)]\tLoss: 173.933838\tBCE:173.9336\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 50 [5120/60000 (9%)]\tLoss: 169.690262\tBCE:169.6900\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 50 [7680/60000 (13%)]\tLoss: 177.598495\tBCE:177.5983\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 50 [10240/60000 (17%)]\tLoss: 173.787674\tBCE:173.7875\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 170.870010\tBCE:170.8699\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 50 [15360/60000 (26%)]\tLoss: 178.462921\tBCE:178.4628\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 50 [17920/60000 (30%)]\tLoss: 173.601791\tBCE:173.6017\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 50 [20480/60000 (34%)]\tLoss: 169.690582\tBCE:169.6904\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 50 [23040/60000 (38%)]\tLoss: 173.271439\tBCE:173.2712\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 172.688385\tBCE:172.6882\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 50 [28160/60000 (47%)]\tLoss: 174.572556\tBCE:174.5724\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 50 [30720/60000 (51%)]\tLoss: 175.128586\tBCE:175.1284\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 50 [33280/60000 (56%)]\tLoss: 176.191772\tBCE:176.1916\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 50 [35840/60000 (60%)]\tLoss: 176.059448\tBCE:176.0592\tKLD:0.0003\tC_loss:0.0000\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 178.478165\tBCE:178.4780\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 50 [40960/60000 (68%)]\tLoss: 171.068451\tBCE:171.0683\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 50 [43520/60000 (73%)]\tLoss: 176.038513\tBCE:176.0384\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 50 [46080/60000 (77%)]\tLoss: 171.582504\tBCE:171.5823\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 50 [48640/60000 (81%)]\tLoss: 178.438797\tBCE:178.4387\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 178.769089\tBCE:178.7690\tKLD:0.0001\tC_loss:0.0000\n",
      "Train Epoch: 50 [53760/60000 (90%)]\tLoss: 171.620163\tBCE:171.6200\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 50 [56320/60000 (94%)]\tLoss: 175.729111\tBCE:175.7289\tKLD:0.0002\tC_loss:0.0000\n",
      "Train Epoch: 50 [58880/60000 (98%)]\tLoss: 166.155762\tBCE:166.1555\tKLD:0.0002\tC_loss:0.0000\n",
      "====> Epoch: 50 Average loss: 172.8323\tClassifier Accuracy: 100.0000\n",
      "torch.Size([256, 784])\n",
      "--- torch.Size([256, 1, 28, 28])\n",
      "====> Test set loss: 171.4448\n",
      "Random number: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch_train_loss = list()\n",
    "epoch_train_class_acc = list()\n",
    "epoch_test_loss = list()\n",
    "epoch_test_class_acc = list()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc = train(epoch)\n",
    "    test_loss, test_acc =test(epoch)\n",
    "    epoch_train_loss.append(train_loss)\n",
    "    epoch_train_class_acc.append(train_acc)\n",
    "    epoch_test_loss.append(test_loss)\n",
    "    epoch_test_class_acc.append(test_acc)\n",
    "\n",
    "    # Generate random digits every n epochs\n",
    "    with torch.inference_mode():\n",
    "        if epoch%5==0:\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "        \n",
    "            c = np.zeros(shape=(sample.shape[0],))\n",
    "            rand = np.random.randint(0, 10)\n",
    "            print(f\"Random number: {rand}\")\n",
    "            c[:] = rand\n",
    "            c = torch.FloatTensor(c)\n",
    "            c = c.to(torch.int64)\n",
    "            c = c.to(device)\n",
    "            c = F.one_hot(c, cond_shape)\n",
    "            sample = model.decoder((sample, c)).cpu()\n",
    "            \n",
    "            generated_image = sample[:, 0:sample.shape[1]]\n",
    "            \n",
    "            \n",
    "            save_image(generated_image.view(64, 1, 28, 28),\n",
    "                    'results/sample_' + str(epoch) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    sample = torch.randn(64, 20).to(device)\n",
    "        \n",
    "    c = np.zeros(shape=(sample.shape[0],))\n",
    "    num = i\n",
    "    c[:] = num\n",
    "    c = torch.FloatTensor(c)\n",
    "    c = c.to(torch.int64)\n",
    "    c = c.to(device)\n",
    "    c = F.one_hot(c, cond_shape)\n",
    "    sample = model.decoder((sample, c)).cpu()\n",
    "\n",
    "    generated_image = sample[:, 0:sample.shape[1]]\n",
    "\n",
    "\n",
    "    save_image(generated_image.view(64, 1, 28, 28),\n",
    "            'results/generated_conv_' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the digit \"6\" from gaussian noise\n",
      "Classifier says the below image is a 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe98f88e670>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "i = random.randint(0, 9)\n",
    "sample = torch.randn(1, 20).to(device)\n",
    "c = np.zeros(shape=(sample.shape[0],))\n",
    "num = i\n",
    "print(f\"Generating the digit \\\"{i}\\\" from gaussian noise\")\n",
    "c[:] = num\n",
    "c = torch.FloatTensor(c)\n",
    "c = c.to(torch.int64)\n",
    "c = c.to(device)\n",
    "c = F.one_hot(c, cond_shape)\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    sample = model.decoder((sample, c))\n",
    "    sample = sample.reshape([1, 1, 28, 28])\n",
    "    c_out = model.classifier(sample)\n",
    "\n",
    "c_out = torch.argmax(c_out).item()\n",
    "print(f\"Classifier says the below image is a {c_out}\")\n",
    "plt.imshow(sample[0].cpu().squeeze())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_train_class_acc)\n",
    "plt.plot(epoch_test_class_acc)\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmvElEQVR4nO3de5xcdX3/8ddnLjuz2c09IZALJEAQAkiQNA2CClIKQX9AtVIELG3tL1p5tGIrFfyh/rC1P+qjpUqtXCxUKIpQLj+pYrkViP4kYBIjt0QSgZDN/UKS3WRvM/P5/fH9zuxskk12szu7ZM/7+XjMY86cM5fvmZ097/M533Mxd0dERAQgNdQNEBGRdw6FgoiIVCgURESkQqEgIiIVCgUREalQKIiISIVCQWSAmJmb2bFD3Q6R/lAoyLBnZi1Vt5KZtVY9vryH15xlZk0D2IZnzOxPB+r9RGolM9QNEKk1d28sD5vZm8CfuvuTQ9cikXcuVQqSWGaWM7NvmNm6ePtGHNcA/ASYXFVRTDazuWb2nJltN7P1ZvYtM6vrZxtSZna9ma02s01mdreZjY7T8mZ2j5ltjZ/5CzObFKf9kZm9bmbNZvZGTxWPSF8pFCTJ/hcwD5gNnALMBa53913AfGCduzfG2zqgCHwOmACcDpwDfKafbfijeDsbOBpoBL4Vp10JjAamAeOBTwOtMbRuBua7+0jgvcCyfrZDBFAoSLJdDnzV3Te5+2bgBuATPT3Z3Ze4+yJ3L7j7m8BtwAcGoA03ufvr7t4CXAdcamYZoJMQBse6ezF+/s74uhJwkpnVu/t6d3+ln+0QARQKkmyTgdVVj1fHcftkZseZ2Y/MbIOZ7QT+jlA1DHQbMsAk4N+Bx4AfxM1bXzezbKxk/oBQOaw3sx+b2fH9bIcIoFCQZFsHHFX1+Mg4DmBfpw++BVgBzHT3UcAXAatBGwrARnfvdPcb3H0WYRPRh4E/BHD3x9z9XOCI2Kbv9LMdIoBCQZLtXuB6M5toZhOALwP3xGkbgfHlTt9oJLATaIlr5n/Wx8/LxM7j8i0b2/A5M5thZo2E6uM+dy+Y2dlmdrKZpePndgIlM5tkZhfFvoV2oIWwOUmk3xQKkmR/CywGXgReApbGcbj7CsIC+/W4589k4PPAZUAzYc38vj5+3i1Aa9Xt34A7CZuJFgJvAG3An8fnHw48QAiE5cCz8bkp4C8JVcY2Qr9GXwNKZJ9MF9kREZEyVQoiIlKhUBARkQqFgoiIVCgURESk4pA+Id6ECRN8+vTpQ90MEZFDypIlS7a4+8R9TTukQ2H69OksXrx4qJshInJIMbPVPU3T5iMREalQKIiISIVCQUREKg7pPoV96ezspKmpiba2tqFuSs3l83mmTp1KNpsd6qaIyDAx7EKhqamJkSNHMn36dMz6ewLLdy53Z+vWrTQ1NTFjxoyhbo6IDBPDbvNRW1sb48ePH9aBAGBmjB8/PhEVkYgMnmEXCsCwD4SypMyniAyeYRkKB9LWWWTDjjYKRZ2CXkSkWiJDob2zyKbmNjpLA3/a8O3bt/Ptb3+7z6+74IIL2L59+4C3R0SkLxIZCuXNLrW4lkRPoVAoFPb7ukcffZQxY8YMeHtERPpi2O191BupuCm+BoUC1157Lb/5zW+YPXs22WyWfD7P2LFjWbFiBa+99hoXX3wxa9asoa2tjc9+9rMsWLAA6DplR0tLC/Pnz+fMM8/k5z//OVOmTOGHP/wh9fX1A99YEZE9DOtQuOE/X+HVdTv3Gl9yp7WjSD6bJp3qW2ftrMmj+Mr/OLHH6TfeeCMvv/wyy5Yt45lnnuFDH/oQL7/8cmW30TvvvJNx48bR2trKb/3Wb/HRj36U8ePHd3uPlStXcu+99/Kd73yHSy65hAcffJArrriiT+0UETkYwzoU3gnmzp3b7TiCm2++mYcffhiANWvWsHLlyr1CYcaMGcyePRuA0047jTfffHOwmisiCTesQ6GnNfq2ziKvbWzmyHEjGDOirqZtaGhoqAw/88wzPPnkkzz33HOMGDGCs846a5/HGeRyucpwOp2mtbW1pm0UESlLZEdzLfsURo4cSXNz8z6n7dixg7FjxzJixAhWrFjBokWLBr4BIiL9MKwrhZ6U9z4q1WDvo/Hjx3PGGWdw0kknUV9fz6RJkyrTzj//fG699VZOOOEE3vWudzFv3rwB/3wRkf6wWuyWOVjmzJnje15kZ/ny5Zxwwgn7fV2x5LyybgdHjK5n4sjcfp/7Tteb+RURqWZmS9x9zr6mJXzz0aEbiCIitZDIUDAzDKvJwWsiIoeyRIYCgFltOppFRA5liQ2FlKlSEBHZU4JDQZWCiMieEhsKpkpBRGQviQ2FWlUKB3vqbIBvfOMb7N69e4BbJCLSe4kNBTOryS6pCgUROZQl8ohmCJVCLbYeVZ86+9xzz+Wwww7j/vvvp729nd/7vd/jhhtuYNeuXVxyySU0NTVRLBb50pe+xMaNG1m3bh1nn302EyZM4Omnnx74xomIHMDwDoWfXAsbXtrnpCM6izgO2T5+BYefDPNv7HFy9amzH3/8cR544AFeeOEF3J0LL7yQhQsXsnnzZiZPnsyPf/xjIJwTafTo0dx00008/fTTTJgwoW9tEhEZIAnefFT7z3j88cd5/PHHOfXUU3nPe97DihUrWLlyJSeffDJPPPEEX/jCF/jpT3/K6NGja98YEZFeGN6Vwn7W6Ddv282ujgLHHz6qZh/v7lx33XV86lOf2mva0qVLefTRR7n++us555xz+PKXv1yzdoiI9FaiK4Va9ClUnzr7vPPO484776SlpQWAtWvXsmnTJtatW8eIESO44ooruOaaa1i6dOlerxURGQo1qxTMbBpwNzAJcOB2d/+mmX0M+N/ACcBcd19c9ZrrgE8CReAv3P2xWrUvVaO9j6pPnT1//nwuu+wyTj/9dAAaGxu55557WLVqFddccw2pVIpsNsstt9wCwIIFCzj//POZPHmyOppFZEjU7NTZZnYEcIS7LzWzkcAS4GJCQJSA24DPl0PBzGYB9wJzgcnAk8Bx7l7s6TMO9tTZAOt3tLK1pYOTphza2/N16mwR6ashOXW2u69396VxuBlYDkxx9+Xu/ut9vOQi4Afu3u7ubwCrCAFRE+VKQUc1i4h0GZQ+BTObDpwKPL+fp00B1lQ9borjatSmcK9MEBHpUvNQMLNG4EHganffOQDvt8DMFpvZ4s2bN+/zOb1Z+09Ru0tyDhZVOSIy0GoaCmaWJQTC99z9oQM8fS0wrerx1DiuG3e/3d3nuPuciRMn7vUm+XyerVu3HnCBeahXCu7O1q1byefzQ90UERlGarn3kQF3AMvd/aZevOQR4PtmdhOho3km8EJfP3fq1Kk0NTXRUxVRtrujwLZdndj2HJn0oblnbj6fZ+rUqUPdDBEZRmp58NoZwCeAl8xsWRz3RSAH/DMwEfixmS1z9/Pc/RUzux94FSgAV+1vz6OeZLNZZsyYccDnPfrSej7zyFIeu/r9vOvwkX39GBGRYalmoeDuPwN6OpnEwz285mvA12rVpmr5bKgO2jr7nDsiIsPWobndZADkMmlAoSAiUi2xoVCpFAqlIW6JiMg7R2JDoVwptKtSEBGpSGwo5LNx85EqBRGRigSHgjqaRUT2lNhQ0OYjEZG9JTYUuioFbT4SESlLcCjESqGgSkFEpCyxoZBNp0inTJWCiEiVxIYCQC6TUkeziEiVRIdCPpumTZuPREQqkh0KmZQ2H4mIVEl2KGTTtOvgNRGRikSHQi6bVp+CiEiVZIeCOppFRLpJdCjksyna1acgIlKR8FDQ3kciItWSHQqZtCoFEZEqyQ6FbEqVgohIlUSHQi6jvY9ERKolOhTyWR28JiJSLeGhoEpBRKRaokMhF49odvehboqIyDtCokOhfKEdnepCRCRIdCh0XZJToSAiAgkPhcolObVbqogIkPRQiJWCOptFRIJkh0LlOs3afCQiAokPhbj5SJWCiAiQ8FDIVTYfqVIQEYGEh4IqBRGR7hIeCupoFhGplvBQ0MFrIiLVEh0KOe2SKiLSTbJDoXLwmioFERFIeChUjlNQpSAiAtQwFMxsmpk9bWavmtkrZvbZOH6cmT1hZivj/dg43szsZjNbZWYvmtl7atW2Mh3RLCLSXS0rhQLwV+4+C5gHXGVms4BrgafcfSbwVHwMMB+YGW8LgFtq2DYAsmkjZepoFhEpq1kouPt6d18ah5uB5cAU4CLgrvi0u4CL4/BFwN0eLALGmNkRtWofgJnpkpwiIlUGpU/BzKYDpwLPA5PcfX2ctAGYFIenAGuqXtYUx+35XgvMbLGZLd68eXO/26ZLcoqIdKl5KJhZI/AgcLW776ye5uGSZ3267Jm73+7uc9x9zsSJE/vdPl2SU0SkS01DwcyyhED4nrs/FEdvLG8Wiveb4vi1wLSql0+N42oqn01rl1QRkaiWex8ZcAew3N1vqpr0CHBlHL4S+GHV+D+MeyHNA3ZUbWaqmVwmpV1SRUSiTA3f+wzgE8BLZrYsjvsicCNwv5l9ElgNXBKnPQpcAKwCdgN/XMO2VeRUKYiIVNQsFNz9Z4D1MPmcfTzfgatq1Z6e5DMp9SmIiESJPqIZQp+CNh+JiAQKBe2SKiJSoVDIpmkvqFIQEQGFArmMKgURkbLEh0I4TkGVgogIKBR0RLOISBWFQtx8FPaIFRFJtsSHQi5eaKejqH4FERGFQiZeklOdzSIiCgVdklNEpItCIVu+JKcqBRERhUI2bj7SbqkiIgqFfKa8+UiVgohI4kMhp0pBRKQi8aHQ1aegUBAR6VUomFmDmaXi8HFmdmG81OYhr7z5SB3NIiK9rxQWAnkzmwI8Trii2ndr1ajBVOloVqUgItLrUDB33w18BPi2u38MOLF2zRo8leMUdElOEZHeh4KZnQ5cDvw4jkvXpkmDq+uIZlUKIiK9DYWrgeuAh939FTM7Gni6Zq0aRDl1NIuIVGR68yR3fxZ4FiB2OG9x97+oZcMGS7lPQZuPRER6v/fR981slJk1AC8Dr5rZNbVt2uCoS6cwU6UgIgK933w0y913AhcDPwFmEPZAOuSZGblMSpWCiAi9D4VsPC7hYuARd+8Ehs1VaXT1NRGRoLehcBvwJtAALDSzo4CdtWrUYMtnFAoiItD7juabgZurRq02s7Nr06TBl8+mdESziAi972gebWY3mdniePtHQtUwLGjzkYhI0NvNR3cCzcAl8bYT+LdaNWqwqaNZRCTo1eYj4Bh3/2jV4xvMbFkN2jMkcqoURESA3lcKrWZ2ZvmBmZ0BtNamSYMvn03TpkpBRKTXlcKngbvNbHR8/DZwZW2aNPjymRSbVCmIiPR676NfAaeY2aj4eKeZXQ28WMO2DRp1NIuIBH268pq774xHNgP8ZQ3aMyTU0SwiEvTncpw2YK0YYqoURESC/oTCMDrNhQ5eExGBA/QpmFkz+174G1BfkxYNgbD3URF3x2zYFEAiIn2230rB3Ue6+6h93Ea6+4EC5U4z22RmL1eNO8XMnjOzl8zsP8sd13HadWa2ysx+bWbn9X/Wei+fTeMOHUVVCyKSbP3ZfHQg3wXO32PcvwLXuvvJwMPANQBmNgu4lHDd5/OBb5vZoF3us3xJTnU2i0jS1SwU3H0hsG2P0ccBC+PwE0D5KOmLgB+4e7u7vwGsAubWqm170iU5RUSCWlYK+/IKIQAAPgZMi8NTgDVVz2uK4/ZiZgvKJ+bbvHnzgDQqX64U1NksIgk32KHwJ8BnzGwJMBLo6OsbuPvt7j7H3edMnDhxQBqVV6UgIgL0/jQXA8LdVwC/C2BmxwEfipPW0lU1AEyN4wZFVyioUhCRZBvUSsHMDov3KeB64NY46RHgUjPLmdkMYCbwwmC1q6ujWZWCiCRbzSoFM7sXOAuYYGZNwFeARjO7Kj7lIeI1Gdz9FTO7H3gVKABXufugLaFVKYiIBDULBXf/eA+TvtnD878GfK1W7dmffDZUCupTEJGkG+yO5nekSqWgzUciknAKBSCf0eYjERFQKACQy6qjWUQEFAqAKgURkTKFAl2VgjqaRSTpFAqE4xTMoF2hICIJp1AAzIxcJkWbzpIqIgmnUIhymbQqBRFJPIVCpEtyiogoFCrKl+QUEUkyhUKUz6S195GIJJ5CIcpp85GIiEKhLJ9J64hmEUk8hUKkSkFERKFQkc+qT0FERKEQ5bNp2nXwmogknEIhymVSqhREJPEUClE+m1KlICKJp1CIdJyCiIhCoaLc0ezuQ90UEZEho1CI8tkUJYfOokJBRJJLoRDlyldf0wFsIpJgCoUoX75Osw5gE5EEUyhEuWz5Os2qFEQkuRQKUT6Ggs5/JCJJplCI8pnwVej8RyKSZAqFSJuPREQUChXlSkFHNYtIkikUorwqBRERhUJZVyioUhCR5FIoROXjFFQpiEiSKRQiHdEsIqJQqNARzSIiCoWKSp+CKgURSTCFQpTTwWsiIrULBTO708w2mdnLVeNmm9kiM1tmZovNbG4cb2Z2s5mtMrMXzew9tWrXftpLLpOiXR3NIpJgtawUvgucv8e4rwM3uPts4MvxMcB8YGa8LQBuqWG7eqTrNItI0tUsFNx9IbBtz9HAqDg8GlgXhy8C7vZgETDGzI6oVdt6ks+mdUSziCRaZpA/72rgMTP7B0IgvTeOnwKsqXpeUxy3fjAbV74kp4hIUg12R/OfAZ9z92nA54A7+voGZrYg9kcs3rx584A2Lp9NqaNZRBJtsEPhSuChOPwfwNw4vBaYVvW8qXHcXtz9dnef4+5zJk6cOKCNy2XS2iVVRBJtsENhHfCBOPxBYGUcfgT4w7gX0jxgh7sP6qYjKFcKCgURSa6a9SmY2b3AWcAEM2sCvgL8T+CbZpYB2gh7GgE8ClwArAJ2A39cq3btTz6bpqW9MBQfLSLyjlCzUHD3j/cw6bR9PNeBq2rVlt7KZdJsaekY6maIiAwZHdFcJZ/VwWsikmwKhSq5jHZJFZFkUyhUyWdTtOngNRFJMIVClXw2rc1HIpJoCoUqqhREJOkUClXymTTFktNZVDCISDIpFKrkdJ1mEUk4hUKVytXXdP4jEUkohUKVfCaEQrvOfyQiCaVQqNK1+UiVgogkk0KhStfmI1UKIpJMCoUquUz4OrT5SESSSqFQRR3NIpJ0CoUq5VBQpSAiSaVQqJJXR7OIJJxCoUp5l9TVW3cPcUtERIZGMkNh2+vwyF9Aob3b6Clj6zll2hj+/r9W8HePLtfpLkQkcZIZClt/A0vvgp/9U7fR2XSK+xbM4xPzjuL2ha9zyW3PsXZ76xA1UkRk8CUzFGaeCyd/DBb+A2xa0W1SPpvmby4+iW9ddiorN7ZwwTd/ylPLNw5RQ0VEBlcyQwHg/BshNxIe+XMo7b230YffPZkf/fmZTBlTzyfvWsz/0eYkEUmA5IZCw4QQDE0vwC/u2OdTpk9o4KHPvJcr5h3JbQtf57S/eYI/u2cJ33/+LdZsU2e0iAw/5u5D3YaDNmfOHF+8ePHBv4E73PNRWPM8fGYRjJnW41OffW0zj764noUrN7N+RxsAR09o4H0zJ3Di5NHU16UZUZemPpumvi7cxtTXMWlUDjM7+DaKiAwwM1vi7nP2OS3RoQDw9mr49ukw/Qy47H44wALc3fnN5hYWvraFn67czKLXt9G6n3MljcpnOP6IURx/+EiOP3wUxx8xkpmHNdKYyygsRGRIKBQOZNEt8F/XwkfvgJN/v08vbS8U2bSznbbOIq2dRXZ3hPvWjiJbW9pZsaGZFRua+fWGZlraC5XXZVJGQy5DY/mWz9CQy5DPpMhl0+QyKXKZFHWZFLlMfJxNkc+kyWWrxmVSZDMpculwX5cOr8mkjM6iUyiV6CyWwnDRKbozpj7LuIY6xjfWMaIu0//vT0QOKfsLBS0RAOYugJf+A37y13D02dAwvtcvzWXSTBs34oDPc3ea3m5lxYZmVm1qobmtk5b2Qri1FdjVUWDH7g42FUq0F0q0dxbpKJZo7wyPO2rUyZ3PphjfkGN8Yx312TR1mRTZdIps2sjGgMmmUmTi40zKyKRT1KUNzCgUSxTiJUwLMYSKJe8WZNUBlk6nSBmkzUiZkUoZKQt7fdXXpRmRTTOiLlPZHJdJGR2VUCtVhoulEiWHUslxoOROef2mLlMdniny2TR16RRmhNfE57o7JYds2qivS5PPpEmlVL1JsikUAFJpuPCf4bb3w2NfhI/cNuAfYWZMGzeCadlmzuVlOOYcyDX2+vWlkleFRJG2eF8OjI5CqAjK951FryzYM+kU2ZSRzYQF8vbdnWzd1cHWlg627Wpn664Otu3qoLWjyK72Ap1Fr7xXe6FEoRQW+J0xAApFr4RUCAnrFhwps9jWYnz9oVON5rOp0C+UTZNJpyiWnJJ7t3ug8p1mqgI0k7YQdqk978N7hyCKoQRQNbxnUJlBOhW+13T8jjMpI51KkU6FaWbx/Y3uYbbH151OGXWx6qyLVWVdJlXZfFn+zHLbLIZ2uuozM6kwP9WRWb31s/uUcjOcUskrv5lwH34P7h5XCEL7La4kmIFVvbdhlWH38J5dw0E2naqsdJSr7LpM+B16+XvF4/z1/Lf3+PctlKrvQ3tLHj7UK3/H8EZ1mTT1deE3k8929SmmKt9t97/x/tSl914hy8S/c/UWnfJQfTZNQ27gF+EKhbJJJ8KZfwkLvw4bX4ERY6F+j9uUOXDUew/Y77BP634Ji26Flx+EUieMnAzn/S2c+JFevV8qZeRT6XjSvmzfP3+AlX+kvekXKRRj9VMoVRau5QWsOxRKTnshbnrrCPe7Owq0dhQplDxWLxb/YcJwOpUiXV6IGHHhEv55qiusthhM5ZMcVhY85YURRqFUqnx2W9UmwEKxVFmwp1NdC3mgqjoq0VlyOmP4VYdH+dZRjN9V/HzKC74UZCy1jzaFf/xiZWFaorUzvFdnsVRZ0BQ9LHRLHp7bbSFdtSAtlpyOQgz5GPZy6Pv0B47h2vnHD/j7KhSqvf/zUGiFLaug9e1wYFvr2+FW6gzPGXMUzL4MTrkUxk7f//sVO2H5f8Lzt4Y9nOoaYc6fwIz3w7N/Dw/8CSz+N5j/dZg0q+azN5D60kmeidVKQ66GDZJec3c6i16pClJmlcAyi0FSFWxda85VYeL7HNxLyiyGeAj1dMq6rf2WN+eVK6XqzYBO18qHU64gumqS8k+wsxhWKsorAuUK2t0rz69eCdjfTze0s6s6SsebVb22/H7hs0u0lvsRO4u0xZWaonvley1/x+WVgX3+TaBSjXdV/N7t2KjulRnMmjx6P9/8wVNHc2+4Q9sOeO2/YNn34Y2FgMNRZ8ApH4fJp0LLxnBr3tB13/QL2Lk2hMfcT8Gpl0M+/iFLRVjyXXjqq9DeDL/9aTjrC13T+2PXFtj4Mhz+bhgxrv/vJyLDivY+Gmjb18CL98Gv7oWtq/aeXjcSGg+D8cfCaX8Ex50X+i32ZddW+O+/CQHRMAGm/XYIhvxoyI2Kw6PCe00+FTI9rG6XSvDGs+GcTst/1FXZTDwBjjodjjwdjpwHo6cd3Oavg+UOTYth2T3w2mMhII88PWyGmzZ3YEJQRPpEoVAr7rB2CWx/C0YeDo2Twn1dQ9/fa90v4Zkbw3u17YC2ndDR3P05mTxMOS0uVE+HqXOhY1dY4C79d9i+GvJjQvVyzNmw4UV4axGseQHad4b3aDwcRoyHbD3UjYDsiDCcHREW0PXjoH5M976U/Oiw6auuITwv1YsD4Zs3wK9+AMu+B1teC6879ndg5zpYvwxKBcBg0klhXsYcFU47kmsMoVoeTmWh2BFCrljoGnYP30c2D5n6rvtMDtJZSGW6btUb2Att4dbZ1jXc3hK+8/b4vbfvDI9zo2D6mXDEbEjvZ0vr9rfg9Wdhw0swYSZMeU+Yr54CvNbaW2Llugl2bQrz6iXwYrgvFcNwbjSMmwHjjg5/5/2tLLS3hIq2fmz4roc791Dlb1oe/q8OOzGslA2TeVcoHKqKhbiA2h46v1c/B289B+t/Ff6pLQVYGJ7+vlCVHP/hvX+4pWJ4/VuLYN3S8M/duRs6W7vuO3aHBeGeQbQXC+FQDojK/QjINoT73dvg9WdCu6bNC5vNZl0cKh4IQda0OMzL6p+HzWydNTxtiKXA0l3VU29f43F7bl1jCOLpZ4bvefQUWP3/QhC88Sy8/WZ4XiYfQgYgXQeHnxxCfPJ7QhWYzkI6B5m6MD2dC/O9e0uoGHdvgd1bw+a/QlsIlUw+PDeTD7dUKnx/HbvCgrqjOQ43hxBo2QSdu/r+HeVGw7jpMHZGWCnYtaUrVFo2df/71I0M89MwARomhpWM/Oiu30VdY9dKRKkzBm1z1W1H2Iiei8+r3I8M81zogGJ7OLV9oT18F8XOrlCrBFspLLzrRlRV1qO6Kmz38F107Aq/746WMB/FzrCykM6GCr688tDZGkJg06vh1raj+3eUrgt/y3LlPW1uaHfn7vD+nbvDZ3W2hpWI/Jh4G919paK9OWxh2LIq3G9dCc0bu1bGRoyPt3Hxu43vUR/vsyP6Xe0rFIab9pawIH3rufDDP+VSGH/MwLx3sbOrc7317bCAb28O/1AdLVULpLgw6ty99z9FKg0nXAizL4cJxx74M0vFrs8or5F2NIfhUiH+82bDP1YqGx5jXWv6lTX/1rAQKRW6bsWq4Uwu3vaoLHIj91igjAr/7Ls2h4X/mz8Lt83dz6hbqSRmfCDsPHDYCWHtcu2SEHprl4YKsC8LaUt3VXLFjjh/ccFYDilLxWqqsWoh3BA2WTZOiveHx/vD4kIkFW6pdFdItm6DbW+E64u8/UYYfvuNsBBvmAiNE8P7NRwWhnOjwm9i15bw3eze0jXc3tK7+SxXgWZdoealA7+uPN/dbnGTbOdu9t/d3Qe5UXDYrLDjx2HxNmZaqALfei6smFUq3T6oGxkW6sVOaNlQPVMw5kgYNTl8763bwopBsaPn90plQzic/hl4318dxEwqFEQGRsumEBI7msKa4oE2K0EIvK2rwj98Ma4BFzvDgr7YEdb+GyZ0rR3mx/S8ea5YFW7vxFOklEpVa+a7QsinsjFsR4bw2rNvzT0s1NvjSkehbY/qqK7rcU/zXCrFFYqd4Xtu2xGGLdVVzVZXt+lsXFEohr9FeaUhnYWRRxz4u+3YHYP/hfAelUq56lYqhAq/dXvcHByHLRVW4CbMDP2EY2fsXdm7h/nZvTWslO35Pm07wuNjzoZZF/X974RCQUREquwvFJJ76mwREdlLzULBzO40s01m9nLVuPvMbFm8vWlmy6qmXWdmq8zs12Z2Xq3aJSIiPavlEc3fBb4F3F0e4e5/UB42s38EdsThWcClwInAZOBJMzvO3Xs+J7WIiAy4mlUK7r4Q2LavaRaOEb8EuDeOugj4gbu3u/sbwCpgbq3aJiIi+zZUfQrvAza6+8r4eAqwpmp6Uxy3FzNbYGaLzWzx5s2ba9xMEZFkGapQ+DhdVUKfuPvt7j7H3edMnDhxgJslIpJsg36WVDPLAB8BTqsavRaovkDy1DhOREQG0VBUCr8DrHD3pqpxjwCXmlnOzGYAM4EXhqBtIiKJVrNKwczuBc4CJphZE/AVd7+DsJdRt01H7v6Kmd0PvAoUgKt6s+fRkiVLtpjZ6oNs4gRgy0G+9lCX1HnXfCeL5rtnR/U04ZA+ork/zGxxT0f0DXdJnXfNd7Jovg+OjmgWEZEKhYKIiFQkORRuH+oGDKGkzrvmO1k03wchsX0KIiKytyRXCiIisgeFgoiIVCQyFMzs/HiK7lVmdu1Qt6dWejh9+Tgze8LMVsb7sUPZxlows2lm9rSZvWpmr5jZZ+P4YT3vZpY3sxfM7Fdxvm+I42eY2fPx936fmdUNdVtrwczSZvZLM/tRfDzs5zteguCleDmCxXFcv37niQsFM0sD/wLMB2YBH4+n7h6Ovgucv8e4a4Gn3H0m8FR8PNwUgL9y91nAPOCq+Dce7vPeDnzQ3U8BZgPnm9k84O+Bf3L3Y4G3gU8OXRNr6rPA8qrHSZnvs919dtWxCf36nScuFAin5F7l7q+7ewfwA8Kpu4edHk5ffhFwVxy+C7h4MNs0GNx9vbsvjcPNhAXFFIb5vHvQEh9m482BDwIPxPHDbr4BzGwq8CHgX+NjIwHz3YN+/c6TGAq9Pk33MDXJ3dfH4Q3ApKFsTK2Z2XTgVOB5EjDvcRPKMmAT8ATwG2C7uxfiU4br7/0bwF8Dpfh4PMmYbwceN7MlZrYgjuvX73zQz5Iq7xzu7mY2bPdJNrNG4EHganffGVYeg+E67/GcYbPNbAzwMHD80Lao9szsw8Amd19iZmcNcXMG25nuvtbMDgOeMLMV1RMP5neexEoh6afp3mhmRwDE+01D3J6aMLMsIRC+5+4PxdGJmHcAd98OPA2cDoyJp6yH4fl7PwO40MzeJGwO/iDwTYb/fOPua+P9JsJKwFz6+TtPYij8ApgZ90yoI5y19ZEhbtNgegS4Mg5fCfxwCNtSE3F78h3Acne/qWrSsJ53M5sYKwTMrB44l9Cf8jTw+/Fpw26+3f06d5/q7tMJ/8//7e6XM8zn28wazGxkeRj4XeBl+vk7T+QRzWZ2AWEbZBq4092/NrQtqo3q05cDG4GvAP8XuB84ElgNXOLu+7yW9qHKzM4Efgq8RNc25i8S+hWG7byb2bsJHYtpwgrf/e7+VTM7mrAGPQ74JXCFu7cPXUtrJ24++ry7f3i4z3ecv4fjwwzwfXf/mpmNpx+/80SGgoiI7FsSNx+JiEgPFAoiIlKhUBARkQqFgoiIVCgURESkQqEgMkTM7KzyGT1F3ikUCiIiUqFQEDkAM7siXqdgmZndFk8612Jm/xSvW/CUmU2Mz51tZovM7EUze7h8LnszO9bMnozXOlhqZsfEt280swfMbIWZfc+qT9AkMgQUCiL7YWYnAH8AnOHus4EicDnQACx29xOBZwlHiwPcDXzB3d9NOKK6PP57wL/Eax28FyifxfJU4GrCtT2OJpzHR2TI6CypIvt3DnAa8Iu4El9POMFYCbgvPuce4CEzGw2Mcfdn4/i7gP+I56eZ4u4PA7h7G0B8vxfcvSk+XgZMB35W87kS6YFCQWT/DLjL3a/rNtLsS3s872DPF1N9Lp4i+p+UIabNRyL79xTw+/F89eXr3x5F+N8pn4HzMuBn7r4DeNvM3hfHfwJ4Nl79rcnMLo7vkTOzEYM5EyK9pbUSkf1w91fN7HrC1a1SQCdwFbALmBunbSL0O0A4VfGtcaH/OvDHcfwngNvM7KvxPT42iLMh0ms6S6rIQTCzFndvHOp2iAw0bT4SEZEKVQoiIlKhSkFERCoUCiIiUqFQEBGRCoWCiIhUKBRERKTi/wMmk2gTuj9eRQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_train_loss)\n",
    "plt.plot(epoch_test_loss)\n",
    "plt.title('Total Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
