{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# cuda setup\n",
    "device = torch.device(\"cuda\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "\n",
    "# hyper params\n",
    "batch_size = 64\n",
    "latent_size = 20\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "def one_hot(labels, class_size):\n",
    "    targets = torch.zeros(labels.size(0), class_size)\n",
    "    for i, label in enumerate(labels):\n",
    "        targets[i, label] = 1\n",
    "    return targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, feature_size, latent_size, class_size):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.class_size = class_size\n",
    "\n",
    "        # encode\n",
    "        self.fc1  = nn.Linear(feature_size + class_size, 400)\n",
    "        self.fc21 = nn.Linear(400, latent_size)\n",
    "        self.fc22 = nn.Linear(400, latent_size)\n",
    "\n",
    "        # decode\n",
    "        self.fc3 = nn.Linear(latent_size + class_size, 400)\n",
    "        self.fc4 = nn.Linear(400, feature_size)\n",
    "\n",
    "        self.elu = nn.ELU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x, c): # Q(z|x, c)\n",
    "        '''\n",
    "        x: (bs, feature_size)\n",
    "        c: (bs, class_size)\n",
    "        '''\n",
    "        inputs = torch.cat([x, c], 1) # (bs, feature_size+class_size)\n",
    "        h1 = self.elu(self.fc1(inputs))\n",
    "        z_mu = self.fc21(h1)\n",
    "        z_var = self.fc22(h1)\n",
    "        return z_mu, z_var\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z, c): # P(x|z, c)\n",
    "        '''\n",
    "        z: (bs, latent_size)\n",
    "        c: (bs, class_size)\n",
    "        '''\n",
    "        inputs = torch.cat([z, c], 1) # (bs, latent_size+class_size)\n",
    "        h3 = self.elu(self.fc3(inputs))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x.view(-1, 28*28), c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z, c), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a CVAE model\n",
    "model = CVAE(28*28, latent_size, 10).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        labels = one_hot(labels, 10)\n",
    "        recon_batch, mu, logvar = model(data, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.detach().cpu().numpy()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.082520\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 241.336884\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 204.869431\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 201.142181\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 184.393661\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 174.575607\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 164.313385\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 169.107224\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 153.187912\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 158.907883\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 148.246902\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 147.965347\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 149.000305\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 146.318283\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 146.370926\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 133.839584\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 142.266068\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 136.261887\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 134.755661\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 138.216248\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 130.375549\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 124.313812\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 127.798492\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 133.167694\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 130.552505\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 124.343552\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 125.274017\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 127.637978\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 133.982407\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 132.742203\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 120.808044\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 129.486572\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 122.062225\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 121.848282\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 116.057419\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 121.681793\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 127.194382\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 122.197517\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 130.148178\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 112.556831\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 124.173347\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 130.800003\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 125.303337\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 122.805588\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 122.588181\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 124.901566\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 120.360847\n",
      "====> Epoch: 1 Average loss: 143.0014\n",
      "====> Test set loss: 119.3764\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 123.437958\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 120.312592\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 118.378632\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 124.088852\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 116.777542\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 121.387169\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 120.004868\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 119.240242\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 115.954346\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 119.996307\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 119.396233\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 124.762009\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 118.757645\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 117.436127\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 118.937012\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 121.397705\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 116.123596\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 118.837044\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 121.837204\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 116.513702\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 116.488564\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 118.895943\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 121.058228\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 107.556961\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 115.816795\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 113.581947\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 112.093712\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 115.519592\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 113.816788\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 116.853622\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 116.249786\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 116.282333\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 120.207794\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 115.594887\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 115.655060\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 112.728577\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 120.202942\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 112.943802\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 113.645447\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 116.567093\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 118.222633\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 117.475464\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 112.501396\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 116.294334\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 115.192825\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 113.032738\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 113.381638\n",
      "====> Epoch: 2 Average loss: 116.7187\n",
      "====> Test set loss: 112.5136\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 113.354286\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 114.358238\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 114.560310\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 105.361397\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 105.818016\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 121.119720\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 111.571289\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 109.576233\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 115.366928\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 120.347122\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 113.913162\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 113.574707\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 107.907539\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 110.855461\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 99.926361\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 109.211975\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 109.071983\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 115.216812\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 105.150131\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 116.827637\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 110.618683\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 116.770439\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 106.362289\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 110.942154\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 115.186646\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 107.435318\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 108.739815\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 111.992447\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 109.891251\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 113.268188\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 116.458008\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 110.911339\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 105.038254\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 112.089386\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 112.911598\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 112.591675\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 113.233131\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 113.628891\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 118.884056\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 105.626495\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 98.254425\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 109.760307\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 103.633987\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 106.696869\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 108.817291\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 114.012375\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 110.260071\n",
      "====> Epoch: 3 Average loss: 111.5372\n",
      "====> Test set loss: 108.8555\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 110.825775\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 116.205383\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 109.681915\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 105.451111\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 105.163651\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 105.367233\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 109.218719\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 111.823502\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 110.138222\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 106.522911\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 109.948006\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 106.744209\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 113.907410\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 109.052078\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 109.110901\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 113.139374\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 114.126404\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 112.455658\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 107.546837\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 104.761810\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 104.430229\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 110.354393\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 101.506195\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 108.914482\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 103.370140\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 109.178940\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 109.660774\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 110.755646\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 108.617218\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 109.842270\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 106.446114\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 108.833885\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 106.832611\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 113.010536\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 110.428757\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 109.316711\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 103.709320\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 109.640327\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 107.318451\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 107.055069\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 107.957703\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 112.020432\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 112.930885\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 105.834503\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 107.356735\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 111.139687\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 102.522232\n",
      "====> Epoch: 4 Average loss: 108.4492\n",
      "====> Test set loss: 106.6183\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 100.730850\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 105.855453\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 104.863045\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 109.003738\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 105.026100\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 105.133812\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 105.616356\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 105.880272\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 103.937050\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 112.453552\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 114.040245\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 104.934479\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 103.692558\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 107.065887\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 105.007889\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 103.990997\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 111.254379\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 102.456047\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 102.515930\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 111.364120\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 99.296646\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 110.418167\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 112.641556\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 104.977478\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 109.655472\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 107.789352\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 101.153015\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 103.226295\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 103.678795\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 104.862885\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 98.980148\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 108.664505\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 108.526306\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 107.282639\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 107.453545\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 100.654510\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 108.819839\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 104.297379\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 110.975998\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 104.056183\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 104.082550\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 103.727722\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 109.272896\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 104.199493\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 106.805481\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 100.620758\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 106.455704\n",
      "====> Epoch: 5 Average loss: 106.4947\n",
      "====> Test set loss: 104.7934\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 109.045578\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 107.431969\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 109.806595\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 98.702667\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 103.133850\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 109.826309\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 103.941086\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 104.702156\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 108.246193\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 102.764923\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 104.241791\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 107.835587\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 111.842934\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 107.222794\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 109.744659\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 105.854004\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 106.660973\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 111.854485\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 100.818878\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 105.102722\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 99.454620\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 103.778931\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 102.594604\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 106.091507\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 107.732033\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 105.209045\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 110.790215\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 104.557068\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 106.009842\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 110.643234\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 103.723389\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 105.984871\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 100.427734\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 100.176003\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 107.356956\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 111.054893\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 107.543472\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 106.969398\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 109.336670\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 104.986374\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 106.844879\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 103.388512\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 107.675034\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 108.047989\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 105.783821\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 104.295731\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 109.570343\n",
      "====> Epoch: 6 Average loss: 105.2978\n",
      "====> Test set loss: 104.2351\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 105.202332\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 103.578453\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 102.966644\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 112.681427\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 105.543373\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 104.206596\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 105.462402\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 110.416718\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 112.011444\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 110.093781\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 102.178818\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 105.615799\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 101.033218\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 100.753830\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 105.939804\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 105.826195\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 99.529579\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 105.695312\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 107.446762\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 102.181503\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 103.529137\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 104.055893\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 95.985748\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 102.600876\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 102.774910\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 101.958733\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 103.348114\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 110.196259\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 103.248795\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 108.417862\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 101.620972\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 100.730476\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 106.812210\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 108.325272\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 105.304626\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 109.457817\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 105.279404\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 101.224365\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 107.640266\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 107.718338\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 107.329666\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 107.115097\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 110.255219\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 100.390366\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 101.676056\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 104.989929\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 101.270721\n",
      "====> Epoch: 7 Average loss: 104.2203\n",
      "====> Test set loss: 103.3333\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 100.557434\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 103.775223\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 106.517151\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 102.971649\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 106.944183\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 96.202606\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 104.603592\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 109.668518\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 104.318939\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 95.385086\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 108.249863\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 106.773766\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 106.037262\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 102.757996\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 101.490723\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 102.233833\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 106.885254\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 99.602448\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 108.100906\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 105.964706\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 98.457703\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 100.683548\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 104.190605\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 96.699684\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 107.158440\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 110.111137\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 104.753593\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 102.375687\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 111.007774\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 102.760307\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 103.672562\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 108.195679\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 98.679436\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 102.505898\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 108.053108\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 92.225639\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 97.542671\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 99.277367\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 103.741890\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 106.035500\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 99.902267\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 102.800133\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 107.532158\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 101.866219\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 108.652451\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 108.682648\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 102.711868\n",
      "====> Epoch: 8 Average loss: 103.4898\n",
      "====> Test set loss: 102.6743\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 111.579163\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 98.515877\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 100.177986\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 109.484962\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 96.851158\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 103.622124\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 100.619911\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 104.635040\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 99.672165\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 103.262726\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 104.446335\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 97.091232\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 99.597397\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 107.401527\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 99.972687\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 103.446518\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 107.086449\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 101.926666\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 98.887009\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 108.967468\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 101.017509\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 99.616745\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 101.530243\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 99.290474\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 100.611084\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 105.437317\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 100.033508\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 100.327682\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 103.713058\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 104.116150\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 104.038132\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 102.482353\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 104.508125\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 105.548386\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 106.804939\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 101.157852\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 103.438179\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 100.408310\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 102.879539\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 102.516052\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 102.215302\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 110.018951\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 100.438454\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 101.873192\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 104.138359\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 103.305099\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 102.925598\n",
      "====> Epoch: 9 Average loss: 102.8337\n",
      "====> Test set loss: 102.1961\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 97.857651\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 106.328018\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 94.470909\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 102.557663\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 99.220383\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 101.582558\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 103.561424\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 110.299126\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 103.274597\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 104.081985\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 103.383423\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 103.224571\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 106.129257\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 112.218155\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 92.579147\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 96.480698\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 102.247391\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 109.194107\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 101.290604\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 100.872826\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 101.730949\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 100.448631\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 99.539604\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 103.568993\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 97.448265\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 98.645233\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 103.870346\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 99.540489\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 103.109726\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 101.656296\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 102.762695\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 106.382950\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 102.817314\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 100.866089\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 104.699188\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 102.743927\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 105.322067\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 105.830528\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 101.328041\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 102.994431\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 101.180214\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 106.250351\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 99.822128\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 100.037354\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 100.168533\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 103.193481\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 101.845421\n",
      "====> Epoch: 10 Average loss: 102.3950\n",
      "====> Test set loss: 101.5756\n"
     ]
    }
   ],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, labels) in enumerate(test_loader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            labels = one_hot(labels, 10)\n",
    "            recon_batch, mu, logvar = model(data, labels)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).detach().cpu().numpy()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 5)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(-1, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        with torch.no_grad():\n",
    "            c = torch.eye(10, 10).cuda()\n",
    "            sample = torch.randn(10, 20).to(device)\n",
    "            sample = model.decode(sample, c).cpu()\n",
    "            save_image(sample.view(10, 1, 28, 28),\n",
    "                       'sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.randn(1, 20).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.eye(1, 10).cuda()\n",
    "c[0][0]=0\n",
    "c[0][5]=1\n",
    "c.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = model.decode(sample, c).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb9ae0722b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample1 = sample\n",
    "sample1=sample1.detach().numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(sample1.reshape(1, 28, 28).squeeze())\n",
    "# sample1.reshape(1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
